{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/vexo/source/fonts/SourceSansPro.ttf","path":"fonts/SourceSansPro.ttf","modified":0,"renderable":1},{"_id":"themes/vexo/source/js/gitment.js","path":"js/gitment.js","modified":0,"renderable":1},{"_id":"themes/vexo/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/vexo/source/js/qrious.js","path":"js/qrious.js","modified":0,"renderable":1},{"_id":"themes/vexo/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/alipay.jpg","path":"css/images/alipay.jpg","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/catalog.png","path":"css/images/catalog.png","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/error_icon.png","path":"css/images/error_icon.png","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/escheres.png","path":"css/images/escheres.png","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/logo.png","path":"css/images/logo.png","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/menu.png","path":"css/images/menu.png","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/top.png","path":"css/images/top.png","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/images/wechat.jpg","path":"css/images/wechat.jpg","modified":0,"renderable":1},{"_id":"themes/vexo/source/css/plugins/gitment.css","path":"css/plugins/gitment.css","modified":0,"renderable":1},{"_id":"themes/butterfly/source/css/index.styl","path":"css/index.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/css/var.styl","path":"css/var.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/404.jpg","path":"img/404.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/algolia.svg","path":"img/algolia.svg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/sea.jpg","path":"img/sea.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/pic1.png","path":"img/pic1.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/pic2.png","path":"img/pic2.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/pic3.png","path":"img/pic3.png","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1627447000146},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1627447000146},{"_id":"themes/landscape/.npmignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1627447000146},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1627447000146},{"_id":"themes/landscape/_config.yml","hash":"79ac6b9ed6a4de5a21ea53fc3f5a3de92e2475ff","modified":1627447000146},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1627447000150},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1627447000150},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1627447000150},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1627447000150},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1627447000150},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1627447000150},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1627447000150},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1627447000150},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1627447000150},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1627447000150},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1627447000150},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1627447000150},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1627447000150},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1627447000150},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1627447000150},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1627447000150},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1627447000150},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1627447000150},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1627447000150},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1627447000150},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1627447000150},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1627447000150},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1627447000150},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1627447000150},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1627447000150},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1627447000150},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1627447000150},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1627447000154},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1627447000154},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1627447000154},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1627447000150},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1627447000154},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1627447000150},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1627447000154},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1627447000154},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1627447000154},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1627447000154},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1627447000154},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1627447000154},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1627447000154},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1627447000154},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1627447000154},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1627447000154},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1627447000154},{"_id":"public/2021/07/28/hello-world/index.html","hash":"18206e8e3336033293a413c695b3ba544dc43ee4","modified":1630074889035},{"_id":"public/archives/2021/index.html","hash":"aed83918bf371169a5137727dc046c3d30470072","modified":1630075014154},{"_id":"public/archives/index.html","hash":"7d4852b1fb3b0e359db9de421c1bafaad64316f4","modified":1630075014154},{"_id":"public/archives/2021/07/index.html","hash":"07ef0d2bda5834e33511bb57b04c7eeea7c71c11","modified":1630075014154},{"_id":"public/index.html","hash":"e679863ca76444ab4e002ce3376a0ad9623a74d7","modified":1630075014154},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1627474983557},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1627474983557},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1627474983557},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1627474983557},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1627474983557},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1627474983557},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1627474983557},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1627474983557},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1627474983557},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1627474983557},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1627474983557},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1627474983557},{"_id":"public/js/script.js","hash":"1e0a395deba4f5ae66a5758590e094d26d777421","modified":1627478149719},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1627474983557},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1627474983557},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1627474983557},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1627474983557},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1627474983557},{"_id":"public/css/style.css","hash":"5adfacca2813469f858b0916dbfdeaf7d0f083e9","modified":1627478149719},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1627474983557},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1627474983557},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1627474983557},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1627474983557},{"_id":"source/_posts/test.md","hash":"85a8b3e167ab786a2a4a825e110c0f4c09bc5021","modified":1627476052875},{"_id":"public/2021/07/28/test/index.html","hash":"8adc6251955149dc7f9d27873adda5fbf00a973a","modified":1627787316394},{"_id":"source/series/index.md","hash":"364b07ea1e0b28e792fc7bbddac0069c771dbefa","modified":1627478099732},{"_id":"source/project/index.md","hash":"b8f5482c157514bd2df4ce8a4e4d01a957497924","modified":1627478099732},{"_id":"source/tags/index.md","hash":"a79a6d1880b14fb2b87cc8932bccd64768a3d509","modified":1629443292843},{"_id":"source/about/index.md","hash":"68c8af0453deca5f6b03eb18ee6f89fb0c1f1f75","modified":1627478099732},{"_id":"themes/vexo/.gitignore","hash":"37fb9fd49e7f944716efd3284a6bf55adb6dd0c2","modified":1627477771162},{"_id":"themes/vexo/LICENSE","hash":"3e135cd69c0e02c0a49dd43d571f600223cc61d1","modified":1627477771162},{"_id":"themes/vexo/_config.yml","hash":"6d60a2ff2436a8d669cca200aaa25ad59cc967b1","modified":1627477771162},{"_id":"themes/vexo/.travis.yml","hash":"cc482f06f4ea6962058f2548dcb814b954876e76","modified":1627477771162},{"_id":"themes/vexo/README.md","hash":"76d30348cde696a75dd4f50b0747c5e8e3779046","modified":1627477771162},{"_id":"themes/vexo/lint.sh","hash":"f580302e4aa9ccfb95a253851da6501d145613fe","modified":1627477771162},{"_id":"themes/vexo/package.json","hash":"44e46ed458ca4351db2a773bb4c09ac814b7b1e0","modified":1627477771162},{"_id":"themes/vexo/layout/archive.ejs","hash":"f2480a73048bcb8673d78283090634d0857b80fb","modified":1627477771162},{"_id":"themes/vexo/layout/about.ejs","hash":"76ba7418788eb2bb9ba46844f4d750734847d0b4","modified":1627477771162},{"_id":"themes/vexo/layout/index.ejs","hash":"9f12f5928d68d4d68175b825e18f89f3b0dfdb69","modified":1627477771162},{"_id":"themes/vexo/layout/layout.ejs","hash":"a7b8f1debdca12d667ecd1bcc3d4bc6e13a23d7b","modified":1627477771162},{"_id":"themes/vexo/layout/page.ejs","hash":"c5a6f2f18b0cf664c7a34945ff46f83c81d6924c","modified":1627477771162},{"_id":"themes/vexo/layout/project.ejs","hash":"a023b833a64006989c718c8d6125978a87fa2c3e","modified":1627477771162},{"_id":"themes/vexo/layout/series.ejs","hash":"32010111ef27d2eefec44aba5a865064659d613a","modified":1627477771162},{"_id":"themes/vexo/layout/tags.ejs","hash":"256413f3c6098eba42fce8f24319913e4b2512dc","modified":1627477771162},{"_id":"themes/vexo/_source/project/index.md","hash":"b8f5482c157514bd2df4ce8a4e4d01a957497924","modified":1627477771162},{"_id":"themes/vexo/_source/about/index.md","hash":"68c8af0453deca5f6b03eb18ee6f89fb0c1f1f75","modified":1627477771162},{"_id":"themes/vexo/_source/series/index.md","hash":"364b07ea1e0b28e792fc7bbddac0069c771dbefa","modified":1627477771162},{"_id":"themes/vexo/_source/tags/index.md","hash":"80a15f1b5daff22b04849109e976bc91a410b83e","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/archive.ejs","hash":"9abbf14034d581569c0b6c992fe22035cb5306b3","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/catalog.ejs","hash":"0352ce39c28074dcfc3bd6416680195eeb384fd1","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/footer.ejs","hash":"6cf6b5ddb1c4c2e219fdb56ef0ab5c984bb4e7e9","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/head.ejs","hash":"cbd8c09e3349ed4facec92d5068c4ee777d52418","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/header.ejs","hash":"8a3a4d5895330176e18bd6500b86b494c9246e7c","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/nav.ejs","hash":"3d8ddc1f6e135a240d40edd157cf37f5d0a12df6","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/pager.ejs","hash":"3a1b9680fbfa3baa76933c7c17216996381ad241","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/tag.ejs","hash":"5d2a2c3f8ca7000945ab426a0c6939421974b224","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/top.ejs","hash":"f09dea486246a580213005b21d4b38810dd16fb3","modified":1627477771162},{"_id":"themes/vexo/layout/_third-party/google_analytics.ejs","hash":"d09107a4ee5753a2fbbdb16ddf79d23ff0618c19","modified":1627477771162},{"_id":"themes/vexo/layout/_third-party/localsearch.ejs","hash":"fdf9e9f25b673c3f66694581cf5ebb578e5393b9","modified":1627477771162},{"_id":"themes/vexo/layout/_third-party/mathjax.ejs","hash":"aa58f0cfe22e7151c1a0521bbfa5cbd76f6dcd9d","modified":1627477771162},{"_id":"themes/vexo/source/css/_config.styl","hash":"0c9c0e77d6b8813cb76494e8d757843aab9e117b","modified":1627477771162},{"_id":"themes/vexo/source/css/style.styl","hash":"9dcd5509187654c29d0d908016a052fd164d257c","modified":1627477771166},{"_id":"themes/vexo/source/js/local-search.js","hash":"2d4c35e67f6ae2234a220c2898534d5bcb5245a2","modified":1627477771170},{"_id":"themes/vexo/source/js/qrious.js","hash":"a9271e81e2ac6a692b1c133811afa33f0f3d7dc5","modified":1627477771170},{"_id":"themes/vexo/source/js/script.js","hash":"1e0a395deba4f5ae66a5758590e094d26d777421","modified":1627477771170},{"_id":"themes/vexo/layout/_partial/search/localsearch_button.ejs","hash":"3cb21e8d581ee998796f3f4bf9cd00abb9a67072","modified":1627477771162},{"_id":"themes/vexo/layout/_partial/search/localsearch_view.ejs","hash":"79e5e1a159d18b9e05961a47a4607b14be447e50","modified":1627477771162},{"_id":"themes/vexo/source/css/_partial/about.styl","hash":"1fc9572052194d94f86224a860e6b52435492e57","modified":1627477771162},{"_id":"themes/vexo/source/css/_partial/archive.styl","hash":"e80ddf26f2af3523632afeabd57f81592537985a","modified":1627477771162},{"_id":"themes/vexo/source/css/_partial/catalog.styl","hash":"e3fd04eb0ae723def29592a0063e0706046cb6f2","modified":1627477771162},{"_id":"themes/vexo/source/css/_partial/categories.styl","hash":"d78cf86064428693e2173b4f23535cf311d18091","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/footer.styl","hash":"d90207a2b25e7a7a12b94cfcd5dde070422783e7","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/header.styl","hash":"6154d83977e8e725c52786b14a4d9b3bc62b1061","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/markdown.styl","hash":"2a1a99a0a8a9ffab1e64b7d7c9241ce8d5c84c39","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/nav.styl","hash":"e92c010c5cd460e75c67083df8cdd0bf4d25cde4","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/pager.styl","hash":"888384c67429c7568aa38b5ebe5acae3cc4de367","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/project.styl","hash":"e9b6faadf4852bce3a4141cba0a102a7afb81e9f","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/search.styl","hash":"4fa3d5c7efd1de510719157bd7a6ab3c9f24e9fd","modified":1627477771166},{"_id":"themes/vexo/source/css/_partial/tags.styl","hash":"5198a7f7c221341138ae5c65185e86b6e13e8e26","modified":1627477771166},{"_id":"themes/vexo/source/css/images/alipay.jpg","hash":"c49822ea6f06f868c2404fb00a93f913c8fff7b5","modified":1627477771166},{"_id":"themes/vexo/source/css/images/catalog.png","hash":"541d20dd600fc2c9230329ceb6885d86e6c151dd","modified":1627477771166},{"_id":"themes/vexo/source/css/images/error_icon.png","hash":"efec6e759508dd02e6fa8c4facd9a25a61aae055","modified":1627477771166},{"_id":"themes/vexo/source/css/images/escheres.png","hash":"55deece3236dcc2fb44c28dec3e8bacbb7b46542","modified":1627477771166},{"_id":"themes/vexo/source/css/images/logo.png","hash":"718c6e48956249121cf3cca1a22a99f8372a3f0d","modified":1627477771166},{"_id":"themes/vexo/source/css/images/menu.png","hash":"bdaa35eb1ed119caeb934e15a05b9f4a5396d957","modified":1627477771166},{"_id":"themes/vexo/source/css/images/top.png","hash":"611a257907474ca02828319f81b006c1d818bb84","modified":1627477771166},{"_id":"themes/vexo/source/css/images/wechat.jpg","hash":"5bed6d3eb9f71b227b0ea0187c1a7ba8caf5ee64","modified":1627477771166},{"_id":"themes/vexo/source/css/plugins/gitment.css","hash":"541ff18d7f3542b5663dc6aad06d43e135332b71","modified":1627477771166},{"_id":"themes/vexo/source/fonts/SourceSansPro.ttf","hash":"1e9f0372c269da205fdbac8cf27cb9cf59f6ad45","modified":1627477771166},{"_id":"themes/vexo/source/js/gitment.js","hash":"376446d9c5930576016f97dd63e5e6616c94d8d4","modified":1627477771170},{"_id":"public/series/index.html","hash":"53c09ce05b6140555a65d83eacc4d477514b8698","modified":1630075014154},{"_id":"public/project/index.html","hash":"09bc62ea9eacfd12241806855090e40a7b101b00","modified":1630075014154},{"_id":"public/tags/index.html","hash":"461c15b7a06ec7aa6114b5865ed287d804eea34b","modified":1630075014154},{"_id":"public/about/index.html","hash":"51a09647413e10e21dc065705b0d68e88d2904aa","modified":1630075014154},{"_id":"public/css/images/catalog.png","hash":"541d20dd600fc2c9230329ceb6885d86e6c151dd","modified":1627478149719},{"_id":"public/css/images/alipay.jpg","hash":"c49822ea6f06f868c2404fb00a93f913c8fff7b5","modified":1627478149719},{"_id":"public/css/images/error_icon.png","hash":"efec6e759508dd02e6fa8c4facd9a25a61aae055","modified":1627478149719},{"_id":"public/css/images/escheres.png","hash":"55deece3236dcc2fb44c28dec3e8bacbb7b46542","modified":1627478149719},{"_id":"public/css/images/logo.png","hash":"718c6e48956249121cf3cca1a22a99f8372a3f0d","modified":1627478149719},{"_id":"public/css/images/menu.png","hash":"bdaa35eb1ed119caeb934e15a05b9f4a5396d957","modified":1627478149719},{"_id":"public/css/images/top.png","hash":"611a257907474ca02828319f81b006c1d818bb84","modified":1627478149719},{"_id":"public/css/images/wechat.jpg","hash":"5bed6d3eb9f71b227b0ea0187c1a7ba8caf5ee64","modified":1627478149719},{"_id":"public/fonts/SourceSansPro.ttf","hash":"1e9f0372c269da205fdbac8cf27cb9cf59f6ad45","modified":1627478149719},{"_id":"public/js/local-search.js","hash":"2d4c35e67f6ae2234a220c2898534d5bcb5245a2","modified":1627478149719},{"_id":"public/js/qrious.js","hash":"a9271e81e2ac6a692b1c133811afa33f0f3d7dc5","modified":1627478149719},{"_id":"public/css/plugins/gitment.css","hash":"541ff18d7f3542b5663dc6aad06d43e135332b71","modified":1627478149719},{"_id":"public/js/gitment.js","hash":"376446d9c5930576016f97dd63e5e6616c94d8d4","modified":1627478149719},{"_id":"source/_posts/Bits-Intergers-Floating-Points.md","hash":"a0c33fc0418f04209798853b3fd4520afa2fa822","modified":1629469827864},{"_id":"public/2021/07/28/Bits-Intergers-Floating-Points/index.html","hash":"11f61fabba35ead2a50189d87c7cb565a6c99649","modified":1630074889035},{"_id":"source/_posts/endian","hash":"b057ee3e6c773791cb34e48be1e40e345e4a15ee","modified":1627486699518},{"_id":"source/_posts/endian.c","hash":"991d1d12579f312c63302e6624f697f287d15125","modified":1627486697186},{"_id":"source/_posts/conver","hash":"10ee5b6abfee756ca1fb9fed58b6a365163a573e","modified":1627551474659},{"_id":"source/_posts/conver.c","hash":"3c4c13c11dcb52973409ffd9b6113bd9acca4177","modified":1627551495456},{"_id":"source/_posts/Machine-Level-Representation-of-Programs.md","hash":"8d99acec5ed8c7cb90422c95604243305fb16bfc","modified":1629878350100},{"_id":"source/_posts/bag.md","hash":"e89fb74a393387484227fb9f687ae9482af5c9e7","modified":1627874073607},{"_id":"public/2021/08/01/Machine-Level-Representation-of-Programs/index.html","hash":"72b4a671d93cda3c8424a99b6017ebef82385140","modified":1630074889035},{"_id":"public/2021/08/01/bag/index.html","hash":"5b764968a5145cb249cee3ff06df7ca3c47f29fa","modified":1627787316394},{"_id":"public/archives/2021/08/index.html","hash":"ec7a00e79d1e6149946535d2c952117b4ab8966e","modified":1630075014154},{"_id":"source/_posts/.deep-learning-part-1.md.swp","hash":"2c3d5ccf8d56bcafb5ba1d380cb8b01ef618b8e3","modified":1628499859585},{"_id":"source/_posts/Exceptional-Control-Flow.md","hash":"77290b4e98c007721f268422bfc89c63674bddf4","modified":1629794460219},{"_id":"source/_posts/Concurrent-Programming.md","hash":"a35ec9315c7c46b6da3ef8ff9c36f74b1ab06c4b","modified":1629470001656},{"_id":"source/_posts/Network-Programming.md","hash":"4d2f0dd9c66b8b8826425ca4a3c6c5b5911aa369","modified":1629469869331},{"_id":"source/_posts/Pytorch-Dataloader.md","hash":"34b3173037ad85dc3628d0cd99995b57bbd0a00f","modified":1628604036588},{"_id":"source/_posts/deep-learning-part-1.md","hash":"e1e378d93342d59c21877ef0c1d2437de3f8b9f8","modified":1628494751306},{"_id":"source/_posts/Pytorch-Embedding.md","hash":"3508c2bf3900ef4a85f9ede5107fcbd8dec488a7","modified":1628563645240},{"_id":"source/_posts/System-Level-I-O.md","hash":"739e886407ae15faa11667e6c590baea4c9c36c3","modified":1629905462125},{"_id":"source/_posts/surprise.md","hash":"34294885c9d0d9bb94d45c2960af83d5dd55bc34","modified":1628181628183},{"_id":"themes/butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1629442423966},{"_id":"themes/butterfly/README.md","hash":"cedd13fcd8c75a68742265dd8eced4087e940ffd","modified":1629442423970},{"_id":"themes/butterfly/README_CN.md","hash":"459d6f3200863021bee1fe72a719aef236fb4090","modified":1629442423970},{"_id":"themes/butterfly/package.json","hash":"c18b118422237b964063874a099945b79573feca","modified":1629442423974},{"_id":"themes/butterfly/.github/stale.yml","hash":"05a55a87fa7f122c59683e41c8b2e37e79f688f0","modified":1629442423966},{"_id":"themes/butterfly/_config.yml","hash":"24b479baabfae6b9a56a45787e987cc439839d22","modified":1629469167290},{"_id":"themes/butterfly/languages/default.yml","hash":"7ca673fb629ea74f5ba5e75b4f0f95248cfb5090","modified":1629442423970},{"_id":"themes/butterfly/languages/en.yml","hash":"cd333235ff1648a6bf58dfafc81f2c57672a15a5","modified":1629442423970},{"_id":"themes/butterfly/languages/zh-CN.yml","hash":"741e522b2387f94764a73844e7b084cc7e927c54","modified":1629442423970},{"_id":"themes/butterfly/languages/zh-TW.yml","hash":"79a50c40d9f5463f1fa42aa870ac6b8b84540412","modified":1629442423970},{"_id":"themes/butterfly/layout/archive.pug","hash":"bd62286afb64a51c97e800c5945620d51605d5fa","modified":1629442423970},{"_id":"themes/butterfly/layout/category.pug","hash":"60c1b795b6e227b5dd81963b51d29d1b81d0bf49","modified":1629442423970},{"_id":"themes/butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1629442423974},{"_id":"themes/butterfly/layout/page.pug","hash":"82aa988527a11835e7ac86ce4f23b8cd20014dfa","modified":1629442423974},{"_id":"themes/butterfly/layout/post.pug","hash":"8d398c8925182699d9f2b9f1b727f06228488312","modified":1629442423974},{"_id":"themes/butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1629442423974},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/bug_report.md","hash":"476802922b774b679225102ac30a9d9183394701","modified":1629442423966},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/custom.md","hash":"eff495eb1584cf4586e33c76e8b2fa6a469a179b","modified":1629442423966},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/feature_request.md","hash":"f6867a2f0417fe89a0f2008730ee19dd38422021","modified":1629442423966},{"_id":"themes/butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":1629442423966},{"_id":"themes/butterfly/layout/includes/404.pug","hash":"7d378e328a53cc99d5acc9682dce53f5eb61537d","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/additional-js.pug","hash":"4156224c47bfc2482281ac4e4df701c30476ff00","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head.pug","hash":"1377952022ee0a9eaa7a2fd1098f1571efc468d9","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/layout.pug","hash":"6f2608c4d93d3d10ae6b2cd7f8918f303f024321","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/pagination.pug","hash":"0b80f04950bd0fe5e6c4e7b7559adf4d0ce28436","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/rightside.pug","hash":"2d0453adf92a3fd3466cf0793f14685d17b8b51d","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/sidebar.pug","hash":"8dafc2dcd8c33f70a546fee443f0b6d80b3cd243","modified":1629442423970},{"_id":"themes/butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1629442423974},{"_id":"themes/butterfly/scripts/events/init.js","hash":"018aa446265fe627301b1d53d7cba4f4ff1960ac","modified":1629442423974},{"_id":"themes/butterfly/scripts/events/welcome.js","hash":"d575137c8779e50422c2416f4d0832fdea346ee6","modified":1629442423974},{"_id":"themes/butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1629442423974},{"_id":"themes/butterfly/scripts/helpers/aside_categories.js","hash":"e00efdb5d02bc5c6eb4159e498af69fa61a7dbb9","modified":1629442423974},{"_id":"themes/butterfly/scripts/helpers/inject_head_js.js","hash":"65f2442e04c4defd16e7c1e67701d3bb41d9577a","modified":1629442423974},{"_id":"themes/butterfly/scripts/helpers/page.js","hash":"c6611d97087c51845cb1ab4821696a62fa33daeb","modified":1629442423974},{"_id":"themes/butterfly/scripts/helpers/related_post.js","hash":"54b9324e3506dcc9c9991ef5e11e37e66e21594f","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/button.js","hash":"b816ded1451f28c7c54151ffe6c259b110253ae3","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/gallery.js","hash":"94826ea6bcc4d2304199adae12c4e2b272caf529","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/hide.js","hash":"f33858ffb9e88191e644796e11d2f901eb332308","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/inlineImg.js","hash":"a43ee2c7871bdd93cb6beb804429e404570f7929","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/label.js","hash":"03b2afef41d02bd1045c89578a02402c28356006","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/mermaid.js","hash":"35f073021db93699fcac9ef351e26c59c31aadf7","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/note.js","hash":"c16c6eb058af2b36bcd583b2591076c7ebdd51ad","modified":1629442423974},{"_id":"themes/butterfly/scripts/tag/tabs.js","hash":"6c6e415623d0fd39da016d9e353bb4f5cca444f5","modified":1629442423974},{"_id":"themes/butterfly/scripts/filters/post_lazyload.js","hash":"4cc2d517195c8779471d326ada09f9371cbad4dd","modified":1629442423974},{"_id":"themes/butterfly/scripts/filters/random_cover.js","hash":"9821872007cf57efae4b728dc575ef9d004547bb","modified":1629442423974},{"_id":"themes/butterfly/source/css/index.styl","hash":"861998e4ac67a59529a8245a9130d68f826c9c12","modified":1629442423978},{"_id":"themes/butterfly/source/css/var.styl","hash":"40c3f64422205a24e68ce1ad8fe8163f24fdd525","modified":1629442423978},{"_id":"themes/butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1629442423978},{"_id":"themes/butterfly/source/img/algolia.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1629442423978},{"_id":"themes/butterfly/source/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1629442423978},{"_id":"themes/butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1629442423978},{"_id":"themes/butterfly/source/js/main.js","hash":"8ef2821ceb92d81aa0f8c02ee932f094adcafb2b","modified":1629442423978},{"_id":"themes/butterfly/source/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1629442423982},{"_id":"themes/butterfly/source/js/utils.js","hash":"8319b59c26ce8cd2b0ae7d030c4912215148fa92","modified":1629442423982},{"_id":"themes/butterfly/layout/includes/head/Open_Graph.pug","hash":"6c41f49a3e682067533dd9384e6e4511fc3a1349","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/analytics.pug","hash":"90d01b88d0f406d00184960b1afe9230aec2ebe6","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/config_site.pug","hash":"889ef16fa34a39e5533bc170e62f20f3450cc522","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/config.pug","hash":"4def0aab9e2172ad1f29abd1535d8e08ff23aa0b","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/noscript.pug","hash":"d16ad2ee0ff5751fd7f8a5ce1b83935518674977","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/preconnect.pug","hash":"e55f8bdb876d5429a908498db1307b94094c0d06","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/header/index.pug","hash":"65fa23680af0daf64930a399c2f2ca37809a8149","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/header/menu_item.pug","hash":"24370508ee87f14418e8f06e9d79ad8c52a342c4","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/header/nav.pug","hash":"c205b9fd72b2fe19e6d15c5b5ab0fb38c653032e","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/header/post-info.pug","hash":"92f81a437c9db49f7ebcf608bc09488ecdb55a21","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/header/social.pug","hash":"0d953e51d04a9294a64153c89c20f491a9ec42d4","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/loading/loading-js.pug","hash":"4cfcf0100e37ce91864703cd44f1cb99cb5493ea","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/loading/loading.pug","hash":"5276937fbcceb9d62879dc47be880cd469a27349","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/mixins/article-sort.pug","hash":"2fb74d0b0e4b98749427c5a1a1b0acb6c85fadc4","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/mixins/post-ui.pug","hash":"4c3c5cb69b3aead8c232cb0fbc251929f28aad75","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/page/categories.pug","hash":"1f30952fed73dec21b42e2e30b7fe2e84618d2e4","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/page/flink.pug","hash":"b53a2d4f9c37b375a4446d2273dcfb7712d91b3e","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/page/default-page.pug","hash":"dbec869c62135695495703a29ad7655e9965d461","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/page/tags.pug","hash":"93d4ebc7dc8228c7a10ddeb5a553d0dcdabbe145","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/post/post-copyright.pug","hash":"88e3b611b03149665e4113cfa39595c1a3fca7e5","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/post/reward.pug","hash":"5b404356f311d2ee36478291ca3553210867b738","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/aplayer.pug","hash":"292646dfab135973b09f0fa9e3931e83da2ed30e","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/effect.pug","hash":"b9d54a01d7c2a7a183cb7209e99430ce7fea1fe3","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/pangu.pug","hash":"d5fec7dedc52ab23865fb4db002755e9bdaadc9f","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/pjax.pug","hash":"933cb710d2dbcea25c6426a57c6f49d2f48b792c","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/prismjs.pug","hash":"1fbecfd299068f90d727f0c8c65e2a792fa6e3e2","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/subtitle.pug","hash":"d50e5c22cd6bc3c378bc581918136746cfa3447f","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_announcement.pug","hash":"3d8e3706a056389176f55dd21956aabc78046761","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_author.pug","hash":"0366c658cdcff839aa1df2e2d252a03a53fd427e","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_newest_comment.pug","hash":"27afd2274bd5f2cbbf1bad9f0afe2b2b72c213ca","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_post_toc.pug","hash":"ae9336bf31cdad08ff586ead4295912a96563c76","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_recent_post.pug","hash":"9c1229af6ab48961021886882c473514101fba21","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_tags.pug","hash":"438aea3e713ed16b7559b9a80a9c5ec0221263df","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo.pug","hash":"0612aaee878f33ea8d3da0293c7dc3b6cd871466","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/widget/index.pug","hash":"b5525891f6affd02c1ea3b2327c026882efe428b","modified":1629442423974},{"_id":"themes/butterfly/source/css/_global/index.styl","hash":"e211efbd1952d4b1b881287aa43423133c1d166e","modified":1629442423974},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl","hash":"85e72c70a0cef29e40be1968f5d23b06c6f8e3aa","modified":1629442423974},{"_id":"themes/butterfly/source/css/_global/function.styl","hash":"eda47f3e807a466ba8275627ea936c5100c43818","modified":1629442423974},{"_id":"themes/butterfly/source/css/_highlight/theme.styl","hash":"fa7a4c1685f391d60ed863e869b9604b59746c27","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/aside.styl","hash":"7feb755ca7c22da36bbad11e74ecd95fdcf3b879","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/chat.styl","hash":"29f48f9370f245e6e575b5836bccf47eb5688d8b","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/comments.styl","hash":"f1b63892baafa48ab872bc79671d57aafd511f6c","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/footer.styl","hash":"dd8cdf639ba2b726437c77fa7aa8d5edbabe8f9b","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/head.styl","hash":"98235fcda3b87ad6f7e91eafbed94d0d6ae847ca","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/loading.styl","hash":"7d18a7be9cfea65091de3ef00014063d2d649912","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/pagination.styl","hash":"90fe01c968696a9f791cb2b84fca621cbbb56f47","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/post.styl","hash":"d748951d9fbcd04dda839085af78b01b8fa04cba","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/reward.styl","hash":"ea1ba40dd5954c2ed718a126336fb7f94da4e66f","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/relatedposts.styl","hash":"0551c5893d1589a3d17ce161e50ecb1d724cc6e8","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/rightside.styl","hash":"7a072589e6097dbe942783131964f2372fdf1eb6","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/sidebar.styl","hash":"2c5fb77c448ce0a734040c8ce532b28fed688899","modified":1629442423978},{"_id":"themes/butterfly/source/css/_layout/third-party.styl","hash":"978c397d0966eaf9e6e2afd13866f8f4900b509f","modified":1629442423978},{"_id":"themes/butterfly/source/css/_mode/darkmode.styl","hash":"4e629f510b73f998ab208b739c5bd7dcd168d1a7","modified":1629442423978},{"_id":"themes/butterfly/source/css/_mode/readmode.styl","hash":"f59a9a0059d5261251bdd6de45aa97dd2d11e633","modified":1629442423978},{"_id":"themes/butterfly/source/css/_page/404.styl","hash":"b0488ceacde74af139d66c8db5cb36cc21737b9b","modified":1629442423978},{"_id":"themes/butterfly/source/css/_page/archives.styl","hash":"6874adc2e276443f354bbe50d0072e9bec37243c","modified":1629442423978},{"_id":"themes/butterfly/source/css/_page/categories.styl","hash":"e554549f0a0ae85362f0b0e8687981741f486f6b","modified":1629442423978},{"_id":"themes/butterfly/source/css/_page/common.styl","hash":"97fec1e814f88237862f4d800a35362b802f6625","modified":1629442423978},{"_id":"themes/butterfly/source/css/_page/flink.styl","hash":"2cc49d3f6a6beb9f7bff93e292f88aa5681da1d0","modified":1629442423978},{"_id":"themes/butterfly/source/css/_page/homepage.styl","hash":"7c4152162a03aa8331a783df5695e4ebbb816a8c","modified":1629442423978},{"_id":"themes/butterfly/source/css/_page/tags.styl","hash":"9a881c031f463c486bd25248c2814fd09f97892b","modified":1629442423978},{"_id":"themes/butterfly/source/css/_search/algolia.styl","hash":"917e0e399e117217184ca63d3eb5c4843bcccf7b","modified":1629442423978},{"_id":"themes/butterfly/source/css/_search/index.styl","hash":"f168f5c669978f633abe118cdcc4a12cfc883c01","modified":1629442423978},{"_id":"themes/butterfly/source/css/_search/local-search.styl","hash":"6befe4c51b86d0c1de130beeecad9e28d6442713","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/button.styl","hash":"1c3f9d7efc3b9dfcfa8926a1132d0c44ffc7d4b2","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/gallery.styl","hash":"53ecae272e16223a436c497abbf25dd5f0fc4aaa","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/hexo.styl","hash":"d0386ba6d8d63afc72b9673e8f3e89df6446ffc2","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/hide.styl","hash":"21964fdd6d74ffbea519418bab65024aee5f3736","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/label.styl","hash":"f741e85295ce15c70a6027ec15a542636dd5dcca","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/tabs.styl","hash":"1756791581c0ec51cb03353a09dac4778d944349","modified":1629442423978},{"_id":"themes/butterfly/source/css/_tags/note.styl","hash":"86fee274a62f7f034547342930f445c47378eb55","modified":1629442423978},{"_id":"themes/butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1629442423978},{"_id":"themes/butterfly/source/js/search/algolia.js","hash":"65b45e61586f7e66c3f338370bfd9daadd71a4b7","modified":1629442423978},{"_id":"themes/butterfly/source/js/search/local-search.js","hash":"459e2541afda483916d16fce4aaa56b41bcd42ba","modified":1629442423978},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"d85c3737b5c9548553a78b757a7698df126a52cf","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"7848ec58c6ec03243abf80a3b22b4dc10f3edf53","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"e3bf847553515174f6085df982f0623e9783db7a","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"ef1b2b5b980d6aeaa5d06b97d1afc9644b155a16","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"400ce038548d6f9ddb486150c724c87b6923a88b","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"bba9871f446c10ffcc8fa9023f5a2eb701a86bae","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/chat/chatra.pug","hash":"481cd5053bafb1a19f623554a27d3aa077ea59c3","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/chat/crisp.pug","hash":"76634112c64023177260d1317ae39cef2a68e35f","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"cfe63e7d26a6665df6aa32ca90868ad48e05ec04","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/chat/gitter.pug","hash":"d1d2474420bf4edc2e43ccdff6f92b8b082143df","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/chat/index.pug","hash":"3f05f8311ae559d768ee3d0925e84ed767c314d3","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/chat/tidio.pug","hash":"24a926756c2300b9c561aaab6bd3a71fdd16e16d","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqus.pug","hash":"a111407fdcafcf1099e26ffa69786f8822c5d9fb","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"2e52c64e89f16267596a8465841dd46f51820982","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"c46a932257212f82e4a9974fbbc5de8878c8b383","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"0b7571919e8ad51285deda56a1868fccf8c563d7","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/index.pug","hash":"da9813f8dc0d388869c15413cf056012cfb69e1a","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/js.pug","hash":"bafb3d5710824caa59a56017afb058fd2b4eac65","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/livere.pug","hash":"52ea8aa26b84d3ad38ae28cdf0f163e9ca8dced7","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"16378d8646ea3f4ac99c18f0296dd85b13f9d775","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/utterances.pug","hash":"b871ea208e36398b4d668db9a9a0b61c79415381","modified":1629442423970},{"_id":"themes/butterfly/layout/includes/third-party/comments/valine.pug","hash":"2b45fe09d5b591dca156b76dae99981f8d8e1c61","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/comments/waline.pug","hash":"36f3c603d2a2ecddaa6d2675a89d76ad94968f72","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/math/katex.pug","hash":"f9b00ead54573ba6e6eb33481588af144aab648d","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/math/mathjax.pug","hash":"a47d8f9f593091cc91192c0c49deaa2c0d2317fd","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/math/mermaid.pug","hash":"3f3a3cd8bea2103dedd754f767aca5cb84d5f586","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"b443d6b16baf3ea250041342cc0361a42a412b7f","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"34088a15655704d12e9b1807b47b3f6a860c9eec","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"f6506ccfd1ce994b9e53aa95588d0b6dbad11411","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"cb38ffe911023092a90a28f2ba8317a92b22cd0c","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"59b4c26a827ace5a54855881d199977103ff6f50","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"a2bc2601b7e0ae5caf1fc51a07390562d928620a","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/search/algolia.pug","hash":"d8f59e94eafc669c49349561dc5bbea3915aecb7","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/search/index.pug","hash":"da3b9437d061ee68dbc383057db5c73034c49605","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/search/local-search.pug","hash":"613280d61b8ab9612014ec016ae3d3698d36fd1a","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/share/add-this.pug","hash":"2980f1889226ca981aa23b8eb1853fde26dcf89a","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/share/addtoany.pug","hash":"309f51bc5302e72fc469d54c577fbcfe57fb07a8","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/share/index.pug","hash":"4c4a9c15215ae8ac5eadb0e086b278f76db9ee92","modified":1629442423974},{"_id":"themes/butterfly/layout/includes/third-party/share/share-js.pug","hash":"006acc91ce25fc7c7d778ca043e970f57dc46b83","modified":1629442423974},{"_id":"themes/butterfly/source/css/_highlight/highlight/diff.styl","hash":"8c0726fb8d9a497d2f900b0be2845efaa68e3d87","modified":1629442423974},{"_id":"themes/butterfly/source/css/_highlight/highlight/index.styl","hash":"89cbcc8e087788ecec18b5fa58710afacdb7d080","modified":1629442423978},{"_id":"themes/butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1629442423978},{"_id":"themes/butterfly/source/css/_highlight/prismjs/index.styl","hash":"e0e7065124ef0d99f8322a47bc47838982e04ad0","modified":1629442423978},{"_id":"themes/butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"8970cc1916c982b64a1478792b2822d1d31e276d","modified":1629442423978},{"_id":"public/2021/08/10/Pytorch-Embedding/index.html","hash":"b0aa4d62306f72324224229ff556f8ec78342731","modified":1630074889035},{"_id":"public/2021/08/10/Pytorch-Dataloader/index.html","hash":"613567f7443ca1b37ad51e0920608419d712a1eb","modified":1630074889035},{"_id":"public/2021/08/06/surprise/index.html","hash":"0f85026cb2e0580be8d711fb699a1ae7f8f5b107","modified":1630074889035},{"_id":"public/2021/08/09/deep-learning-part-1/index.html","hash":"4ed768b096dc2fb1a22e5c911ee7d32cc8a4d313","modified":1630074889035},{"_id":"public/2021/08/02/bag/index.html","hash":"3d0cae5ca3ce9f85d7a5b9347f92b16fc282216e","modified":1629469621349},{"_id":"public/2021/08/11/Exceptional-Control-Flow/index.html","hash":"ef60e3436ab0a855ef2debaeb2cf034a522a2d4b","modified":1630074889035},{"_id":"public/2021/08/11/Concurrent-Programming/index.html","hash":"a5c1c8bc60ab03068d01fb384ab7e09affa0db9a","modified":1630074889035},{"_id":"public/2021/08/10/System-Level-I-O/index.html","hash":"17ad71385ec1b43cdf5e94ee1ac61e32ed72251e","modified":1630074889035},{"_id":"public/2021/08/02/Network-Programming/index.html","hash":"d51e97afe98f2a6f5492f160306eafff160f7e48","modified":1630074889035},{"_id":"public/archives/page/2/index.html","hash":"39f8829d9d4366074a7242e699dc8b59100ce924","modified":1630075014154},{"_id":"public/archives/2021/page/2/index.html","hash":"81df8ef9e3ae3c61908c2c584e4ef8ddbd8eb8fd","modified":1630075014154},{"_id":"public/page/2/index.html","hash":"400574f96e95917e1b745e6c9bfa16c95fe5b55d","modified":1630075014154},{"_id":"public/img/algolia.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1629443147111},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1629443147111},{"_id":"public/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1629443147111},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1629443147111},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1629443147111},{"_id":"public/js/search/algolia.js","hash":"65b45e61586f7e66c3f338370bfd9daadd71a4b7","modified":1629443147111},{"_id":"public/js/utils.js","hash":"8319b59c26ce8cd2b0ae7d030c4912215148fa92","modified":1629443147111},{"_id":"public/js/search/local-search.js","hash":"459e2541afda483916d16fce4aaa56b41bcd42ba","modified":1629443147111},{"_id":"public/css/index.css","hash":"4e5d9ead0952d31f549bbc22765a615a4540f58b","modified":1629443147111},{"_id":"public/js/main.js","hash":"8ef2821ceb92d81aa0f8c02ee932f094adcafb2b","modified":1629443147111},{"_id":"public/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1629443147111},{"_id":"source/tags/index-1.md","hash":"1ec061485a8832db0cfcab2ba28f04cbceed10b4","modified":1629443245054},{"_id":"public/tags/index-1.html","hash":"b69df7b762e60b55d900f01bc2df10782cdf51e5","modified":1630075014154},{"_id":"source/categories/index.md","hash":"e7e4d88cfeeb37a9138654560f8f0f96b16a5e45","modified":1629443400617},{"_id":"source/link/index.md","hash":"825b112bdcb75221d1aa7e37dbc97930fbac4bc0","modified":1629443435090},{"_id":"public/categories/index.html","hash":"6c4a641de2908ccc686ff41911d57aa5de7d9a15","modified":1630075014154},{"_id":"public/link/index.html","hash":"6d68d3493427014a82b3546a985523f49d1701e5","modified":1630075014154},{"_id":"themes/butterfly/._config.yml.swp","hash":"eebef90b2d360f83ac9e3496ed442e89028df373","modified":1629466669565},{"_id":"themes/butterfly/source/img/sea.jpg","hash":"2be9e7b00b47eee6ef96f1b5a1b22ae5903731bc","modified":1629446640777},{"_id":"public/img/sea.jpg","hash":"2be9e7b00b47eee6ef96f1b5a1b22ae5903731bc","modified":1629468100809},{"_id":"themes/butterfly/source/img/pic2.png","hash":"75a224ec7914d55f0eb5ac5312f628b9f1808e4c","modified":1629469090877},{"_id":"themes/butterfly/source/img/pic1.png","hash":"ecff0b79a3ba0098a206c0b0f7e93f172e112b07","modified":1629469039122},{"_id":"themes/butterfly/source/img/pic3.png","hash":"f328f17e3fdfed6230bfd9acd77e6046efad3c1e","modified":1629469136680},{"_id":"public/img/pic2.png","hash":"75a224ec7914d55f0eb5ac5312f628b9f1808e4c","modified":1629469176791},{"_id":"public/img/pic1.png","hash":"ecff0b79a3ba0098a206c0b0f7e93f172e112b07","modified":1629469176791},{"_id":"public/img/pic3.png","hash":"f328f17e3fdfed6230bfd9acd77e6046efad3c1e","modified":1629469176791},{"_id":"public/categories/深入理解计算机系统/index.html","hash":"6cef4336722fdea879ea52a0d5fdb883e0161ec3","modified":1630075014154},{"_id":"public/tags/异常控制流/index.html","hash":"d6361266f836075f91b67bdb8f6e8cdb62b42dca","modified":1630075014154},{"_id":"public/tags/Linux文件/index.html","hash":"1ca6a17576553ec569410d59022c576218d5bd5a","modified":1630075014154},{"_id":"public/2021/08/20/bag/index.html","hash":"75a161fd5378c8ae94a5a8119c1f4965b902ef63","modified":1630074889035},{"_id":"public/tags/汇编/index.html","hash":"ee1e6a8b8927864eeff602008a40ee9264db3e60","modified":1630075014154},{"_id":"public/tags/计算机的位级理解/index.html","hash":"6913f3a585c7a6cd38776fdc11a3d1217bfab0e0","modified":1630075014154},{"_id":"public/tags/网络编程/index.html","hash":"0a777e2e509cd2f0869ce91b335d009852c2ae27","modified":1630075014154},{"_id":"public/tags/并发/index.html","hash":"101e0b0f93a1dba673e9f95d7db195e1e7a404ae","modified":1630075014154},{"_id":"source/_posts/Exceptional-Control-Flow/exceptions.jpg","hash":"c69e3aede08b8876171624f8d8bcb6a645127247","modified":1629479205617},{"_id":"public/2021/08/11/Exceptional-Control-Flow/exceptions.jpg","hash":"c69e3aede08b8876171624f8d8bcb6a645127247","modified":1629480167665},{"_id":"source/_posts/异常控制流.md","hash":"f03591144cb174155b372767b9303409d65c8834","modified":1629479899830},{"_id":"source/_posts/异常控制流/IMG_0021.jpg","hash":"c69e3aede08b8876171624f8d8bcb6a645127247","modified":1629479603094},{"_id":"public/2021/08/11/异常控制流/index.html","hash":"05e4d85180597465107389d7bd40cc7516c8f93a","modified":1629480167665},{"_id":"public/2021/08/11/异常控制流/IMG_0021.jpg","hash":"c69e3aede08b8876171624f8d8bcb6a645127247","modified":1629479611901},{"_id":"source/_posts/异常控制流/exceptions.jpg","hash":"c69e3aede08b8876171624f8d8bcb6a645127247","modified":1629479603094},{"_id":"public/2021/08/11/异常控制流/exceptions.jpg","hash":"c69e3aede08b8876171624f8d8bcb6a645127247","modified":1629479634973},{"_id":"public/archives/2021/08/page/2/index.html","hash":"0366a55b44c616179457734723777d7c51a5a434","modified":1630075014154},{"_id":"source/_posts/Exceptional-Control-Flow/process.jpg","hash":"c643a6ea336a6f8cd3f87ecfe33bbca700699d9e","modified":1629778379157},{"_id":"source/_posts/Exceptional-Control-Flow/context_switch.jpg","hash":"e866e2bb1f577311c0f17aba1b74d4b80202051e","modified":1629777952405},{"_id":"public/2021/08/11/Exceptional-Control-Flow/process.jpg","hash":"c643a6ea336a6f8cd3f87ecfe33bbca700699d9e","modified":1629778455855},{"_id":"public/2021/08/11/Exceptional-Control-Flow/context_switch.jpg","hash":"e866e2bb1f577311c0f17aba1b74d4b80202051e","modified":1629778455855},{"_id":"source/_posts/Exceptional-Control-Flow/signal_received.jpg","hash":"49e4076e1e37336ea05fc72a553190d5b6d5c2e0","modified":1629787905623},{"_id":"source/_posts/Exceptional-Control-Flow/signal_handler.jpg","hash":"f3331dc4faaef62811b23906cd99c6fd9629b76b","modified":1629787908831},{"_id":"source/_posts/Exceptional-Control-Flow/signal_type.jpg","hash":"f36df5772322e52c57fb6a0305914bda379512b2","modified":1629788767106},{"_id":"public/2021/08/11/Exceptional-Control-Flow/signal_received.jpg","hash":"49e4076e1e37336ea05fc72a553190d5b6d5c2e0","modified":1629788862041},{"_id":"public/2021/08/11/Exceptional-Control-Flow/signal_handler.jpg","hash":"f3331dc4faaef62811b23906cd99c6fd9629b76b","modified":1629788862041},{"_id":"public/2021/08/11/Exceptional-Control-Flow/signal_type.jpg","hash":"f36df5772322e52c57fb6a0305914bda379512b2","modified":1629788862041},{"_id":"source/_posts/Exceptional-Control-Flow/race.jpg","hash":"b0417d6d3e553d3611e1ee8ba7ad090cc9335813","modified":1629793946109},{"_id":"public/2021/08/11/Exceptional-Control-Flow/race.jpg","hash":"b0417d6d3e553d3611e1ee8ba7ad090cc9335813","modified":1629793977584},{"_id":"source/_posts/Exceptional-Control-Flow/guidelines.jpg","hash":"1ab8961867de03b8f902e31c51f5be33d6356223","modified":1629794403377},{"_id":"public/2021/08/11/Exceptional-Control-Flow/guidelines.jpg","hash":"1ab8961867de03b8f902e31c51f5be33d6356223","modified":1629794474245},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/stack_cr.jpg","hash":"c42948d11c707c90d278bf53e58af0d381ca03f3","modified":1629873330455},{"_id":"public/2021/08/01/Machine-Level-Representation-of-Programs/stack_cr.jpg","hash":"c42948d11c707c90d278bf53e58af0d381ca03f3","modified":1629873375882},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/jump_table.jpg","hash":"35c81a98163789cb07f99828db52c40baaa5fe6d","modified":1629877820723},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/save.jpg","hash":"2c18ff2108ab2907b9aaa21829961bcdf8be375d","modified":1629877777004},{"_id":"public/2021/08/01/Machine-Level-Representation-of-Programs/jump_table.jpg","hash":"35c81a98163789cb07f99828db52c40baaa5fe6d","modified":1629877851901},{"_id":"public/2021/08/01/Machine-Level-Representation-of-Programs/save.jpg","hash":"2c18ff2108ab2907b9aaa21829961bcdf8be375d","modified":1629877851901},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/caller_or_callee_saved.jpg","hash":"2c18ff2108ab2907b9aaa21829961bcdf8be375d","modified":1629877777004},{"_id":"public/2021/08/01/Machine-Level-Representation-of-Programs/caller_or_callee_saved.jpg","hash":"2c18ff2108ab2907b9aaa21829961bcdf8be375d","modified":1629877953450},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/saved.jpg","hash":"2c18ff2108ab2907b9aaa21829961bcdf8be375d","modified":1629877777004},{"_id":"public/2021/08/01/Machine-Level-Representation-of-Programs/saved.jpg","hash":"2c18ff2108ab2907b9aaa21829961bcdf8be375d","modified":1629878141814},{"_id":"source/_posts/System-Level-I-O/table.jpg","hash":"96c3ccad208b772c000357d2382938bce55653d4","modified":1629905008794},{"_id":"public/2021/08/10/System-Level-I-O/table.jpg","hash":"96c3ccad208b772c000357d2382938bce55653d4","modified":1629905069496},{"_id":"source/_posts/System-Level-I-O/info.jpg","hash":"9f3647987fb1eb84ea41daee48c8bb17a86dd472","modified":1629905266712},{"_id":"public/2021/08/10/System-Level-I-O/info.jpg","hash":"9f3647987fb1eb84ea41daee48c8bb17a86dd472","modified":1629905294656},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch.md","hash":"350c073ec5209b8a1f5f626af8c76a3ff6a06a67","modified":1629993010096},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/index.html","hash":"228eb14536cbc3e78b38205e6b0e8a19563de1d3","modified":1630074889035},{"_id":"public/categories/深度学习/index.html","hash":"8644e3ba65bbaaf62fc7bb275ab21f5ad56870c2","modified":1630075014154},{"_id":"public/tags/pytorch深度学习的一般流程/index.html","hash":"3249a5cb598145514571955de674e6ebd3f938a8","modified":1630075014154},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/1.png","hash":"1e6038d4493edae42e9fbbff5aa396ee99160854","modified":1629974819318},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/3.png","hash":"f2ae59555f575d4dab63c579ec6c21cc1f894639","modified":1629974835302},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/2.png","hash":"954da0b669ab6f1d4701e142e08509a7dab1924a","modified":1629974816758},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/5.png","hash":"9ef59856a2aa62f805429dd7bd4a84cfdfdc8565","modified":1629974809654},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/4.png","hash":"a93beb28d8843712443ba7fc218c604906e79a0a","modified":1629974821282},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/6.png","hash":"f023d9e5960180f64aa6515e460d8d49989d437f","modified":1629974839582},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/8.png","hash":"bf9cbbb9cd2ea45e3f7908447d0796d05c9e5208","modified":1629974832890},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/7.png","hash":"7a2c96f2733b7274fa5e0d7d81c58a127c7082c4","modified":1629974837662},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/init.png","hash":"b7b0c8096ef9307af7d99d03cad08703711d9cec","modified":1629974806358},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/1.png","hash":"1e6038d4493edae42e9fbbff5aa396ee99160854","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/3.png","hash":"f2ae59555f575d4dab63c579ec6c21cc1f894639","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/2.png","hash":"954da0b669ab6f1d4701e142e08509a7dab1924a","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/4.png","hash":"a93beb28d8843712443ba7fc218c604906e79a0a","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/5.png","hash":"9ef59856a2aa62f805429dd7bd4a84cfdfdc8565","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/6.png","hash":"f023d9e5960180f64aa6515e460d8d49989d437f","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/init.png","hash":"b7b0c8096ef9307af7d99d03cad08703711d9cec","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/7.png","hash":"7a2c96f2733b7274fa5e0d7d81c58a127c7082c4","modified":1629992655453},{"_id":"public/2021/08/26/deeplearning-pipeline-for-pytorch/8.png","hash":"bf9cbbb9cd2ea45e3f7908447d0796d05c9e5208","modified":1629992655453},{"_id":"source/_posts/Convolution.md","hash":"59e8cc1117c365426be88aec5d9fe2e6bcbc3bcd","modified":1630075009148},{"_id":"public/2021/08/27/Convolution/index.html","hash":"4d91e53b80a4b079d7f61c4bb2c0e1849bd070ac","modified":1630075014154},{"_id":"public/tags/卷积/index.html","hash":"c1c8ada8fecb4fc2e5da1afc797dccf403d7ac82","modified":1630075014154},{"_id":"source/_posts/Convolution/c.gif","hash":"81da87f6108f57be95eefaf713da44ca7927ebaa","modified":1630035976595},{"_id":"public/2021/08/27/Convolution/c.gif","hash":"81da87f6108f57be95eefaf713da44ca7927ebaa","modified":1630036010003},{"_id":"source/_posts/models-summary.md","hash":"9f5371ae47a9bf30b3b09ba12e8f68dc732a7f34","modified":1630074884333},{"_id":"source/_posts/models-summary/GAN.png","hash":"02b6c9d6185888dbf52aa0b5d9b7c28ba46c3f6c","modified":1630055766560},{"_id":"source/_posts/models-summary/RNN.png","hash":"3246705981cc4972f8fddf04669e102a674d6fb4","modified":1630055308387},{"_id":"source/_posts/models-summary/coditionGAN.png","hash":"c05ac9cd7e34afb65368640412f441c3cdc8580f","modified":1630055850057},{"_id":"source/_posts/models-summary/dsc.png","hash":"1b6542fce004a848da74d7b49a0dc4dd6c96494c","modified":1630056489251},{"_id":"source/_posts/models-summary/resnet.png","hash":"cbd483134548a4bb57cf7d73caff8607c0073f85","modified":1630056150206},{"_id":"source/_posts/models-summary/LSTM.png","hash":"0cfe0d80b0a40e6d008b389632da569e816b9f3c","modified":1630055467729},{"_id":"source/_posts/models-summary/CycleGAN.png","hash":"1ae9fa1e8e29c18edb8d6c188b2ea23f42b7f70c","modified":1630056079798},{"_id":"source/_posts/models-summary/pw.png","hash":"40130a48af94585d693407a687605ce1822bb33f","modified":1630056401335},{"_id":"source/_posts/models-summary/BiRNN.png","hash":"5a06ed8da88f654273cd152b8ed05ae4d856a3fa","modified":1630055412323},{"_id":"source/_posts/models-summary/dw.png","hash":"559c76f6b69fde77a7955e7ffe0c5c85d7679f10","modified":1630056344028},{"_id":"source/_posts/models-summary/c.png","hash":"1e625d8dbced3a37220ed8b9a587a5e202981c6f","modified":1630055237949},{"_id":"public/2021/08/27/models-summary/index.html","hash":"92848cde48327d643d6c6921fb5de79beebf30cb","modified":1630074889035},{"_id":"public/tags/模型总结/index.html","hash":"175ea28819d8fa692c5f7e3435a18488a6273891","modified":1630075014154},{"_id":"public/2021/08/27/models-summary/RNN.png","hash":"3246705981cc4972f8fddf04669e102a674d6fb4","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/coditionGAN.png","hash":"c05ac9cd7e34afb65368640412f441c3cdc8580f","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/GAN.png","hash":"02b6c9d6185888dbf52aa0b5d9b7c28ba46c3f6c","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/resnet.png","hash":"cbd483134548a4bb57cf7d73caff8607c0073f85","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/dsc.png","hash":"1b6542fce004a848da74d7b49a0dc4dd6c96494c","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/LSTM.png","hash":"0cfe0d80b0a40e6d008b389632da569e816b9f3c","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/CycleGAN.png","hash":"1ae9fa1e8e29c18edb8d6c188b2ea23f42b7f70c","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/pw.png","hash":"40130a48af94585d693407a687605ce1822bb33f","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/BiRNN.png","hash":"5a06ed8da88f654273cd152b8ed05ae4d856a3fa","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/dw.png","hash":"559c76f6b69fde77a7955e7ffe0c5c85d7679f10","modified":1630056549172},{"_id":"public/2021/08/27/models-summary/c.png","hash":"1e625d8dbced3a37220ed8b9a587a5e202981c6f","modified":1630056549172},{"_id":"source/_posts/models-summary/conditionGAN.png","hash":"c05ac9cd7e34afb65368640412f441c3cdc8580f","modified":1630055850057},{"_id":"source/_posts/models-summary/dsc2.png","hash":"04d520eb3308735fc3cb5f688fd5f12935b84eda","modified":1630074506963},{"_id":"source/_posts/models-summary/pw2.png","hash":"0f695f22966c9a3a59227368efa6d0698d3078b6","modified":1630074631724},{"_id":"public/2021/08/27/models-summary/dsc2.png","hash":"04d520eb3308735fc3cb5f688fd5f12935b84eda","modified":1630074744597},{"_id":"public/2021/08/27/models-summary/pw2.png","hash":"0f695f22966c9a3a59227368efa6d0698d3078b6","modified":1630074744597},{"_id":"public/2021/08/27/models-summary/conditionGAN.png","hash":"c05ac9cd7e34afb65368640412f441c3cdc8580f","modified":1630074744597}],"Category":[{"name":"深入理解计算机系统","_id":"ckskg3dm70000i7kk03tmd7su"},{"name":"深度学习","_id":"ckst2t1380001ffkk4sdc7viy"}],"Data":[],"Page":[{"title":"Series","layout":"series","_content":"","source":"series/index.md","raw":"---\ntitle: Series\nlayout: series\n---\n","date":"2021-07-28T13:14:59.732Z","updated":"2021-07-28T13:14:59.732Z","path":"series/index.html","comments":1,"_id":"ckrnigxdt00003jkk0i789tj5","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Project","layout":"project","_content":"","source":"project/index.md","raw":"---\ntitle: Project\nlayout: project\n---\n","date":"2021-07-28T13:14:59.732Z","updated":"2021-07-28T13:14:59.732Z","path":"project/index.html","comments":1,"_id":"ckrnigxdv00013jkk9hu87u6u","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","layout":"tags","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\nlayout: tags\ntype: \"tags\"\n---\n","date":"2021-08-20T07:08:12.851Z","updated":"2021-08-20T07:08:12.843Z","path":"tags/index.html","_id":"ckrnigxdw00023jkk9ywr76g0","comments":1,"content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg","excerpt":"","more":""},{"title":"About","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: About\nlayout: about\n---","date":"2021-07-28T13:14:59.732Z","updated":"2021-07-28T13:14:59.732Z","path":"about/index.html","comments":1,"_id":"ckrnigxdw00033jkk62bg5kjg","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2021-08-20T07:07:25.000Z","_content":"","source":"tags/index-1.md","raw":"---\ntitle: tags\ndate: 2021-08-20 15:07:25\n---\n","updated":"2021-08-20T07:07:25.054Z","path":"tags/index-1.html","_id":"cksk0fqoq00002jkk3awuaa67","comments":1,"layout":"page","content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg","excerpt":"","more":""},{"title":"categories","date":"2021-08-20T07:09:36.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-08-20 15:09:36\ntype: \"categories\"\n---\n","updated":"2021-08-20T07:10:00.617Z","path":"categories/index.html","_id":"cksk0ijxd0000chkkh4bz48n6","comments":1,"layout":"page","content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg","excerpt":"","more":""},{"title":"link","date":"2021-08-20T07:10:16.000Z","_content":"","source":"link/index.md","raw":"---\ntitle: link\ndate: 2021-08-20 15:10:16\n---\n","updated":"2021-08-20T07:10:16.270Z","path":"link/index.html","_id":"cksk0jemx0001chkk8pmgbxwm","comments":1,"layout":"page","content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg","excerpt":"","more":""}],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2021-07-28T04:36:40.146Z","updated":"2021-07-28T04:36:40.146Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrngl2cm00001okk5eqs7drl","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"Bits & Intergers & Floating Points","date":"2021-07-28T14:01:50.000Z","mathjax":true,"cover":"/img/pic3.png","_content":"\n# Bits & Interters & Floating Points\n\n## Bit\n计算机以二进制理解世界，也就是一串0和1，重要的是bit representation（二进制表示）以及 decoding（解读方式)。\n以文件的传输为例，ASCII文件中的文字首先被编码成二进制形式，然后发送给接收者，接收端以一定的规则解码得到原来的文件。\n```\n发送者 (encoding) ------> 01100110 (decoding)------> 接收者\n```\ndecoding方式带来的影响：\n```\n以4-bit的data type 为例\n1111 --       int     --> -1\n1111 --  unsigned int --> 15\n```\n由于 int 采用补码形式表示，而usigned不是导致值不同（后文详解）\n\n### Data Size\n1 bit 即是1个0或1，8 bits == 1 byte, 16 bits == 2 bytes == 1 words\nword size 就是指针所占的字位数\n\n### Address\n当我们谈到计算机的内存与地址时，一般指的是虚拟内存，每一bit都有它的地址\n### Byte Ordering\n既然bit都有自己的地址，那么连续的bits的地址一定是连续的，它们存储方式有两种big endian, little endian。\n```\naddress        0x100  0x101  0x102 0x103\nbig endian        01     23     45    67\nlittle endian     67     45     23    01\n```\nbig endian 即是大地址结尾，而little endian 即是以小地址结尾。\n在我的机子上，其以little endian方式存储，可以用以下的C代码验证\n```C\n#include <stdio.h>\n\nvoid show_bytes(char* c_p, size_t n)\n{i\n\t\tfor(size_t i = 0; i<n; i++)\n\t\t{\n\t\t\t\tprintf(\"%X \", *c_p);\n\t\t\t\tc_p++;\n\t\t}\n\t\tputs(\"\");\n}\n\nint main(void)\n{\n\tint m = 0x12345678;\n\tprintf(\"%X\\n\", m);\n\tshow_bytes((char*)(&m), sizeof(int));\n\t\n\treturn 0;\n}\n```\n\n```bash\n./endian\n12345678\n78 56 34 12  \n```\n\n了解大小端的意义如下：\n- 网络传输大多用big-endian, 而计算机一般用little-endian\n- 汇编、机器码的顺序\n- C中的cast操作，如union可能带来问题\n\n### String (ASCII)\n由于ASCII的范围0~127，每一次都读取一个byte，所以不受大小端的影响\n\n---\n\n## Intergers\n\n\n### Unsigned & Two's Complement\n\n假设有一个data的bit vector, [w<sub>k-1</sub>, w<sub>k-2</sub> .. w<sub>0</sub>], unsigned : 求和每一位的2的次幂乘以w<sub>x</sub>, Two's Complement ： 第一位为符号位，其拥有负权重2<sub>k-1</sup>。\n\n这也即当符号为为1时，其值为负而0为正的原因：最大的负数 -1 ： 2<sub>k-1</sup> + (2<sub>k-1</sup> -1)，同时记U至T的转换为U2T(x) = x<sub>t</sub> - w<sub>k-1</sub> * 2<sub>k</sup>\n\n---\n\n### Bit-Level Operations\n&（位与） |（位或）^(异或) >>（右移） <<(左移)\t~(取补码)\n```\n&  0 1       |  0 1     ^ 0 1    ~ 0 1\n0  0 0       0  0 1     0 0 1      \n1  0 1       1  1 1     1 1 0      1 0\n```\n\n#### 关于 &,&&　　|,|| \n当逻辑运算符&&，||操作的对象为一个bit时，相当于&, |\n\n#### 关于~\n**x　+　~x+1　=　0**　也即~x+1为x的加法逆元\n\n#### 关于 ^\n* 用于Floating Points符号位的计算较为方便（NaN除外）, 相当于求其加法逆元(Integers用~x+1方便)\n* x^x = 0\n\n####  左移　右移\n左移、右移顾名思义就是将bit vector向左、右移动，多出来的bit自然舍去，问题在于缺少的bit位是用0/1来补，而这在U/T的表现又不同\n\nU/T 左移 ： 用0补齐缺少位，称为逻辑左移\n\nU 右移 ： 用0补齐缺少位，称为逻辑右移\n\nT 右移 ： 补齐位与符号位一致，可以为0/1，称为算术右移\n　\n左移k位，所有对应位的权重都增大k,相当于乘以 2<sub>k</sup>  （int符号位虽然是负权重，但是结果相同）\n\n右移k位，所有对应位的权重都减少k,相当于除以 2<sub>k</sup> ,需要注意的是结果需要**Rounding**(取整)。C中的整数取整都是向0取整，而应用>>时正数：向下取整，也即向0取整，而对于负数来说，其依然是向下取整，所有需要加上相应的bias,使其满足向0取整。bias = 1 << k-1\n\n### Conversion\n转变分为三种\n- int 与 unsigned int\n- 小字节类型转大字节类型\n- 大字节类型转小字节类型\n\nint 与 unsigned int 的转换在C中并不改变其bit-represention,只是改变其解读形式。\n\n大字节转小字节则是要截断高位。\n\n小字节转大字节，则需要拓展高位，这其中又分为sign expansion && zero expansion。\n\n　　首先考虑U-U的小转大，采用零拓展，即高位全用0填补。\n　　T-T的拓展，采用符号拓展，即高位拓展取决与符号为。\n　　那么U-T,T-U的小字节转大字节是怎么实现的呢？规律是先变size再改变类型。也就是其拓展与改变前的拓展类型一致，举例如下：\n```C\n#include <stdio.h>\n\nint main(void)\n{\n\tunsigned short int s_u = 0xcfff;\n\tshort int s_t = 0xcfff;\n\tint i_u = (int)s_u;\n\tint i_t = (int)s_t;\n\tprintf(\"i_u : %x\\n\", i_u);\n\tprintf(\"i_t : %x\\n\", i_t);\n\treturn 0;\n}\n```\n```bash\ni_u : cfff\ni_t : ffffcfff\n```\n\n### Integers Arithmetic\n\n这些运算以及数据本身构成了群，如无整型与加法构成了无整型加群，在这里引入群中的一个概念，加法逆元，若a+b=0，则称b为a的逆元。\n\n#### Modular addition\n加法其实是Modualr addition,即对最后的结果取模。\n```C\n        U                    T\n\t x=0:    x          x=Tmin: Tmin \n-x                         \n\t x>0:   2**w - x    x>Tmin: -x\n\n\n溢出检测：\n     !(x + y < x)       !(x<0 && y<0 && sum >=0) && !(x>=0 && y>=0 && sum<0) (OR !((x<0 == y<0) && (sum<0 != x<0)))\n```\n加法逆元的C表达式：\n> ~x+1\n\n#### Muplication\nunsigned 计算时同样时将结果mod 2<sub>w</sub>, signed计算则是先将其转为unsigned计算，最后转回signed。这也同样表明了两种类型的乘法运算结果truncted后的bit-representation是一样的。\n\n溢出检测的两种方式：\n```C\n///1.群的性质\nint tmult_ok(int x, int y)\n{\n\tint p = x*y;\n\treturn !x || p/x == y;\n}\n\n///2.用更大的数位表示\nint tmult_ok(int x, int y)\n{\n\tin64_t pll = (in64_t)x*y;\n\treturn pll == (int)pll;\n}\n\n```\n> Tips : size_t被定义为unsigned类型，而malloc接收一个size_t类型，那么溢出后得到一个较小的整数的话是无法被发现的，只能在malloc前对其siz做溢出检测。\n\n## Floating Point\n首先给出浮点数的一般表达式\n\n$$\nv = (-1)^s*M*2^E\n$$\n浮点数的bit-representation如下\n```\n单精度\n|s|            exp             |         frac              |\n31 30                        23 22                         0\n双精度\n|s|            exp                    |                   frac                                  |\n63 62                               52 51                                                       0\n```\n由sign, exp, frac三部分组成，sign即是符号位，exp参与E的表达，fac参与小数部分的表达，具体表达式分为Normalized values, Denormalized values, Special values情况。其中Denormalized vaules 表达的是接近0的数，Special values表示的是无穷，以及NaN(Not a Number)的情况，Normalized values 表示的是除上述两种情况外的一般的数。\n\n以单精度为例\n```\nNormalized values                  exp !=0 && exp !=255\nM = 1+f  E = e - bias   e is the value of the bit representation (exp)\nDenormalized values\nM = f    E = 1 - bias\nSpecial values\n1. Infinite\nexp = all 1  f = 0\n2. NaN\nexp = all 1 f != 0\n```\nbias = $2^(k-1) - 1$, 127 for single precision and 1023 for double。So the exponent ranges form -126~127 for single precision and -1022~1023 for doulbe.\n采用这中方式编码的好处：如果采用类似U/T的编码方式，那么会丢失精度，而且所表达的范围也会变小\n\n### Floating Points Arithmetic\n\n#### Addition\n两步走：1.向较大的E（指数位）对齐 2.相加再调整M、E。\n```\n   |         (-1)^s1*M1             |\n          |      (-1)^s2*M2                       |\n\t\t                            |----E1-E2----|\n```\n显然当大数+小数时，小数的精度丢失比较严重,所以不满足结合律。而且加法满足单调性，a>b => a+x > b+x, 只要x不为NaN即可。\n加法的性质：\n- 闭合（可能产生无穷/NaN）\n- 满足交换律\n- 不满足结合律\n- 存在逆元\n- 满足单调性\n\n#### Muplication\n- 满足交换律\n- 不满足结合律\n- 不满足乘法分配率\n- 满足单调性\na>=b and c>=0  => ac>=bc, c<=0时亦然，只要c不为NaN即可。\n\n#### compare to integer\ninteger的运算满足交换律，结合律，分配律，但不满足单调性的原理。Floating Point的运算满足交换律，不满足分配律、结合律，但是满足单调性原理。\n\n\n> Tips:关于FP不满足结合律的问题：自然界中的问题很多都是连续的，也就是说在某些状况下是不会出现极大值与极小值运算的情形，也就不用考虑不满足结合率带来的影响，但是在其他的情景下，如金融中是可能出现的。这也提醒我们要时刻把握具体情景，从需求出发。\n\n#### conversion\nfloat int double 之间的转变是改变位表示的，double/float cast 为 int是需要向0取整（NaN可能转为Tmin），int cast to double 不会出现问题，int cast to float可能要round。\n\n\n","source":"_posts/Bits-Intergers-Floating-Points.md","raw":"---\ntitle: Bits & Intergers & Floating Points\ndate: 2021-07-28 22:01:50\ntags: 计算机的位级理解\ncategories: 深入理解计算机系统\nmathjax: true\ncover:\n---\n\n# Bits & Interters & Floating Points\n\n## Bit\n计算机以二进制理解世界，也就是一串0和1，重要的是bit representation（二进制表示）以及 decoding（解读方式)。\n以文件的传输为例，ASCII文件中的文字首先被编码成二进制形式，然后发送给接收者，接收端以一定的规则解码得到原来的文件。\n```\n发送者 (encoding) ------> 01100110 (decoding)------> 接收者\n```\ndecoding方式带来的影响：\n```\n以4-bit的data type 为例\n1111 --       int     --> -1\n1111 --  unsigned int --> 15\n```\n由于 int 采用补码形式表示，而usigned不是导致值不同（后文详解）\n\n### Data Size\n1 bit 即是1个0或1，8 bits == 1 byte, 16 bits == 2 bytes == 1 words\nword size 就是指针所占的字位数\n\n### Address\n当我们谈到计算机的内存与地址时，一般指的是虚拟内存，每一bit都有它的地址\n### Byte Ordering\n既然bit都有自己的地址，那么连续的bits的地址一定是连续的，它们存储方式有两种big endian, little endian。\n```\naddress        0x100  0x101  0x102 0x103\nbig endian        01     23     45    67\nlittle endian     67     45     23    01\n```\nbig endian 即是大地址结尾，而little endian 即是以小地址结尾。\n在我的机子上，其以little endian方式存储，可以用以下的C代码验证\n```C\n#include <stdio.h>\n\nvoid show_bytes(char* c_p, size_t n)\n{i\n\t\tfor(size_t i = 0; i<n; i++)\n\t\t{\n\t\t\t\tprintf(\"%X \", *c_p);\n\t\t\t\tc_p++;\n\t\t}\n\t\tputs(\"\");\n}\n\nint main(void)\n{\n\tint m = 0x12345678;\n\tprintf(\"%X\\n\", m);\n\tshow_bytes((char*)(&m), sizeof(int));\n\t\n\treturn 0;\n}\n```\n\n```bash\n./endian\n12345678\n78 56 34 12  \n```\n\n了解大小端的意义如下：\n- 网络传输大多用big-endian, 而计算机一般用little-endian\n- 汇编、机器码的顺序\n- C中的cast操作，如union可能带来问题\n\n### String (ASCII)\n由于ASCII的范围0~127，每一次都读取一个byte，所以不受大小端的影响\n\n---\n\n## Intergers\n\n\n### Unsigned & Two's Complement\n\n假设有一个data的bit vector, [w<sub>k-1</sub>, w<sub>k-2</sub> .. w<sub>0</sub>], unsigned : 求和每一位的2的次幂乘以w<sub>x</sub>, Two's Complement ： 第一位为符号位，其拥有负权重2<sub>k-1</sup>。\n\n这也即当符号为为1时，其值为负而0为正的原因：最大的负数 -1 ： 2<sub>k-1</sup> + (2<sub>k-1</sup> -1)，同时记U至T的转换为U2T(x) = x<sub>t</sub> - w<sub>k-1</sub> * 2<sub>k</sup>\n\n---\n\n### Bit-Level Operations\n&（位与） |（位或）^(异或) >>（右移） <<(左移)\t~(取补码)\n```\n&  0 1       |  0 1     ^ 0 1    ~ 0 1\n0  0 0       0  0 1     0 0 1      \n1  0 1       1  1 1     1 1 0      1 0\n```\n\n#### 关于 &,&&　　|,|| \n当逻辑运算符&&，||操作的对象为一个bit时，相当于&, |\n\n#### 关于~\n**x　+　~x+1　=　0**　也即~x+1为x的加法逆元\n\n#### 关于 ^\n* 用于Floating Points符号位的计算较为方便（NaN除外）, 相当于求其加法逆元(Integers用~x+1方便)\n* x^x = 0\n\n####  左移　右移\n左移、右移顾名思义就是将bit vector向左、右移动，多出来的bit自然舍去，问题在于缺少的bit位是用0/1来补，而这在U/T的表现又不同\n\nU/T 左移 ： 用0补齐缺少位，称为逻辑左移\n\nU 右移 ： 用0补齐缺少位，称为逻辑右移\n\nT 右移 ： 补齐位与符号位一致，可以为0/1，称为算术右移\n　\n左移k位，所有对应位的权重都增大k,相当于乘以 2<sub>k</sup>  （int符号位虽然是负权重，但是结果相同）\n\n右移k位，所有对应位的权重都减少k,相当于除以 2<sub>k</sup> ,需要注意的是结果需要**Rounding**(取整)。C中的整数取整都是向0取整，而应用>>时正数：向下取整，也即向0取整，而对于负数来说，其依然是向下取整，所有需要加上相应的bias,使其满足向0取整。bias = 1 << k-1\n\n### Conversion\n转变分为三种\n- int 与 unsigned int\n- 小字节类型转大字节类型\n- 大字节类型转小字节类型\n\nint 与 unsigned int 的转换在C中并不改变其bit-represention,只是改变其解读形式。\n\n大字节转小字节则是要截断高位。\n\n小字节转大字节，则需要拓展高位，这其中又分为sign expansion && zero expansion。\n\n　　首先考虑U-U的小转大，采用零拓展，即高位全用0填补。\n　　T-T的拓展，采用符号拓展，即高位拓展取决与符号为。\n　　那么U-T,T-U的小字节转大字节是怎么实现的呢？规律是先变size再改变类型。也就是其拓展与改变前的拓展类型一致，举例如下：\n```C\n#include <stdio.h>\n\nint main(void)\n{\n\tunsigned short int s_u = 0xcfff;\n\tshort int s_t = 0xcfff;\n\tint i_u = (int)s_u;\n\tint i_t = (int)s_t;\n\tprintf(\"i_u : %x\\n\", i_u);\n\tprintf(\"i_t : %x\\n\", i_t);\n\treturn 0;\n}\n```\n```bash\ni_u : cfff\ni_t : ffffcfff\n```\n\n### Integers Arithmetic\n\n这些运算以及数据本身构成了群，如无整型与加法构成了无整型加群，在这里引入群中的一个概念，加法逆元，若a+b=0，则称b为a的逆元。\n\n#### Modular addition\n加法其实是Modualr addition,即对最后的结果取模。\n```C\n        U                    T\n\t x=0:    x          x=Tmin: Tmin \n-x                         \n\t x>0:   2**w - x    x>Tmin: -x\n\n\n溢出检测：\n     !(x + y < x)       !(x<0 && y<0 && sum >=0) && !(x>=0 && y>=0 && sum<0) (OR !((x<0 == y<0) && (sum<0 != x<0)))\n```\n加法逆元的C表达式：\n> ~x+1\n\n#### Muplication\nunsigned 计算时同样时将结果mod 2<sub>w</sub>, signed计算则是先将其转为unsigned计算，最后转回signed。这也同样表明了两种类型的乘法运算结果truncted后的bit-representation是一样的。\n\n溢出检测的两种方式：\n```C\n///1.群的性质\nint tmult_ok(int x, int y)\n{\n\tint p = x*y;\n\treturn !x || p/x == y;\n}\n\n///2.用更大的数位表示\nint tmult_ok(int x, int y)\n{\n\tin64_t pll = (in64_t)x*y;\n\treturn pll == (int)pll;\n}\n\n```\n> Tips : size_t被定义为unsigned类型，而malloc接收一个size_t类型，那么溢出后得到一个较小的整数的话是无法被发现的，只能在malloc前对其siz做溢出检测。\n\n## Floating Point\n首先给出浮点数的一般表达式\n\n$$\nv = (-1)^s*M*2^E\n$$\n浮点数的bit-representation如下\n```\n单精度\n|s|            exp             |         frac              |\n31 30                        23 22                         0\n双精度\n|s|            exp                    |                   frac                                  |\n63 62                               52 51                                                       0\n```\n由sign, exp, frac三部分组成，sign即是符号位，exp参与E的表达，fac参与小数部分的表达，具体表达式分为Normalized values, Denormalized values, Special values情况。其中Denormalized vaules 表达的是接近0的数，Special values表示的是无穷，以及NaN(Not a Number)的情况，Normalized values 表示的是除上述两种情况外的一般的数。\n\n以单精度为例\n```\nNormalized values                  exp !=0 && exp !=255\nM = 1+f  E = e - bias   e is the value of the bit representation (exp)\nDenormalized values\nM = f    E = 1 - bias\nSpecial values\n1. Infinite\nexp = all 1  f = 0\n2. NaN\nexp = all 1 f != 0\n```\nbias = $2^(k-1) - 1$, 127 for single precision and 1023 for double。So the exponent ranges form -126~127 for single precision and -1022~1023 for doulbe.\n采用这中方式编码的好处：如果采用类似U/T的编码方式，那么会丢失精度，而且所表达的范围也会变小\n\n### Floating Points Arithmetic\n\n#### Addition\n两步走：1.向较大的E（指数位）对齐 2.相加再调整M、E。\n```\n   |         (-1)^s1*M1             |\n          |      (-1)^s2*M2                       |\n\t\t                            |----E1-E2----|\n```\n显然当大数+小数时，小数的精度丢失比较严重,所以不满足结合律。而且加法满足单调性，a>b => a+x > b+x, 只要x不为NaN即可。\n加法的性质：\n- 闭合（可能产生无穷/NaN）\n- 满足交换律\n- 不满足结合律\n- 存在逆元\n- 满足单调性\n\n#### Muplication\n- 满足交换律\n- 不满足结合律\n- 不满足乘法分配率\n- 满足单调性\na>=b and c>=0  => ac>=bc, c<=0时亦然，只要c不为NaN即可。\n\n#### compare to integer\ninteger的运算满足交换律，结合律，分配律，但不满足单调性的原理。Floating Point的运算满足交换律，不满足分配律、结合律，但是满足单调性原理。\n\n\n> Tips:关于FP不满足结合律的问题：自然界中的问题很多都是连续的，也就是说在某些状况下是不会出现极大值与极小值运算的情形，也就不用考虑不满足结合率带来的影响，但是在其他的情景下，如金融中是可能出现的。这也提醒我们要时刻把握具体情景，从需求出发。\n\n#### conversion\nfloat int double 之间的转变是改变位表示的，double/float cast 为 int是需要向0取整（NaN可能转为Tmin），int cast to double 不会出现问题，int cast to float可能要round。\n\n\n","slug":"Bits-Intergers-Floating-Points","published":1,"updated":"2021-08-27T03:58:21.059Z","_id":"ckrnkqmec0000gckk2sozbnip","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Bits-amp-Interters-amp-Floating-Points\"><a href=\"#Bits-amp-Interters-amp-Floating-Points\" class=\"headerlink\" title=\"Bits &amp; Interters &amp; Floating Points\"></a>Bits &amp; Interters &amp; Floating Points</h1><h2 id=\"Bit\"><a href=\"#Bit\" class=\"headerlink\" title=\"Bit\"></a>Bit</h2><p>计算机以二进制理解世界，也就是一串0和1，重要的是bit representation（二进制表示）以及 decoding（解读方式)。<br>以文件的传输为例，ASCII文件中的文字首先被编码成二进制形式，然后发送给接收者，接收端以一定的规则解码得到原来的文件。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">发送者 (encoding) ------&gt; 01100110 (decoding)------&gt; 接收者</span><br></pre></td></tr></table></figure>\n<p>decoding方式带来的影响：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">以4-bit的data type 为例</span><br><span class=\"line\">1111 --       int     --&gt; -1</span><br><span class=\"line\">1111 --  unsigned int --&gt; 15</span><br></pre></td></tr></table></figure>\n<p>由于 int 采用补码形式表示，而usigned不是导致值不同（后文详解）</p>\n<h3 id=\"Data-Size\"><a href=\"#Data-Size\" class=\"headerlink\" title=\"Data Size\"></a>Data Size</h3><p>1 bit 即是1个0或1，8 bits == 1 byte, 16 bits == 2 bytes == 1 words<br>word size 就是指针所占的字位数</p>\n<h3 id=\"Address\"><a href=\"#Address\" class=\"headerlink\" title=\"Address\"></a>Address</h3><p>当我们谈到计算机的内存与地址时，一般指的是虚拟内存，每一bit都有它的地址</p>\n<h3 id=\"Byte-Ordering\"><a href=\"#Byte-Ordering\" class=\"headerlink\" title=\"Byte Ordering\"></a>Byte Ordering</h3><p>既然bit都有自己的地址，那么连续的bits的地址一定是连续的，它们存储方式有两种big endian, little endian。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">address        0x100  0x101  0x102 0x103</span><br><span class=\"line\">big endian        01     23     45    67</span><br><span class=\"line\">little endian     67     45     23    01</span><br></pre></td></tr></table></figure>\n<p>big endian 即是大地址结尾，而little endian 即是以小地址结尾。<br>在我的机子上，其以little endian方式存储，可以用以下的C代码验证</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">show_bytes</span><span class=\"params\">(<span class=\"keyword\">char</span>* c_p, <span class=\"keyword\">size_t</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;<span class=\"function\">i</span></span><br><span class=\"line\"><span class=\"function\">\t\t<span class=\"title\">for</span><span class=\"params\">(<span class=\"keyword\">size_t</span> i = <span class=\"number\">0</span>; i&lt;n; i++)</span></span></span><br><span class=\"line\"><span class=\"function\">\t\t</span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%X &quot;</span>, *c_p);</span><br><span class=\"line\">\t\t\t\tc_p++;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"built_in\">puts</span>(<span class=\"string\">&quot;&quot;</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> m = <span class=\"number\">0x12345678</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%X\\n&quot;</span>, m);</span><br><span class=\"line\">\tshow_bytes((<span class=\"keyword\">char</span>*)(&amp;m), <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">int</span>));</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./endian</span><br><span class=\"line\">12345678</span><br><span class=\"line\">78 56 34 12  </span><br></pre></td></tr></table></figure>\n\n<p>了解大小端的意义如下：</p>\n<ul>\n<li>网络传输大多用big-endian, 而计算机一般用little-endian</li>\n<li>汇编、机器码的顺序</li>\n<li>C中的cast操作，如union可能带来问题</li>\n</ul>\n<h3 id=\"String-ASCII\"><a href=\"#String-ASCII\" class=\"headerlink\" title=\"String (ASCII)\"></a>String (ASCII)</h3><p>由于ASCII的范围0~127，每一次都读取一个byte，所以不受大小端的影响</p>\n<hr>\n<h2 id=\"Intergers\"><a href=\"#Intergers\" class=\"headerlink\" title=\"Intergers\"></a>Intergers</h2><h3 id=\"Unsigned-amp-Two’s-Complement\"><a href=\"#Unsigned-amp-Two’s-Complement\" class=\"headerlink\" title=\"Unsigned &amp; Two’s Complement\"></a>Unsigned &amp; Two’s Complement</h3><p>假设有一个data的bit vector, [w<sub>k-1</sub>, w<sub>k-2</sub> .. w<sub>0</sub>], unsigned : 求和每一位的2的次幂乘以w<sub>x</sub>, Two’s Complement ： 第一位为符号位，其拥有负权重2<sub>k-1。</sub></p>\n<p>这也即当符号为为1时，其值为负而0为正的原因：最大的负数 -1 ： 2<sub>k-1 + (2<sub>k-1 -1)，同时记U至T的转换为U2T(x) = x<sub>t</sub> - w<sub>k-1</sub> * 2<sub>k</sub></sub></sub></p>\n<hr>\n<h3 id=\"Bit-Level-Operations\"><a href=\"#Bit-Level-Operations\" class=\"headerlink\" title=\"Bit-Level Operations\"></a>Bit-Level Operations</h3><p>&amp;（位与） |（位或）^(异或) &gt;&gt;（右移） &lt;&lt;(左移)    ~(取补码)</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&amp;  0 1       |  0 1     ^ 0 1    ~ 0 1</span><br><span class=\"line\">0  0 0       0  0 1     0 0 1      </span><br><span class=\"line\">1  0 1       1  1 1     1 1 0      1 0</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"关于-amp-amp-amp\"><a href=\"#关于-amp-amp-amp\" class=\"headerlink\" title=\"关于 &amp;,&amp;&amp;　　|,||\"></a>关于 &amp;,&amp;&amp;　　|,||</h4><p>当逻辑运算符&amp;&amp;，||操作的对象为一个bit时，相当于&amp;, |</p>\n<h4 id=\"关于\"><a href=\"#关于\" class=\"headerlink\" title=\"关于~\"></a>关于~</h4><p><strong>x　+　~x+1　=　0</strong>　也即~x+1为x的加法逆元</p>\n<h4 id=\"关于-1\"><a href=\"#关于-1\" class=\"headerlink\" title=\"关于 ^\"></a>关于 ^</h4><ul>\n<li>用于Floating Points符号位的计算较为方便（NaN除外）, 相当于求其加法逆元(Integers用~x+1方便)</li>\n<li>x^x = 0</li>\n</ul>\n<h4 id=\"左移-右移\"><a href=\"#左移-右移\" class=\"headerlink\" title=\"左移　右移\"></a>左移　右移</h4><p>左移、右移顾名思义就是将bit vector向左、右移动，多出来的bit自然舍去，问题在于缺少的bit位是用0/1来补，而这在U/T的表现又不同</p>\n<p>U/T 左移 ： 用0补齐缺少位，称为逻辑左移</p>\n<p>U 右移 ： 用0补齐缺少位，称为逻辑右移</p>\n<p>T 右移 ： 补齐位与符号位一致，可以为0/1，称为算术右移<br>　<br>左移k位，所有对应位的权重都增大k,相当于乘以 2<sub>k  （int符号位虽然是负权重，但是结果相同）</sub></p>\n<p>右移k位，所有对应位的权重都减少k,相当于除以 2<sub>k ,需要注意的是结果需要<strong>Rounding</strong>(取整)。C中的整数取整都是向0取整，而应用&gt;&gt;时正数：向下取整，也即向0取整，而对于负数来说，其依然是向下取整，所有需要加上相应的bias,使其满足向0取整。bias = 1 &lt;&lt; k-1</sub></p>\n<h3 id=\"Conversion\"><a href=\"#Conversion\" class=\"headerlink\" title=\"Conversion\"></a>Conversion</h3><p>转变分为三种</p>\n<ul>\n<li>int 与 unsigned int</li>\n<li>小字节类型转大字节类型</li>\n<li>大字节类型转小字节类型</li>\n</ul>\n<p>int 与 unsigned int 的转换在C中并不改变其bit-represention,只是改变其解读形式。</p>\n<p>大字节转小字节则是要截断高位。</p>\n<p>小字节转大字节，则需要拓展高位，这其中又分为sign expansion &amp;&amp; zero expansion。</p>\n<p>　　首先考虑U-U的小转大，采用零拓展，即高位全用0填补。<br>　　T-T的拓展，采用符号拓展，即高位拓展取决与符号为。<br>　　那么U-T,T-U的小字节转大字节是怎么实现的呢？规律是先变size再改变类型。也就是其拓展与改变前的拓展类型一致，举例如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">unsigned</span> <span class=\"keyword\">short</span> <span class=\"keyword\">int</span> s_u = <span class=\"number\">0xcfff</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">short</span> <span class=\"keyword\">int</span> <span class=\"keyword\">s_t</span> = <span class=\"number\">0xcfff</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> i_u = (<span class=\"keyword\">int</span>)s_u;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> <span class=\"keyword\">i_t</span> = (<span class=\"keyword\">int</span>)<span class=\"keyword\">s_t</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;i_u : %x\\n&quot;</span>, i_u);</span><br><span class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;i_t : %x\\n&quot;</span>, <span class=\"keyword\">i_t</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i_u : cfff</span><br><span class=\"line\">i_t : ffffcfff</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Integers-Arithmetic\"><a href=\"#Integers-Arithmetic\" class=\"headerlink\" title=\"Integers Arithmetic\"></a>Integers Arithmetic</h3><p>这些运算以及数据本身构成了群，如无整型与加法构成了无整型加群，在这里引入群中的一个概念，加法逆元，若a+b=0，则称b为a的逆元。</p>\n<h4 id=\"Modular-addition\"><a href=\"#Modular-addition\" class=\"headerlink\" title=\"Modular addition\"></a>Modular addition</h4><p>加法其实是Modualr addition,即对最后的结果取模。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        U                    T</span><br><span class=\"line\">\t x=<span class=\"number\">0</span>:    x          x=Tmin: Tmin </span><br><span class=\"line\">-x                         </span><br><span class=\"line\">\t x&gt;<span class=\"number\">0</span>:   <span class=\"number\">2</span>**w - x    x&gt;Tmin: -x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">溢出检测：</span><br><span class=\"line\">     !(x + y &lt; x)       !(x&lt;<span class=\"number\">0</span> &amp;&amp; y&lt;<span class=\"number\">0</span> &amp;&amp; sum &gt;=<span class=\"number\">0</span>) &amp;&amp; !(x&gt;=<span class=\"number\">0</span> &amp;&amp; y&gt;=<span class=\"number\">0</span> &amp;&amp; sum&lt;<span class=\"number\">0</span>) (OR !((x&lt;<span class=\"number\">0</span> == y&lt;<span class=\"number\">0</span>) &amp;&amp; (sum&lt;<span class=\"number\">0</span> != x&lt;<span class=\"number\">0</span>)))</span><br></pre></td></tr></table></figure>\n<p>加法逆元的C表达式：</p>\n<blockquote>\n<p>~x+1</p>\n</blockquote>\n<h4 id=\"Muplication\"><a href=\"#Muplication\" class=\"headerlink\" title=\"Muplication\"></a>Muplication</h4><p>unsigned 计算时同样时将结果mod 2<sub>w</sub>, signed计算则是先将其转为unsigned计算，最后转回signed。这也同样表明了两种类型的乘法运算结果truncted后的bit-representation是一样的。</p>\n<p>溢出检测的两种方式：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">///1.群的性质</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">tmult_ok</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> y)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> p = x*y;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> !x || p/x == y;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">///2.用更大的数位表示</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">tmult_ok</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> y)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">in64_t</span> pll = (<span class=\"keyword\">in64_t</span>)x*y;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> pll == (<span class=\"keyword\">int</span>)pll;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Tips : size_t被定义为unsigned类型，而malloc接收一个size_t类型，那么溢出后得到一个较小的整数的话是无法被发现的，只能在malloc前对其siz做溢出检测。</p>\n</blockquote>\n<h2 id=\"Floating-Point\"><a href=\"#Floating-Point\" class=\"headerlink\" title=\"Floating Point\"></a>Floating Point</h2><p>首先给出浮点数的一般表达式</p>\n<p>$$<br>v = (-1)^s<em>M</em>2^E<br>$$<br>浮点数的bit-representation如下</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">单精度</span><br><span class=\"line\">|s|            exp             |         frac              |</span><br><span class=\"line\">31 30                        23 22                         0</span><br><span class=\"line\">双精度</span><br><span class=\"line\">|s|            exp                    |                   frac                                  |</span><br><span class=\"line\">63 62                               52 51                                                       0</span><br></pre></td></tr></table></figure>\n<p>由sign, exp, frac三部分组成，sign即是符号位，exp参与E的表达，fac参与小数部分的表达，具体表达式分为Normalized values, Denormalized values, Special values情况。其中Denormalized vaules 表达的是接近0的数，Special values表示的是无穷，以及NaN(Not a Number)的情况，Normalized values 表示的是除上述两种情况外的一般的数。</p>\n<p>以单精度为例</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Normalized values                  exp !=0 &amp;&amp; exp !=255</span><br><span class=\"line\">M = 1+f  E = e - bias   e is the value of the bit representation (exp)</span><br><span class=\"line\">Denormalized values</span><br><span class=\"line\">M = f    E = 1 - bias</span><br><span class=\"line\">Special values</span><br><span class=\"line\">1. Infinite</span><br><span class=\"line\">exp = all 1  f = 0</span><br><span class=\"line\">2. NaN</span><br><span class=\"line\">exp = all 1 f != 0</span><br></pre></td></tr></table></figure>\n<p>bias = $2^(k-1) - 1$, 127 for single precision and 1023 for double。So the exponent ranges form -126<del>127 for single precision and -1022</del>1023 for doulbe.<br>采用这中方式编码的好处：如果采用类似U/T的编码方式，那么会丢失精度，而且所表达的范围也会变小</p>\n<h3 id=\"Floating-Points-Arithmetic\"><a href=\"#Floating-Points-Arithmetic\" class=\"headerlink\" title=\"Floating Points Arithmetic\"></a>Floating Points Arithmetic</h3><h4 id=\"Addition\"><a href=\"#Addition\" class=\"headerlink\" title=\"Addition\"></a>Addition</h4><p>两步走：1.向较大的E（指数位）对齐 2.相加再调整M、E。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">|         (-1)^s1*M1             |</span><br><span class=\"line\">       |      (-1)^s2*M2                       |</span><br><span class=\"line\">                           |----E1-E2----|</span><br></pre></td></tr></table></figure>\n<p>显然当大数+小数时，小数的精度丢失比较严重,所以不满足结合律。而且加法满足单调性，a&gt;b =&gt; a+x &gt; b+x, 只要x不为NaN即可。<br>加法的性质：</p>\n<ul>\n<li>闭合（可能产生无穷/NaN）</li>\n<li>满足交换律</li>\n<li>不满足结合律</li>\n<li>存在逆元</li>\n<li>满足单调性</li>\n</ul>\n<h4 id=\"Muplication-1\"><a href=\"#Muplication-1\" class=\"headerlink\" title=\"Muplication\"></a>Muplication</h4><ul>\n<li>满足交换律</li>\n<li>不满足结合律</li>\n<li>不满足乘法分配率</li>\n<li>满足单调性<br>a&gt;=b and c&gt;=0  =&gt; ac&gt;=bc, c&lt;=0时亦然，只要c不为NaN即可。</li>\n</ul>\n<h4 id=\"compare-to-integer\"><a href=\"#compare-to-integer\" class=\"headerlink\" title=\"compare to integer\"></a>compare to integer</h4><p>integer的运算满足交换律，结合律，分配律，但不满足单调性的原理。Floating Point的运算满足交换律，不满足分配律、结合律，但是满足单调性原理。</p>\n<blockquote>\n<p>Tips:关于FP不满足结合律的问题：自然界中的问题很多都是连续的，也就是说在某些状况下是不会出现极大值与极小值运算的情形，也就不用考虑不满足结合率带来的影响，但是在其他的情景下，如金融中是可能出现的。这也提醒我们要时刻把握具体情景，从需求出发。</p>\n</blockquote>\n<h4 id=\"conversion\"><a href=\"#conversion\" class=\"headerlink\" title=\"conversion\"></a>conversion</h4><p>float int double 之间的转变是改变位表示的，double/float cast 为 int是需要向0取整（NaN可能转为Tmin），int cast to double 不会出现问题，int cast to float可能要round。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Bits-amp-Interters-amp-Floating-Points\"><a href=\"#Bits-amp-Interters-amp-Floating-Points\" class=\"headerlink\" title=\"Bits &amp; Interters &amp; Floating Points\"></a>Bits &amp; Interters &amp; Floating Points</h1><h2 id=\"Bit\"><a href=\"#Bit\" class=\"headerlink\" title=\"Bit\"></a>Bit</h2><p>计算机以二进制理解世界，也就是一串0和1，重要的是bit representation（二进制表示）以及 decoding（解读方式)。<br>以文件的传输为例，ASCII文件中的文字首先被编码成二进制形式，然后发送给接收者，接收端以一定的规则解码得到原来的文件。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">发送者 (encoding) ------&gt; 01100110 (decoding)------&gt; 接收者</span><br></pre></td></tr></table></figure>\n<p>decoding方式带来的影响：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">以4-bit的data type 为例</span><br><span class=\"line\">1111 --       int     --&gt; -1</span><br><span class=\"line\">1111 --  unsigned int --&gt; 15</span><br></pre></td></tr></table></figure>\n<p>由于 int 采用补码形式表示，而usigned不是导致值不同（后文详解）</p>\n<h3 id=\"Data-Size\"><a href=\"#Data-Size\" class=\"headerlink\" title=\"Data Size\"></a>Data Size</h3><p>1 bit 即是1个0或1，8 bits == 1 byte, 16 bits == 2 bytes == 1 words<br>word size 就是指针所占的字位数</p>\n<h3 id=\"Address\"><a href=\"#Address\" class=\"headerlink\" title=\"Address\"></a>Address</h3><p>当我们谈到计算机的内存与地址时，一般指的是虚拟内存，每一bit都有它的地址</p>\n<h3 id=\"Byte-Ordering\"><a href=\"#Byte-Ordering\" class=\"headerlink\" title=\"Byte Ordering\"></a>Byte Ordering</h3><p>既然bit都有自己的地址，那么连续的bits的地址一定是连续的，它们存储方式有两种big endian, little endian。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">address        0x100  0x101  0x102 0x103</span><br><span class=\"line\">big endian        01     23     45    67</span><br><span class=\"line\">little endian     67     45     23    01</span><br></pre></td></tr></table></figure>\n<p>big endian 即是大地址结尾，而little endian 即是以小地址结尾。<br>在我的机子上，其以little endian方式存储，可以用以下的C代码验证</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">show_bytes</span><span class=\"params\">(<span class=\"keyword\">char</span>* c_p, <span class=\"keyword\">size_t</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;<span class=\"function\">i</span></span><br><span class=\"line\"><span class=\"function\">\t\t<span class=\"title\">for</span><span class=\"params\">(<span class=\"keyword\">size_t</span> i = <span class=\"number\">0</span>; i&lt;n; i++)</span></span></span><br><span class=\"line\"><span class=\"function\">\t\t</span>&#123;</span><br><span class=\"line\">\t\t\t\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%X &quot;</span>, *c_p);</span><br><span class=\"line\">\t\t\t\tc_p++;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"built_in\">puts</span>(<span class=\"string\">&quot;&quot;</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> m = <span class=\"number\">0x12345678</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;%X\\n&quot;</span>, m);</span><br><span class=\"line\">\tshow_bytes((<span class=\"keyword\">char</span>*)(&amp;m), <span class=\"keyword\">sizeof</span>(<span class=\"keyword\">int</span>));</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./endian</span><br><span class=\"line\">12345678</span><br><span class=\"line\">78 56 34 12  </span><br></pre></td></tr></table></figure>\n\n<p>了解大小端的意义如下：</p>\n<ul>\n<li>网络传输大多用big-endian, 而计算机一般用little-endian</li>\n<li>汇编、机器码的顺序</li>\n<li>C中的cast操作，如union可能带来问题</li>\n</ul>\n<h3 id=\"String-ASCII\"><a href=\"#String-ASCII\" class=\"headerlink\" title=\"String (ASCII)\"></a>String (ASCII)</h3><p>由于ASCII的范围0~127，每一次都读取一个byte，所以不受大小端的影响</p>\n<hr>\n<h2 id=\"Intergers\"><a href=\"#Intergers\" class=\"headerlink\" title=\"Intergers\"></a>Intergers</h2><h3 id=\"Unsigned-amp-Two’s-Complement\"><a href=\"#Unsigned-amp-Two’s-Complement\" class=\"headerlink\" title=\"Unsigned &amp; Two’s Complement\"></a>Unsigned &amp; Two’s Complement</h3><p>假设有一个data的bit vector, [w<sub>k-1</sub>, w<sub>k-2</sub> .. w<sub>0</sub>], unsigned : 求和每一位的2的次幂乘以w<sub>x</sub>, Two’s Complement ： 第一位为符号位，其拥有负权重2<sub>k-1。</sub></p>\n<p>这也即当符号为为1时，其值为负而0为正的原因：最大的负数 -1 ： 2<sub>k-1 + (2<sub>k-1 -1)，同时记U至T的转换为U2T(x) = x<sub>t</sub> - w<sub>k-1</sub> * 2<sub>k</sub></sub></sub></p>\n<hr>\n<h3 id=\"Bit-Level-Operations\"><a href=\"#Bit-Level-Operations\" class=\"headerlink\" title=\"Bit-Level Operations\"></a>Bit-Level Operations</h3><p>&amp;（位与） |（位或）^(异或) &gt;&gt;（右移） &lt;&lt;(左移)    ~(取补码)</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&amp;  0 1       |  0 1     ^ 0 1    ~ 0 1</span><br><span class=\"line\">0  0 0       0  0 1     0 0 1      </span><br><span class=\"line\">1  0 1       1  1 1     1 1 0      1 0</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"关于-amp-amp-amp\"><a href=\"#关于-amp-amp-amp\" class=\"headerlink\" title=\"关于 &amp;,&amp;&amp;　　|,||\"></a>关于 &amp;,&amp;&amp;　　|,||</h4><p>当逻辑运算符&amp;&amp;，||操作的对象为一个bit时，相当于&amp;, |</p>\n<h4 id=\"关于\"><a href=\"#关于\" class=\"headerlink\" title=\"关于~\"></a>关于~</h4><p><strong>x　+　~x+1　=　0</strong>　也即~x+1为x的加法逆元</p>\n<h4 id=\"关于-1\"><a href=\"#关于-1\" class=\"headerlink\" title=\"关于 ^\"></a>关于 ^</h4><ul>\n<li>用于Floating Points符号位的计算较为方便（NaN除外）, 相当于求其加法逆元(Integers用~x+1方便)</li>\n<li>x^x = 0</li>\n</ul>\n<h4 id=\"左移-右移\"><a href=\"#左移-右移\" class=\"headerlink\" title=\"左移　右移\"></a>左移　右移</h4><p>左移、右移顾名思义就是将bit vector向左、右移动，多出来的bit自然舍去，问题在于缺少的bit位是用0/1来补，而这在U/T的表现又不同</p>\n<p>U/T 左移 ： 用0补齐缺少位，称为逻辑左移</p>\n<p>U 右移 ： 用0补齐缺少位，称为逻辑右移</p>\n<p>T 右移 ： 补齐位与符号位一致，可以为0/1，称为算术右移<br>　<br>左移k位，所有对应位的权重都增大k,相当于乘以 2<sub>k  （int符号位虽然是负权重，但是结果相同）</sub></p>\n<p>右移k位，所有对应位的权重都减少k,相当于除以 2<sub>k ,需要注意的是结果需要<strong>Rounding</strong>(取整)。C中的整数取整都是向0取整，而应用&gt;&gt;时正数：向下取整，也即向0取整，而对于负数来说，其依然是向下取整，所有需要加上相应的bias,使其满足向0取整。bias = 1 &lt;&lt; k-1</sub></p>\n<h3 id=\"Conversion\"><a href=\"#Conversion\" class=\"headerlink\" title=\"Conversion\"></a>Conversion</h3><p>转变分为三种</p>\n<ul>\n<li>int 与 unsigned int</li>\n<li>小字节类型转大字节类型</li>\n<li>大字节类型转小字节类型</li>\n</ul>\n<p>int 与 unsigned int 的转换在C中并不改变其bit-represention,只是改变其解读形式。</p>\n<p>大字节转小字节则是要截断高位。</p>\n<p>小字节转大字节，则需要拓展高位，这其中又分为sign expansion &amp;&amp; zero expansion。</p>\n<p>　　首先考虑U-U的小转大，采用零拓展，即高位全用0填补。<br>　　T-T的拓展，采用符号拓展，即高位拓展取决与符号为。<br>　　那么U-T,T-U的小字节转大字节是怎么实现的呢？规律是先变size再改变类型。也就是其拓展与改变前的拓展类型一致，举例如下：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;stdio.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">unsigned</span> <span class=\"keyword\">short</span> <span class=\"keyword\">int</span> s_u = <span class=\"number\">0xcfff</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">short</span> <span class=\"keyword\">int</span> <span class=\"keyword\">s_t</span> = <span class=\"number\">0xcfff</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> i_u = (<span class=\"keyword\">int</span>)s_u;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> <span class=\"keyword\">i_t</span> = (<span class=\"keyword\">int</span>)<span class=\"keyword\">s_t</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;i_u : %x\\n&quot;</span>, i_u);</span><br><span class=\"line\">\t<span class=\"built_in\">printf</span>(<span class=\"string\">&quot;i_t : %x\\n&quot;</span>, <span class=\"keyword\">i_t</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">i_u : cfff</span><br><span class=\"line\">i_t : ffffcfff</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Integers-Arithmetic\"><a href=\"#Integers-Arithmetic\" class=\"headerlink\" title=\"Integers Arithmetic\"></a>Integers Arithmetic</h3><p>这些运算以及数据本身构成了群，如无整型与加法构成了无整型加群，在这里引入群中的一个概念，加法逆元，若a+b=0，则称b为a的逆元。</p>\n<h4 id=\"Modular-addition\"><a href=\"#Modular-addition\" class=\"headerlink\" title=\"Modular addition\"></a>Modular addition</h4><p>加法其实是Modualr addition,即对最后的结果取模。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">        U                    T</span><br><span class=\"line\">\t x=<span class=\"number\">0</span>:    x          x=Tmin: Tmin </span><br><span class=\"line\">-x                         </span><br><span class=\"line\">\t x&gt;<span class=\"number\">0</span>:   <span class=\"number\">2</span>**w - x    x&gt;Tmin: -x</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">溢出检测：</span><br><span class=\"line\">     !(x + y &lt; x)       !(x&lt;<span class=\"number\">0</span> &amp;&amp; y&lt;<span class=\"number\">0</span> &amp;&amp; sum &gt;=<span class=\"number\">0</span>) &amp;&amp; !(x&gt;=<span class=\"number\">0</span> &amp;&amp; y&gt;=<span class=\"number\">0</span> &amp;&amp; sum&lt;<span class=\"number\">0</span>) (OR !((x&lt;<span class=\"number\">0</span> == y&lt;<span class=\"number\">0</span>) &amp;&amp; (sum&lt;<span class=\"number\">0</span> != x&lt;<span class=\"number\">0</span>)))</span><br></pre></td></tr></table></figure>\n<p>加法逆元的C表达式：</p>\n<blockquote>\n<p>~x+1</p>\n</blockquote>\n<h4 id=\"Muplication\"><a href=\"#Muplication\" class=\"headerlink\" title=\"Muplication\"></a>Muplication</h4><p>unsigned 计算时同样时将结果mod 2<sub>w</sub>, signed计算则是先将其转为unsigned计算，最后转回signed。这也同样表明了两种类型的乘法运算结果truncted后的bit-representation是一样的。</p>\n<p>溢出检测的两种方式：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">///1.群的性质</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">tmult_ok</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> y)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> p = x*y;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> !x || p/x == y;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">///2.用更大的数位表示</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">tmult_ok</span><span class=\"params\">(<span class=\"keyword\">int</span> x, <span class=\"keyword\">int</span> y)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">in64_t</span> pll = (<span class=\"keyword\">in64_t</span>)x*y;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> pll == (<span class=\"keyword\">int</span>)pll;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Tips : size_t被定义为unsigned类型，而malloc接收一个size_t类型，那么溢出后得到一个较小的整数的话是无法被发现的，只能在malloc前对其siz做溢出检测。</p>\n</blockquote>\n<h2 id=\"Floating-Point\"><a href=\"#Floating-Point\" class=\"headerlink\" title=\"Floating Point\"></a>Floating Point</h2><p>首先给出浮点数的一般表达式</p>\n<p>$$<br>v = (-1)^s<em>M</em>2^E<br>$$<br>浮点数的bit-representation如下</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">单精度</span><br><span class=\"line\">|s|            exp             |         frac              |</span><br><span class=\"line\">31 30                        23 22                         0</span><br><span class=\"line\">双精度</span><br><span class=\"line\">|s|            exp                    |                   frac                                  |</span><br><span class=\"line\">63 62                               52 51                                                       0</span><br></pre></td></tr></table></figure>\n<p>由sign, exp, frac三部分组成，sign即是符号位，exp参与E的表达，fac参与小数部分的表达，具体表达式分为Normalized values, Denormalized values, Special values情况。其中Denormalized vaules 表达的是接近0的数，Special values表示的是无穷，以及NaN(Not a Number)的情况，Normalized values 表示的是除上述两种情况外的一般的数。</p>\n<p>以单精度为例</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Normalized values                  exp !=0 &amp;&amp; exp !=255</span><br><span class=\"line\">M = 1+f  E = e - bias   e is the value of the bit representation (exp)</span><br><span class=\"line\">Denormalized values</span><br><span class=\"line\">M = f    E = 1 - bias</span><br><span class=\"line\">Special values</span><br><span class=\"line\">1. Infinite</span><br><span class=\"line\">exp = all 1  f = 0</span><br><span class=\"line\">2. NaN</span><br><span class=\"line\">exp = all 1 f != 0</span><br></pre></td></tr></table></figure>\n<p>bias = $2^(k-1) - 1$, 127 for single precision and 1023 for double。So the exponent ranges form -126<del>127 for single precision and -1022</del>1023 for doulbe.<br>采用这中方式编码的好处：如果采用类似U/T的编码方式，那么会丢失精度，而且所表达的范围也会变小</p>\n<h3 id=\"Floating-Points-Arithmetic\"><a href=\"#Floating-Points-Arithmetic\" class=\"headerlink\" title=\"Floating Points Arithmetic\"></a>Floating Points Arithmetic</h3><h4 id=\"Addition\"><a href=\"#Addition\" class=\"headerlink\" title=\"Addition\"></a>Addition</h4><p>两步走：1.向较大的E（指数位）对齐 2.相加再调整M、E。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">|         (-1)^s1*M1             |</span><br><span class=\"line\">       |      (-1)^s2*M2                       |</span><br><span class=\"line\">                           |----E1-E2----|</span><br></pre></td></tr></table></figure>\n<p>显然当大数+小数时，小数的精度丢失比较严重,所以不满足结合律。而且加法满足单调性，a&gt;b =&gt; a+x &gt; b+x, 只要x不为NaN即可。<br>加法的性质：</p>\n<ul>\n<li>闭合（可能产生无穷/NaN）</li>\n<li>满足交换律</li>\n<li>不满足结合律</li>\n<li>存在逆元</li>\n<li>满足单调性</li>\n</ul>\n<h4 id=\"Muplication-1\"><a href=\"#Muplication-1\" class=\"headerlink\" title=\"Muplication\"></a>Muplication</h4><ul>\n<li>满足交换律</li>\n<li>不满足结合律</li>\n<li>不满足乘法分配率</li>\n<li>满足单调性<br>a&gt;=b and c&gt;=0  =&gt; ac&gt;=bc, c&lt;=0时亦然，只要c不为NaN即可。</li>\n</ul>\n<h4 id=\"compare-to-integer\"><a href=\"#compare-to-integer\" class=\"headerlink\" title=\"compare to integer\"></a>compare to integer</h4><p>integer的运算满足交换律，结合律，分配律，但不满足单调性的原理。Floating Point的运算满足交换律，不满足分配律、结合律，但是满足单调性原理。</p>\n<blockquote>\n<p>Tips:关于FP不满足结合律的问题：自然界中的问题很多都是连续的，也就是说在某些状况下是不会出现极大值与极小值运算的情形，也就不用考虑不满足结合率带来的影响，但是在其他的情景下，如金融中是可能出现的。这也提醒我们要时刻把握具体情景，从需求出发。</p>\n</blockquote>\n<h4 id=\"conversion\"><a href=\"#conversion\" class=\"headerlink\" title=\"conversion\"></a>conversion</h4><p>float int double 之间的转变是改变位表示的，double/float cast 为 int是需要向0取整（NaN可能转为Tmin），int cast to double 不会出现问题，int cast to float可能要round。</p>\n"},{"title":"Machine-Level Representation of Programs","date":"2021-08-01T02:36:57.000Z","mathjax":true,"cover":"/img/pic3.png","_content":"\n# Machine-Level Representation of Programs\n\n## 编译与反汇编\n先回忆一个C文件生成程序的过程：预处理，编译，汇编，链接。现阶段的编译器对代码的优化程度大，以至于反汇编时的汇编代码产生大幅度的变形，为了学习上的理解方便，我们使用gcc -Og的指令，使其代码不在编译阶段发生变形，同时使用objdump工具反汇编程序，得到汇编代码。\n```bash\ngcc -Og prog.c -o prog\nobjdump -d prog\n```\n\n## 信息的Accessing\n\n### 寄存器\n用来暂时储存运算结果，x86-64一共有16个64bit寄存器，每个中实际上4个寄存器，分别为64, 32, 16, 8 bit.\n\n```\n63                          31               15            7          0\n-----------------------------------------------------------------------\n|%rax                         |%eax            |%ax         |%al       |\n-----------------------------------------------------------------------\n```\n### 操作符\n\n|Type|Form|Value|\n|:----:|:---:|:---:|\n|立即数|$Imm|Imm|\n|寄存器|r|寄存器内的值 R(r)|\n|内存访问|(r)|访问寄存器中的地址指向的值|\n|内存访问|Imm(rb, ri, s)|计算出的地址指向的值|\n\nrb是base register, ri 是index register, s是缩放因子取1， 2， 4， 8，也即是数据类型的byte数。Imm(rb, ri, s) = Imm + R(rb) + R(ri)`*s。\n\n\n\n\n### Data Movement Instructions\nMOV & MOVZ & MOVS\n由mov_ + b/w/l/q 组成，后缀代表的是操作数据的size。一般情况下，MOV指令只会改变寄存器的指定字节数中的值，但是**movl**是一个例外，它操作32-bit int值，但是同时将高32-bit设置为0。**movq**是另一个例外，它只接受32-bit补码表示的值，但是q暗示它处理的是4-word 64-bit的值，所以它会将其sign extend.**movabsq**只能接收一个64-bit立即数，且移动到一个寄存器中。\n\nMOVZ, MOVS 分别代表将小字节移入大字节时进行zero拓展和符号拓展。\n\n## Arithmetic and Logical Operations\n**leaq** S, D  ==>  D <-- &S。取S的地址写入D中。如leaq (%rax, %rax, 4), %rax，那么%rax的值被改为地址变量5R(%rax)+7。但是这个指令通常与地址计算无关,而是用来做计算。example:\n```\nC code:\nlong scale(long x, long y, long z)\n{\n\tlong t = x + 4*y + z;\n\treturn t;\n}\n\n汇编:\n# x in %rdi, y in %rsi, z in %rdx\n\nleaq (%rdi, %rsi, 4), %rax\nleaq (%rdx, %rdx, 2), %rdx\nleaq (%rax, %rdx, 4), %rax\nret\n\n```\n\n### 其他的一些操作符\n一元操作指令：incq, decq 分别使stack的最高8个字节自增，自减，也即++,--\n二元操作指令：addq,subq; addq %rax, %rdx  ==> R(%rdx) += R(%rax); subq %rax, %rdx ==> R(%rdx) -= R(%rax); sarq/shrq salq/shlq 分别为算数/逻辑左移，算数/逻辑右移。二元运算操作符的结果都被第二个参数保存。\n\n>Tips : 使用x^x = 0的性质：xorq %rdx, %rdx比 movq $0, %rdx更有效。\n\n\n### 特殊的一元操作符\nimulq, mulq, S 将S与R(%rax)相乘得到128-bit,其中的高64-bit存储在%rdx中，低64-bit存储在%rax中。cqto 将SignExtend(R[%rax])的结果以上述形式储存。\nidivq, divq S 将R[%rdx:%rax]模后的数存储在%rdx中，将商存储在%rax中。一般来说%rdx预先被设置为0。\n\n\n## control \n\n控制流和跳转息息相关，一般都会做一个**test**访问状态码，看是否满足条件，然后**jump**指令跳转。\n\n### Condition Codes\n操作指令完成后一般都alter condition code(leaq 除外)\n|type|description|\n|:---:|:---|\n|CF|检测unsigned的overflow|\n|ZF|检测是否为0|\n\n可以改变状态码而不用寄存器的两类指令：CMP，TEST。他们都不会改变参数的值。\n|Instructions|Args|Perform|\n|:---:|:---:|:---:|\n|CMP|S1 S2| **S2-S1**|\n|TEST|S1 S2|S1&S2|\n### Accessing the Condition Codes\n- 通过不同状态码的组合将寄存器的1byte设置为0或1\n- 通过状态码jump\n- 有条件地transfer data\n\njumps的编码，常用的是PC relative,用编码后的相对地址. 举例如下：\n```\n4003fa : 74 02                                    je  XXXXXX\n4003fc : ff d0                                    callq *%rax\n\n```\nXXXXXX = 4003fc + 0x02 = 4003fe\n\n\n### switch & jump table\nswitch语句会将多个case组成一个跳转表，在反汇编时可以通过gdb寻址找到跳转表，它是一个array结构，里面存放地址，对应的即是对应指令的地址。\n\n![](jump_table.jpg)\n\n在实践反汇编中，可以通过查找test指令对应的含义来理解条件，再用gdb给出的跳转地址判断这是一个什么样的控制流。具体的控制流就不在此赘述。\n\n\n## Procedures\n\n### stack\n函数调用所产生的数据储存在stack中，stack同时也是一种数据结构，满足First in Last out。栈底为大地址，栈顶为小地址，%rsp中的值指示栈顶的位置，%rsp值减少则栈退回，反之扩增。\n\npushq/popq指令分别将指定值存入/取出栈。\n\n### call & ret\n\ncall调用函数，ret从函数返回。每一个函数调用都有自己的内存空间，也即伴随着stack扩增，同样返回时伴随着stack的回退。\n> call指令的下一条指令的地址称为Retrun address\n\ncall完成：\n- Push Return address\n- 跳转到调用的函数地址\n\nret完成：\n- Pop Return address\n- 将%rip设置为Return address,即执行跳转前的下一条指令\n\n> %rip中的值存放了该执行的指令的地址\n\n![](stack_cr.jpg)\n\n### Caller saved & Callee saved\n当我们想使用寄存器，但是其不为空时，可以把值暂时保存在栈中，当使用完后，恢复它的值。由此根据是调用者保存还是被调用者保存，又分为了**Caller saved**和**Callee saved**。\n\n![caller_or_callee_saved](saved.jpg)\n\n```\n//callee saved模板\npushq %rbx //暂时保存%rbx的值\nsubq $16 %rsp //扩增函数栈空间\n....\n....\n....\naddq $16 %rsp\npopq %rbx //恢复值\n\n```\n以笔者反汇编的经验来看，callee saved比较常见。\n\n## Arrary & Struct Allocation and Access\n- 多维数组在内存上是连续的\n- 结构体存在内存对齐现象\n数组与结构体这一节主要与C语言相关比较多，在汇编层面上只要理解__(.....)__对内存地址对应的值的访问就行。\n","source":"_posts/Machine-Level-Representation-of-Programs.md","raw":"---\ntitle: Machine-Level Representation of Programs\ndate: 2021-08-01 10:36:57\ntags: 汇编\ncategories: 深入理解计算机系统\nmathjax: true\ncover:\n---\n\n# Machine-Level Representation of Programs\n\n## 编译与反汇编\n先回忆一个C文件生成程序的过程：预处理，编译，汇编，链接。现阶段的编译器对代码的优化程度大，以至于反汇编时的汇编代码产生大幅度的变形，为了学习上的理解方便，我们使用gcc -Og的指令，使其代码不在编译阶段发生变形，同时使用objdump工具反汇编程序，得到汇编代码。\n```bash\ngcc -Og prog.c -o prog\nobjdump -d prog\n```\n\n## 信息的Accessing\n\n### 寄存器\n用来暂时储存运算结果，x86-64一共有16个64bit寄存器，每个中实际上4个寄存器，分别为64, 32, 16, 8 bit.\n\n```\n63                          31               15            7          0\n-----------------------------------------------------------------------\n|%rax                         |%eax            |%ax         |%al       |\n-----------------------------------------------------------------------\n```\n### 操作符\n\n|Type|Form|Value|\n|:----:|:---:|:---:|\n|立即数|$Imm|Imm|\n|寄存器|r|寄存器内的值 R(r)|\n|内存访问|(r)|访问寄存器中的地址指向的值|\n|内存访问|Imm(rb, ri, s)|计算出的地址指向的值|\n\nrb是base register, ri 是index register, s是缩放因子取1， 2， 4， 8，也即是数据类型的byte数。Imm(rb, ri, s) = Imm + R(rb) + R(ri)`*s。\n\n\n\n\n### Data Movement Instructions\nMOV & MOVZ & MOVS\n由mov_ + b/w/l/q 组成，后缀代表的是操作数据的size。一般情况下，MOV指令只会改变寄存器的指定字节数中的值，但是**movl**是一个例外，它操作32-bit int值，但是同时将高32-bit设置为0。**movq**是另一个例外，它只接受32-bit补码表示的值，但是q暗示它处理的是4-word 64-bit的值，所以它会将其sign extend.**movabsq**只能接收一个64-bit立即数，且移动到一个寄存器中。\n\nMOVZ, MOVS 分别代表将小字节移入大字节时进行zero拓展和符号拓展。\n\n## Arithmetic and Logical Operations\n**leaq** S, D  ==>  D <-- &S。取S的地址写入D中。如leaq (%rax, %rax, 4), %rax，那么%rax的值被改为地址变量5R(%rax)+7。但是这个指令通常与地址计算无关,而是用来做计算。example:\n```\nC code:\nlong scale(long x, long y, long z)\n{\n\tlong t = x + 4*y + z;\n\treturn t;\n}\n\n汇编:\n# x in %rdi, y in %rsi, z in %rdx\n\nleaq (%rdi, %rsi, 4), %rax\nleaq (%rdx, %rdx, 2), %rdx\nleaq (%rax, %rdx, 4), %rax\nret\n\n```\n\n### 其他的一些操作符\n一元操作指令：incq, decq 分别使stack的最高8个字节自增，自减，也即++,--\n二元操作指令：addq,subq; addq %rax, %rdx  ==> R(%rdx) += R(%rax); subq %rax, %rdx ==> R(%rdx) -= R(%rax); sarq/shrq salq/shlq 分别为算数/逻辑左移，算数/逻辑右移。二元运算操作符的结果都被第二个参数保存。\n\n>Tips : 使用x^x = 0的性质：xorq %rdx, %rdx比 movq $0, %rdx更有效。\n\n\n### 特殊的一元操作符\nimulq, mulq, S 将S与R(%rax)相乘得到128-bit,其中的高64-bit存储在%rdx中，低64-bit存储在%rax中。cqto 将SignExtend(R[%rax])的结果以上述形式储存。\nidivq, divq S 将R[%rdx:%rax]模后的数存储在%rdx中，将商存储在%rax中。一般来说%rdx预先被设置为0。\n\n\n## control \n\n控制流和跳转息息相关，一般都会做一个**test**访问状态码，看是否满足条件，然后**jump**指令跳转。\n\n### Condition Codes\n操作指令完成后一般都alter condition code(leaq 除外)\n|type|description|\n|:---:|:---|\n|CF|检测unsigned的overflow|\n|ZF|检测是否为0|\n\n可以改变状态码而不用寄存器的两类指令：CMP，TEST。他们都不会改变参数的值。\n|Instructions|Args|Perform|\n|:---:|:---:|:---:|\n|CMP|S1 S2| **S2-S1**|\n|TEST|S1 S2|S1&S2|\n### Accessing the Condition Codes\n- 通过不同状态码的组合将寄存器的1byte设置为0或1\n- 通过状态码jump\n- 有条件地transfer data\n\njumps的编码，常用的是PC relative,用编码后的相对地址. 举例如下：\n```\n4003fa : 74 02                                    je  XXXXXX\n4003fc : ff d0                                    callq *%rax\n\n```\nXXXXXX = 4003fc + 0x02 = 4003fe\n\n\n### switch & jump table\nswitch语句会将多个case组成一个跳转表，在反汇编时可以通过gdb寻址找到跳转表，它是一个array结构，里面存放地址，对应的即是对应指令的地址。\n\n![](jump_table.jpg)\n\n在实践反汇编中，可以通过查找test指令对应的含义来理解条件，再用gdb给出的跳转地址判断这是一个什么样的控制流。具体的控制流就不在此赘述。\n\n\n## Procedures\n\n### stack\n函数调用所产生的数据储存在stack中，stack同时也是一种数据结构，满足First in Last out。栈底为大地址，栈顶为小地址，%rsp中的值指示栈顶的位置，%rsp值减少则栈退回，反之扩增。\n\npushq/popq指令分别将指定值存入/取出栈。\n\n### call & ret\n\ncall调用函数，ret从函数返回。每一个函数调用都有自己的内存空间，也即伴随着stack扩增，同样返回时伴随着stack的回退。\n> call指令的下一条指令的地址称为Retrun address\n\ncall完成：\n- Push Return address\n- 跳转到调用的函数地址\n\nret完成：\n- Pop Return address\n- 将%rip设置为Return address,即执行跳转前的下一条指令\n\n> %rip中的值存放了该执行的指令的地址\n\n![](stack_cr.jpg)\n\n### Caller saved & Callee saved\n当我们想使用寄存器，但是其不为空时，可以把值暂时保存在栈中，当使用完后，恢复它的值。由此根据是调用者保存还是被调用者保存，又分为了**Caller saved**和**Callee saved**。\n\n![caller_or_callee_saved](saved.jpg)\n\n```\n//callee saved模板\npushq %rbx //暂时保存%rbx的值\nsubq $16 %rsp //扩增函数栈空间\n....\n....\n....\naddq $16 %rsp\npopq %rbx //恢复值\n\n```\n以笔者反汇编的经验来看，callee saved比较常见。\n\n## Arrary & Struct Allocation and Access\n- 多维数组在内存上是连续的\n- 结构体存在内存对齐现象\n数组与结构体这一节主要与C语言相关比较多，在汇编层面上只要理解__(.....)__对内存地址对应的值的访问就行。\n","slug":"Machine-Level-Representation-of-Programs","published":1,"updated":"2021-08-25T07:59:10.100Z","_id":"ckrsmhqht0000zhkk7mplcltu","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Machine-Level-Representation-of-Programs\"><a href=\"#Machine-Level-Representation-of-Programs\" class=\"headerlink\" title=\"Machine-Level Representation of Programs\"></a>Machine-Level Representation of Programs</h1><h2 id=\"编译与反汇编\"><a href=\"#编译与反汇编\" class=\"headerlink\" title=\"编译与反汇编\"></a>编译与反汇编</h2><p>先回忆一个C文件生成程序的过程：预处理，编译，汇编，链接。现阶段的编译器对代码的优化程度大，以至于反汇编时的汇编代码产生大幅度的变形，为了学习上的理解方便，我们使用gcc -Og的指令，使其代码不在编译阶段发生变形，同时使用objdump工具反汇编程序，得到汇编代码。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc -Og prog.c -o prog</span><br><span class=\"line\">objdump -d prog</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"信息的Accessing\"><a href=\"#信息的Accessing\" class=\"headerlink\" title=\"信息的Accessing\"></a>信息的Accessing</h2><h3 id=\"寄存器\"><a href=\"#寄存器\" class=\"headerlink\" title=\"寄存器\"></a>寄存器</h3><p>用来暂时储存运算结果，x86-64一共有16个64bit寄存器，每个中实际上4个寄存器，分别为64, 32, 16, 8 bit.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">63                          31               15            7          0</span><br><span class=\"line\">-----------------------------------------------------------------------</span><br><span class=\"line\">|%rax                         |%eax            |%ax         |%al       |</span><br><span class=\"line\">-----------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n<h3 id=\"操作符\"><a href=\"#操作符\" class=\"headerlink\" title=\"操作符\"></a>操作符</h3><table>\n<thead>\n<tr>\n<th align=\"center\">Type</th>\n<th align=\"center\">Form</th>\n<th align=\"center\">Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">立即数</td>\n<td align=\"center\">$Imm</td>\n<td align=\"center\">Imm</td>\n</tr>\n<tr>\n<td align=\"center\">寄存器</td>\n<td align=\"center\">r</td>\n<td align=\"center\">寄存器内的值 R(r)</td>\n</tr>\n<tr>\n<td align=\"center\">内存访问</td>\n<td align=\"center\">(r)</td>\n<td align=\"center\">访问寄存器中的地址指向的值</td>\n</tr>\n<tr>\n<td align=\"center\">内存访问</td>\n<td align=\"center\">Imm(rb, ri, s)</td>\n<td align=\"center\">计算出的地址指向的值</td>\n</tr>\n</tbody></table>\n<p>rb是base register, ri 是index register, s是缩放因子取1， 2， 4， 8，也即是数据类型的byte数。Imm(rb, ri, s) = Imm + R(rb) + R(ri)`*s。</p>\n<h3 id=\"Data-Movement-Instructions\"><a href=\"#Data-Movement-Instructions\" class=\"headerlink\" title=\"Data Movement Instructions\"></a>Data Movement Instructions</h3><p>MOV &amp; MOVZ &amp; MOVS<br>由mov_ + b/w/l/q 组成，后缀代表的是操作数据的size。一般情况下，MOV指令只会改变寄存器的指定字节数中的值，但是<strong>movl</strong>是一个例外，它操作32-bit int值，但是同时将高32-bit设置为0。<strong>movq</strong>是另一个例外，它只接受32-bit补码表示的值，但是q暗示它处理的是4-word 64-bit的值，所以它会将其sign extend.<strong>movabsq</strong>只能接收一个64-bit立即数，且移动到一个寄存器中。</p>\n<p>MOVZ, MOVS 分别代表将小字节移入大字节时进行zero拓展和符号拓展。</p>\n<h2 id=\"Arithmetic-and-Logical-Operations\"><a href=\"#Arithmetic-and-Logical-Operations\" class=\"headerlink\" title=\"Arithmetic and Logical Operations\"></a>Arithmetic and Logical Operations</h2><p><strong>leaq</strong> S, D  ==&gt;  D &lt;– &amp;S。取S的地址写入D中。如leaq (%rax, %rax, 4), %rax，那么%rax的值被改为地址变量5R(%rax)+7。但是这个指令通常与地址计算无关,而是用来做计算。example:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C code:</span><br><span class=\"line\">long scale(long x, long y, long z)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tlong t = x + 4*y + z;</span><br><span class=\"line\">\treturn t;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">汇编:</span><br><span class=\"line\"># x in %rdi, y in %rsi, z in %rdx</span><br><span class=\"line\"></span><br><span class=\"line\">leaq (%rdi, %rsi, 4), %rax</span><br><span class=\"line\">leaq (%rdx, %rdx, 2), %rdx</span><br><span class=\"line\">leaq (%rax, %rdx, 4), %rax</span><br><span class=\"line\">ret</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"其他的一些操作符\"><a href=\"#其他的一些操作符\" class=\"headerlink\" title=\"其他的一些操作符\"></a>其他的一些操作符</h3><p>一元操作指令：incq, decq 分别使stack的最高8个字节自增，自减，也即++,–<br>二元操作指令：addq,subq; addq %rax, %rdx  ==&gt; R(%rdx) += R(%rax); subq %rax, %rdx ==&gt; R(%rdx) -= R(%rax); sarq/shrq salq/shlq 分别为算数/逻辑左移，算数/逻辑右移。二元运算操作符的结果都被第二个参数保存。</p>\n<blockquote>\n<p>Tips : 使用x^x = 0的性质：xorq %rdx, %rdx比 movq $0, %rdx更有效。</p>\n</blockquote>\n<h3 id=\"特殊的一元操作符\"><a href=\"#特殊的一元操作符\" class=\"headerlink\" title=\"特殊的一元操作符\"></a>特殊的一元操作符</h3><p>imulq, mulq, S 将S与R(%rax)相乘得到128-bit,其中的高64-bit存储在%rdx中，低64-bit存储在%rax中。cqto 将SignExtend(R[%rax])的结果以上述形式储存。<br>idivq, divq S 将R[%rdx:%rax]模后的数存储在%rdx中，将商存储在%rax中。一般来说%rdx预先被设置为0。</p>\n<h2 id=\"control\"><a href=\"#control\" class=\"headerlink\" title=\"control\"></a>control</h2><p>控制流和跳转息息相关，一般都会做一个<strong>test</strong>访问状态码，看是否满足条件，然后<strong>jump</strong>指令跳转。</p>\n<h3 id=\"Condition-Codes\"><a href=\"#Condition-Codes\" class=\"headerlink\" title=\"Condition Codes\"></a>Condition Codes</h3><p>操作指令完成后一般都alter condition code(leaq 除外)<br>|type|description|<br>|:—:|:—|<br>|CF|检测unsigned的overflow|<br>|ZF|检测是否为0|</p>\n<p>可以改变状态码而不用寄存器的两类指令：CMP，TEST。他们都不会改变参数的值。<br>|Instructions|Args|Perform|<br>|:—:|:—:|:—:|<br>|CMP|S1 S2| <strong>S2-S1</strong>|<br>|TEST|S1 S2|S1&amp;S2|</p>\n<h3 id=\"Accessing-the-Condition-Codes\"><a href=\"#Accessing-the-Condition-Codes\" class=\"headerlink\" title=\"Accessing the Condition Codes\"></a>Accessing the Condition Codes</h3><ul>\n<li>通过不同状态码的组合将寄存器的1byte设置为0或1</li>\n<li>通过状态码jump</li>\n<li>有条件地transfer data</li>\n</ul>\n<p>jumps的编码，常用的是PC relative,用编码后的相对地址. 举例如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4003fa : 74 02                                    je  XXXXXX</span><br><span class=\"line\">4003fc : ff d0                                    callq *%rax</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>XXXXXX = 4003fc + 0x02 = 4003fe</p>\n<h3 id=\"switch-amp-jump-table\"><a href=\"#switch-amp-jump-table\" class=\"headerlink\" title=\"switch &amp; jump table\"></a>switch &amp; jump table</h3><p>switch语句会将多个case组成一个跳转表，在反汇编时可以通过gdb寻址找到跳转表，它是一个array结构，里面存放地址，对应的即是对应指令的地址。</p>\n<p><img src=\"/2021/08/01/Machine-Level-Representation-of-Programs/jump_table.jpg\"></p>\n<p>在实践反汇编中，可以通过查找test指令对应的含义来理解条件，再用gdb给出的跳转地址判断这是一个什么样的控制流。具体的控制流就不在此赘述。</p>\n<h2 id=\"Procedures\"><a href=\"#Procedures\" class=\"headerlink\" title=\"Procedures\"></a>Procedures</h2><h3 id=\"stack\"><a href=\"#stack\" class=\"headerlink\" title=\"stack\"></a>stack</h3><p>函数调用所产生的数据储存在stack中，stack同时也是一种数据结构，满足First in Last out。栈底为大地址，栈顶为小地址，%rsp中的值指示栈顶的位置，%rsp值减少则栈退回，反之扩增。</p>\n<p>pushq/popq指令分别将指定值存入/取出栈。</p>\n<h3 id=\"call-amp-ret\"><a href=\"#call-amp-ret\" class=\"headerlink\" title=\"call &amp; ret\"></a>call &amp; ret</h3><p>call调用函数，ret从函数返回。每一个函数调用都有自己的内存空间，也即伴随着stack扩增，同样返回时伴随着stack的回退。</p>\n<blockquote>\n<p>call指令的下一条指令的地址称为Retrun address</p>\n</blockquote>\n<p>call完成：</p>\n<ul>\n<li>Push Return address</li>\n<li>跳转到调用的函数地址</li>\n</ul>\n<p>ret完成：</p>\n<ul>\n<li>Pop Return address</li>\n<li>将%rip设置为Return address,即执行跳转前的下一条指令</li>\n</ul>\n<blockquote>\n<p>%rip中的值存放了该执行的指令的地址</p>\n</blockquote>\n<p><img src=\"/2021/08/01/Machine-Level-Representation-of-Programs/stack_cr.jpg\"></p>\n<h3 id=\"Caller-saved-amp-Callee-saved\"><a href=\"#Caller-saved-amp-Callee-saved\" class=\"headerlink\" title=\"Caller saved &amp; Callee saved\"></a>Caller saved &amp; Callee saved</h3><p>当我们想使用寄存器，但是其不为空时，可以把值暂时保存在栈中，当使用完后，恢复它的值。由此根据是调用者保存还是被调用者保存，又分为了<strong>Caller saved</strong>和<strong>Callee saved</strong>。</p>\n<p><img src=\"/2021/08/01/Machine-Level-Representation-of-Programs/saved.jpg\" alt=\"caller_or_callee_saved\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//callee saved模板</span><br><span class=\"line\">pushq %rbx //暂时保存%rbx的值</span><br><span class=\"line\">subq $16 %rsp //扩增函数栈空间</span><br><span class=\"line\">....</span><br><span class=\"line\">....</span><br><span class=\"line\">....</span><br><span class=\"line\">addq $16 %rsp</span><br><span class=\"line\">popq %rbx //恢复值</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>以笔者反汇编的经验来看，callee saved比较常见。</p>\n<h2 id=\"Arrary-amp-Struct-Allocation-and-Access\"><a href=\"#Arrary-amp-Struct-Allocation-and-Access\" class=\"headerlink\" title=\"Arrary &amp; Struct Allocation and Access\"></a>Arrary &amp; Struct Allocation and Access</h2><ul>\n<li>多维数组在内存上是连续的</li>\n<li>结构体存在内存对齐现象<br>数组与结构体这一节主要与C语言相关比较多，在汇编层面上只要理解__(…..)__对内存地址对应的值的访问就行。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Machine-Level-Representation-of-Programs\"><a href=\"#Machine-Level-Representation-of-Programs\" class=\"headerlink\" title=\"Machine-Level Representation of Programs\"></a>Machine-Level Representation of Programs</h1><h2 id=\"编译与反汇编\"><a href=\"#编译与反汇编\" class=\"headerlink\" title=\"编译与反汇编\"></a>编译与反汇编</h2><p>先回忆一个C文件生成程序的过程：预处理，编译，汇编，链接。现阶段的编译器对代码的优化程度大，以至于反汇编时的汇编代码产生大幅度的变形，为了学习上的理解方便，我们使用gcc -Og的指令，使其代码不在编译阶段发生变形，同时使用objdump工具反汇编程序，得到汇编代码。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc -Og prog.c -o prog</span><br><span class=\"line\">objdump -d prog</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"信息的Accessing\"><a href=\"#信息的Accessing\" class=\"headerlink\" title=\"信息的Accessing\"></a>信息的Accessing</h2><h3 id=\"寄存器\"><a href=\"#寄存器\" class=\"headerlink\" title=\"寄存器\"></a>寄存器</h3><p>用来暂时储存运算结果，x86-64一共有16个64bit寄存器，每个中实际上4个寄存器，分别为64, 32, 16, 8 bit.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">63                          31               15            7          0</span><br><span class=\"line\">-----------------------------------------------------------------------</span><br><span class=\"line\">|%rax                         |%eax            |%ax         |%al       |</span><br><span class=\"line\">-----------------------------------------------------------------------</span><br></pre></td></tr></table></figure>\n<h3 id=\"操作符\"><a href=\"#操作符\" class=\"headerlink\" title=\"操作符\"></a>操作符</h3><table>\n<thead>\n<tr>\n<th align=\"center\">Type</th>\n<th align=\"center\">Form</th>\n<th align=\"center\">Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">立即数</td>\n<td align=\"center\">$Imm</td>\n<td align=\"center\">Imm</td>\n</tr>\n<tr>\n<td align=\"center\">寄存器</td>\n<td align=\"center\">r</td>\n<td align=\"center\">寄存器内的值 R(r)</td>\n</tr>\n<tr>\n<td align=\"center\">内存访问</td>\n<td align=\"center\">(r)</td>\n<td align=\"center\">访问寄存器中的地址指向的值</td>\n</tr>\n<tr>\n<td align=\"center\">内存访问</td>\n<td align=\"center\">Imm(rb, ri, s)</td>\n<td align=\"center\">计算出的地址指向的值</td>\n</tr>\n</tbody></table>\n<p>rb是base register, ri 是index register, s是缩放因子取1， 2， 4， 8，也即是数据类型的byte数。Imm(rb, ri, s) = Imm + R(rb) + R(ri)`*s。</p>\n<h3 id=\"Data-Movement-Instructions\"><a href=\"#Data-Movement-Instructions\" class=\"headerlink\" title=\"Data Movement Instructions\"></a>Data Movement Instructions</h3><p>MOV &amp; MOVZ &amp; MOVS<br>由mov_ + b/w/l/q 组成，后缀代表的是操作数据的size。一般情况下，MOV指令只会改变寄存器的指定字节数中的值，但是<strong>movl</strong>是一个例外，它操作32-bit int值，但是同时将高32-bit设置为0。<strong>movq</strong>是另一个例外，它只接受32-bit补码表示的值，但是q暗示它处理的是4-word 64-bit的值，所以它会将其sign extend.<strong>movabsq</strong>只能接收一个64-bit立即数，且移动到一个寄存器中。</p>\n<p>MOVZ, MOVS 分别代表将小字节移入大字节时进行zero拓展和符号拓展。</p>\n<h2 id=\"Arithmetic-and-Logical-Operations\"><a href=\"#Arithmetic-and-Logical-Operations\" class=\"headerlink\" title=\"Arithmetic and Logical Operations\"></a>Arithmetic and Logical Operations</h2><p><strong>leaq</strong> S, D  ==&gt;  D &lt;– &amp;S。取S的地址写入D中。如leaq (%rax, %rax, 4), %rax，那么%rax的值被改为地址变量5R(%rax)+7。但是这个指令通常与地址计算无关,而是用来做计算。example:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C code:</span><br><span class=\"line\">long scale(long x, long y, long z)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tlong t = x + 4*y + z;</span><br><span class=\"line\">\treturn t;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">汇编:</span><br><span class=\"line\"># x in %rdi, y in %rsi, z in %rdx</span><br><span class=\"line\"></span><br><span class=\"line\">leaq (%rdi, %rsi, 4), %rax</span><br><span class=\"line\">leaq (%rdx, %rdx, 2), %rdx</span><br><span class=\"line\">leaq (%rax, %rdx, 4), %rax</span><br><span class=\"line\">ret</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"其他的一些操作符\"><a href=\"#其他的一些操作符\" class=\"headerlink\" title=\"其他的一些操作符\"></a>其他的一些操作符</h3><p>一元操作指令：incq, decq 分别使stack的最高8个字节自增，自减，也即++,–<br>二元操作指令：addq,subq; addq %rax, %rdx  ==&gt; R(%rdx) += R(%rax); subq %rax, %rdx ==&gt; R(%rdx) -= R(%rax); sarq/shrq salq/shlq 分别为算数/逻辑左移，算数/逻辑右移。二元运算操作符的结果都被第二个参数保存。</p>\n<blockquote>\n<p>Tips : 使用x^x = 0的性质：xorq %rdx, %rdx比 movq $0, %rdx更有效。</p>\n</blockquote>\n<h3 id=\"特殊的一元操作符\"><a href=\"#特殊的一元操作符\" class=\"headerlink\" title=\"特殊的一元操作符\"></a>特殊的一元操作符</h3><p>imulq, mulq, S 将S与R(%rax)相乘得到128-bit,其中的高64-bit存储在%rdx中，低64-bit存储在%rax中。cqto 将SignExtend(R[%rax])的结果以上述形式储存。<br>idivq, divq S 将R[%rdx:%rax]模后的数存储在%rdx中，将商存储在%rax中。一般来说%rdx预先被设置为0。</p>\n<h2 id=\"control\"><a href=\"#control\" class=\"headerlink\" title=\"control\"></a>control</h2><p>控制流和跳转息息相关，一般都会做一个<strong>test</strong>访问状态码，看是否满足条件，然后<strong>jump</strong>指令跳转。</p>\n<h3 id=\"Condition-Codes\"><a href=\"#Condition-Codes\" class=\"headerlink\" title=\"Condition Codes\"></a>Condition Codes</h3><p>操作指令完成后一般都alter condition code(leaq 除外)<br>|type|description|<br>|:—:|:—|<br>|CF|检测unsigned的overflow|<br>|ZF|检测是否为0|</p>\n<p>可以改变状态码而不用寄存器的两类指令：CMP，TEST。他们都不会改变参数的值。<br>|Instructions|Args|Perform|<br>|:—:|:—:|:—:|<br>|CMP|S1 S2| <strong>S2-S1</strong>|<br>|TEST|S1 S2|S1&amp;S2|</p>\n<h3 id=\"Accessing-the-Condition-Codes\"><a href=\"#Accessing-the-Condition-Codes\" class=\"headerlink\" title=\"Accessing the Condition Codes\"></a>Accessing the Condition Codes</h3><ul>\n<li>通过不同状态码的组合将寄存器的1byte设置为0或1</li>\n<li>通过状态码jump</li>\n<li>有条件地transfer data</li>\n</ul>\n<p>jumps的编码，常用的是PC relative,用编码后的相对地址. 举例如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4003fa : 74 02                                    je  XXXXXX</span><br><span class=\"line\">4003fc : ff d0                                    callq *%rax</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>XXXXXX = 4003fc + 0x02 = 4003fe</p>\n<h3 id=\"switch-amp-jump-table\"><a href=\"#switch-amp-jump-table\" class=\"headerlink\" title=\"switch &amp; jump table\"></a>switch &amp; jump table</h3><p>switch语句会将多个case组成一个跳转表，在反汇编时可以通过gdb寻址找到跳转表，它是一个array结构，里面存放地址，对应的即是对应指令的地址。</p>\n<p><img src=\"/2021/08/01/Machine-Level-Representation-of-Programs/jump_table.jpg\"></p>\n<p>在实践反汇编中，可以通过查找test指令对应的含义来理解条件，再用gdb给出的跳转地址判断这是一个什么样的控制流。具体的控制流就不在此赘述。</p>\n<h2 id=\"Procedures\"><a href=\"#Procedures\" class=\"headerlink\" title=\"Procedures\"></a>Procedures</h2><h3 id=\"stack\"><a href=\"#stack\" class=\"headerlink\" title=\"stack\"></a>stack</h3><p>函数调用所产生的数据储存在stack中，stack同时也是一种数据结构，满足First in Last out。栈底为大地址，栈顶为小地址，%rsp中的值指示栈顶的位置，%rsp值减少则栈退回，反之扩增。</p>\n<p>pushq/popq指令分别将指定值存入/取出栈。</p>\n<h3 id=\"call-amp-ret\"><a href=\"#call-amp-ret\" class=\"headerlink\" title=\"call &amp; ret\"></a>call &amp; ret</h3><p>call调用函数，ret从函数返回。每一个函数调用都有自己的内存空间，也即伴随着stack扩增，同样返回时伴随着stack的回退。</p>\n<blockquote>\n<p>call指令的下一条指令的地址称为Retrun address</p>\n</blockquote>\n<p>call完成：</p>\n<ul>\n<li>Push Return address</li>\n<li>跳转到调用的函数地址</li>\n</ul>\n<p>ret完成：</p>\n<ul>\n<li>Pop Return address</li>\n<li>将%rip设置为Return address,即执行跳转前的下一条指令</li>\n</ul>\n<blockquote>\n<p>%rip中的值存放了该执行的指令的地址</p>\n</blockquote>\n<p><img src=\"/2021/08/01/Machine-Level-Representation-of-Programs/stack_cr.jpg\"></p>\n<h3 id=\"Caller-saved-amp-Callee-saved\"><a href=\"#Caller-saved-amp-Callee-saved\" class=\"headerlink\" title=\"Caller saved &amp; Callee saved\"></a>Caller saved &amp; Callee saved</h3><p>当我们想使用寄存器，但是其不为空时，可以把值暂时保存在栈中，当使用完后，恢复它的值。由此根据是调用者保存还是被调用者保存，又分为了<strong>Caller saved</strong>和<strong>Callee saved</strong>。</p>\n<p><img src=\"/2021/08/01/Machine-Level-Representation-of-Programs/saved.jpg\" alt=\"caller_or_callee_saved\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//callee saved模板</span><br><span class=\"line\">pushq %rbx //暂时保存%rbx的值</span><br><span class=\"line\">subq $16 %rsp //扩增函数栈空间</span><br><span class=\"line\">....</span><br><span class=\"line\">....</span><br><span class=\"line\">....</span><br><span class=\"line\">addq $16 %rsp</span><br><span class=\"line\">popq %rbx //恢复值</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>以笔者反汇编的经验来看，callee saved比较常见。</p>\n<h2 id=\"Arrary-amp-Struct-Allocation-and-Access\"><a href=\"#Arrary-amp-Struct-Allocation-and-Access\" class=\"headerlink\" title=\"Arrary &amp; Struct Allocation and Access\"></a>Arrary &amp; Struct Allocation and Access</h2><ul>\n<li>多维数组在内存上是连续的</li>\n<li>结构体存在内存对齐现象<br>数组与结构体这一节主要与C语言相关比较多，在汇编层面上只要理解__(…..)__对内存地址对应的值的访问就行。</li>\n</ul>\n"},{"_content":"# bag\n我认为基本的数据模型就只有两种，数组和链表，其他的数据类型都是在这两者上衍生而来的，尤其是链表十分灵活，以下的bag实际上就是用链表实现的，而且是一个后进先出的模型，也就是栈的另一种形式。背包实际上是集合，不在于遍历的顺序，但是要能够遍历。\n\n> 背包是一种不支持从中删除与元素的集合数据类型--它的目的就是帮助用例收集元素并迭代遍历所有收集到的元素（用例可以检查背包否为空或者取背包中元素的数量）迭代的顺序不确定且与用例无关。\n","source":"_posts/bag.md","raw":"# bag\n我认为基本的数据模型就只有两种，数组和链表，其他的数据类型都是在这两者上衍生而来的，尤其是链表十分灵活，以下的bag实际上就是用链表实现的，而且是一个后进先出的模型，也就是栈的另一种形式。背包实际上是集合，不在于遍历的顺序，但是要能够遍历。\n\n> 背包是一种不支持从中删除与元素的集合数据类型--它的目的就是帮助用例收集元素并迭代遍历所有收集到的元素（用例可以检查背包否为空或者取背包中元素的数量）迭代的顺序不确定且与用例无关。\n","slug":"bag","published":1,"date":"2021-08-20T14:27:59.470Z","updated":"2021-08-20T14:27:59.470Z","_id":"ckrsmhqhx0001zhkkhtoyemqq","title":"","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"bag\"><a href=\"#bag\" class=\"headerlink\" title=\"bag\"></a>bag</h1><p>我认为基本的数据模型就只有两种，数组和链表，其他的数据类型都是在这两者上衍生而来的，尤其是链表十分灵活，以下的bag实际上就是用链表实现的，而且是一个后进先出的模型，也就是栈的另一种形式。背包实际上是集合，不在于遍历的顺序，但是要能够遍历。</p>\n<blockquote>\n<p>背包是一种不支持从中删除与元素的集合数据类型—它的目的就是帮助用例收集元素并迭代遍历所有收集到的元素（用例可以检查背包否为空或者取背包中元素的数量）迭代的顺序不确定且与用例无关。</p>\n</blockquote>\n","site":{"data":{}},"cover":"/img/pic1.png","excerpt":"","more":"<h1 id=\"bag\"><a href=\"#bag\" class=\"headerlink\" title=\"bag\"></a>bag</h1><p>我认为基本的数据模型就只有两种，数组和链表，其他的数据类型都是在这两者上衍生而来的，尤其是链表十分灵活，以下的bag实际上就是用链表实现的，而且是一个后进先出的模型，也就是栈的另一种形式。背包实际上是集合，不在于遍历的顺序，但是要能够遍历。</p>\n<blockquote>\n<p>背包是一种不支持从中删除与元素的集合数据类型—它的目的就是帮助用例收集元素并迭代遍历所有收集到的元素（用例可以检查背包否为空或者取背包中元素的数量）迭代的顺序不确定且与用例无关。</p>\n</blockquote>\n"},{"title":"Concurrent Programming","date":"2021-08-11T12:18:44.000Z","cover":"/img/pic2.png","_content":"\n\n并发编程\n\n## 多台客户端在迭代服务器模型下的堵塞\n在网络编程一章中，我们接触到了客户端与服务器连接的迭代服务器模型，服务器每次只连接一台客户端，此时另一台客户端向服务器发送请求是会堵塞，那么在连接的pipeline中，这一台客户端究竟堵在了哪一步呢？\n\n- connect 这时客户端的连接请求虽然没有被accept，但服务端已经将它排列在等待队列中\n- writen 数据写入服务端的buffer中\n- read 堵塞在这里！服务端无法向客户端写入。\n\n## 并发服务器\n创建并发服务器有三种方式，Process-based, Event-based, Thread-based。\n\n### Process-based\n\n\n","source":"_posts/Concurrent-Programming.md","raw":"---\ntitle: Concurrent Programming\ndate: 2021-08-11 20:18:44\ntags: 并发\ncategories: 深入理解计算机系统\ncover:\n---\n\n\n并发编程\n\n## 多台客户端在迭代服务器模型下的堵塞\n在网络编程一章中，我们接触到了客户端与服务器连接的迭代服务器模型，服务器每次只连接一台客户端，此时另一台客户端向服务器发送请求是会堵塞，那么在连接的pipeline中，这一台客户端究竟堵在了哪一步呢？\n\n- connect 这时客户端的连接请求虽然没有被accept，但服务端已经将它排列在等待队列中\n- writen 数据写入服务端的buffer中\n- read 堵塞在这里！服务端无法向客户端写入。\n\n## 并发服务器\n创建并发服务器有三种方式，Process-based, Event-based, Thread-based。\n\n### Process-based\n\n\n","slug":"Concurrent-Programming","published":1,"updated":"2021-08-20T14:33:21.656Z","_id":"cksjzdkek00006ykkcoi40ps7","comments":1,"layout":"post","photos":[],"link":"","content":"<p>并发编程</p>\n<h2 id=\"多台客户端在迭代服务器模型下的堵塞\"><a href=\"#多台客户端在迭代服务器模型下的堵塞\" class=\"headerlink\" title=\"多台客户端在迭代服务器模型下的堵塞\"></a>多台客户端在迭代服务器模型下的堵塞</h2><p>在网络编程一章中，我们接触到了客户端与服务器连接的迭代服务器模型，服务器每次只连接一台客户端，此时另一台客户端向服务器发送请求是会堵塞，那么在连接的pipeline中，这一台客户端究竟堵在了哪一步呢？</p>\n<ul>\n<li>connect 这时客户端的连接请求虽然没有被accept，但服务端已经将它排列在等待队列中</li>\n<li>writen 数据写入服务端的buffer中</li>\n<li>read 堵塞在这里！服务端无法向客户端写入。</li>\n</ul>\n<h2 id=\"并发服务器\"><a href=\"#并发服务器\" class=\"headerlink\" title=\"并发服务器\"></a>并发服务器</h2><p>创建并发服务器有三种方式，Process-based, Event-based, Thread-based。</p>\n<h3 id=\"Process-based\"><a href=\"#Process-based\" class=\"headerlink\" title=\"Process-based\"></a>Process-based</h3>","site":{"data":{}},"excerpt":"","more":"<p>并发编程</p>\n<h2 id=\"多台客户端在迭代服务器模型下的堵塞\"><a href=\"#多台客户端在迭代服务器模型下的堵塞\" class=\"headerlink\" title=\"多台客户端在迭代服务器模型下的堵塞\"></a>多台客户端在迭代服务器模型下的堵塞</h2><p>在网络编程一章中，我们接触到了客户端与服务器连接的迭代服务器模型，服务器每次只连接一台客户端，此时另一台客户端向服务器发送请求是会堵塞，那么在连接的pipeline中，这一台客户端究竟堵在了哪一步呢？</p>\n<ul>\n<li>connect 这时客户端的连接请求虽然没有被accept，但服务端已经将它排列在等待队列中</li>\n<li>writen 数据写入服务端的buffer中</li>\n<li>read 堵塞在这里！服务端无法向客户端写入。</li>\n</ul>\n<h2 id=\"并发服务器\"><a href=\"#并发服务器\" class=\"headerlink\" title=\"并发服务器\"></a>并发服务器</h2><p>创建并发服务器有三种方式，Process-based, Event-based, Thread-based。</p>\n<h3 id=\"Process-based\"><a href=\"#Process-based\" class=\"headerlink\" title=\"Process-based\"></a>Process-based</h3>"},{"title":"Network Programming","date":"2021-08-02T03:15:32.000Z","mathjax":true,"cover":"/img/pic3.png","_content":"\n# The Client-Server Programming Model\n\n**transaction**\n\n\n```\n---------------                                              ------------------\n|    Client    |  ------1. Client sends request----------->  |     Server     |                           ---------------\n|    Process   |                                             |     Process    |   <--2.Server process--> |    Resource   |\n----------------  <-----3. Server sends respose------------  ------------------                           ---------------\n4. Client processes reponse\n```\n\n**clients and servers are processes and not machines, or hosts**\n\nA single host can run many different clients and servers concurrently（并发地）. And a client and server transaction can be on tne same or different hosts. \n\n\n\n# Web Servers\nHTTP 是client与server交互的协议。\n\n## Content\nContent是通过HTTP传输的内容，有html. plain. postcript. gif, pang, jpeg等多种类型，通过servers提供content的方式又可以分为serving static conten, serving dynamic content。\n传输的文件都有一个独特的名字URL（universal resource locator）以http://www.google.com:80/index.html为例,client用前缀http:/www.google.com:80决定联系的服务器，服务器位置以及listening的port。servers 用/index.html寻找文件并判断它是动态资源还是静态资源。\n","source":"_posts/Network-Programming.md","raw":"---\ntitle: Network Programming\ndate: 2021-08-02 11:15:32\ntags: 网络编程\ncategories: 深入理解计算机系统\nmathjax: true\ncover:\n---\n\n# The Client-Server Programming Model\n\n**transaction**\n\n\n```\n---------------                                              ------------------\n|    Client    |  ------1. Client sends request----------->  |     Server     |                           ---------------\n|    Process   |                                             |     Process    |   <--2.Server process--> |    Resource   |\n----------------  <-----3. Server sends respose------------  ------------------                           ---------------\n4. Client processes reponse\n```\n\n**clients and servers are processes and not machines, or hosts**\n\nA single host can run many different clients and servers concurrently（并发地）. And a client and server transaction can be on tne same or different hosts. \n\n\n\n# Web Servers\nHTTP 是client与server交互的协议。\n\n## Content\nContent是通过HTTP传输的内容，有html. plain. postcript. gif, pang, jpeg等多种类型，通过servers提供content的方式又可以分为serving static conten, serving dynamic content。\n传输的文件都有一个独特的名字URL（universal resource locator）以http://www.google.com:80/index.html为例,client用前缀http:/www.google.com:80决定联系的服务器，服务器位置以及listening的port。servers 用/index.html寻找文件并判断它是动态资源还是静态资源。\n","slug":"Network-Programming","published":1,"updated":"2021-08-25T10:24:04.377Z","_id":"cksjzdken00026ykk3nhmdcow","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"The-Client-Server-Programming-Model\"><a href=\"#The-Client-Server-Programming-Model\" class=\"headerlink\" title=\"The Client-Server Programming Model\"></a>The Client-Server Programming Model</h1><p><strong>transaction</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---------------                                              ------------------</span><br><span class=\"line\">|    Client    |  ------1. Client sends request-----------&gt;  |     Server     |                           ---------------</span><br><span class=\"line\">|    Process   |                                             |     Process    |   &lt;--2.Server process--&gt; |    Resource   |</span><br><span class=\"line\">----------------  &lt;-----3. Server sends respose------------  ------------------                           ---------------</span><br><span class=\"line\">4. Client processes reponse</span><br></pre></td></tr></table></figure>\n\n<p><strong>clients and servers are processes and not machines, or hosts</strong></p>\n<p>A single host can run many different clients and servers concurrently（并发地）. And a client and server transaction can be on tne same or different hosts. </p>\n<h1 id=\"Web-Servers\"><a href=\"#Web-Servers\" class=\"headerlink\" title=\"Web Servers\"></a>Web Servers</h1><p>HTTP 是client与server交互的协议。</p>\n<h2 id=\"Content\"><a href=\"#Content\" class=\"headerlink\" title=\"Content\"></a>Content</h2><p>Content是通过HTTP传输的内容，有html. plain. postcript. gif, pang, jpeg等多种类型，通过servers提供content的方式又可以分为serving static conten, serving dynamic content。<br>传输的文件都有一个独特的名字URL（universal resource locator）以<a href=\"http://www.google.com/index.html%E4%B8%BA%E4%BE%8B,client%E7%94%A8%E5%89%8D%E7%BC%80http:/www.google.com:80%E5%86%B3%E5%AE%9A%E8%81%94%E7%B3%BB%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%8D%E7%BD%AE%E4%BB%A5%E5%8F%8Alistening%E7%9A%84port%E3%80%82servers\">http://www.google.com:80/index.html为例,client用前缀http:/www.google.com:80决定联系的服务器，服务器位置以及listening的port。servers</a> 用/index.html寻找文件并判断它是动态资源还是静态资源。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"The-Client-Server-Programming-Model\"><a href=\"#The-Client-Server-Programming-Model\" class=\"headerlink\" title=\"The Client-Server Programming Model\"></a>The Client-Server Programming Model</h1><p><strong>transaction</strong></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---------------                                              ------------------</span><br><span class=\"line\">|    Client    |  ------1. Client sends request-----------&gt;  |     Server     |                           ---------------</span><br><span class=\"line\">|    Process   |                                             |     Process    |   &lt;--2.Server process--&gt; |    Resource   |</span><br><span class=\"line\">----------------  &lt;-----3. Server sends respose------------  ------------------                           ---------------</span><br><span class=\"line\">4. Client processes reponse</span><br></pre></td></tr></table></figure>\n\n<p><strong>clients and servers are processes and not machines, or hosts</strong></p>\n<p>A single host can run many different clients and servers concurrently（并发地）. And a client and server transaction can be on tne same or different hosts. </p>\n<h1 id=\"Web-Servers\"><a href=\"#Web-Servers\" class=\"headerlink\" title=\"Web Servers\"></a>Web Servers</h1><p>HTTP 是client与server交互的协议。</p>\n<h2 id=\"Content\"><a href=\"#Content\" class=\"headerlink\" title=\"Content\"></a>Content</h2><p>Content是通过HTTP传输的内容，有html. plain. postcript. gif, pang, jpeg等多种类型，通过servers提供content的方式又可以分为serving static conten, serving dynamic content。<br>传输的文件都有一个独特的名字URL（universal resource locator）以<a href=\"http://www.google.com/index.html%E4%B8%BA%E4%BE%8B,client%E7%94%A8%E5%89%8D%E7%BC%80http:/www.google.com:80%E5%86%B3%E5%AE%9A%E8%81%94%E7%B3%BB%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%8D%E7%BD%AE%E4%BB%A5%E5%8F%8Alistening%E7%9A%84port%E3%80%82servers\">http://www.google.com:80/index.html为例,client用前缀http:/www.google.com:80决定联系的服务器，服务器位置以及listening的port。servers</a> 用/index.html寻找文件并判断它是动态资源还是静态资源。</p>\n"},{"title":"Pytorch Dataloader","date":"2021-08-10T02:32:41.000Z","_content":"\n# Pytorch DataLoader\nDataLoader是pytorch中的一个高级的迭代器，支持\n- Batching the data\n- Shuffling the data\n- Load the data in parallel using multiprocessing workers\n\n使用时需要继承torch.utils.dadta.Dataset类，并重写函数：\n- **__len__** 返回数据集的总量\n- **__getitem__** 通过索引访问数据\n\n\n","source":"_posts/Pytorch-Dataloader.md","raw":"---\ntitle: Pytorch Dataloader\ndate: 2021-08-10 10:32:41\ntags:\n---\n\n# Pytorch DataLoader\nDataLoader是pytorch中的一个高级的迭代器，支持\n- Batching the data\n- Shuffling the data\n- Load the data in parallel using multiprocessing workers\n\n使用时需要继承torch.utils.dadta.Dataset类，并重写函数：\n- **__len__** 返回数据集的总量\n- **__getitem__** 通过索引访问数据\n\n\n","slug":"Pytorch-Dataloader","published":1,"updated":"2021-08-10T14:00:36.588Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cksjzdken00036ykk58he9jm6","content":"<h1 id=\"Pytorch-DataLoader\"><a href=\"#Pytorch-DataLoader\" class=\"headerlink\" title=\"Pytorch DataLoader\"></a>Pytorch DataLoader</h1><p>DataLoader是pytorch中的一个高级的迭代器，支持</p>\n<ul>\n<li>Batching the data</li>\n<li>Shuffling the data</li>\n<li>Load the data in parallel using multiprocessing workers</li>\n</ul>\n<p>使用时需要继承torch.utils.dadta.Dataset类，并重写函数：</p>\n<ul>\n<li><strong><strong>len</strong></strong> 返回数据集的总量</li>\n<li><strong><strong>getitem</strong></strong> 通过索引访问数据</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Pytorch-DataLoader\"><a href=\"#Pytorch-DataLoader\" class=\"headerlink\" title=\"Pytorch DataLoader\"></a>Pytorch DataLoader</h1><p>DataLoader是pytorch中的一个高级的迭代器，支持</p>\n<ul>\n<li>Batching the data</li>\n<li>Shuffling the data</li>\n<li>Load the data in parallel using multiprocessing workers</li>\n</ul>\n<p>使用时需要继承torch.utils.dadta.Dataset类，并重写函数：</p>\n<ul>\n<li><strong><strong>len</strong></strong> 返回数据集的总量</li>\n<li><strong><strong>getitem</strong></strong> 通过索引访问数据</li>\n</ul>\n"},{"title":"Pytorch Embedding","date":"2021-08-10T02:46:30.000Z","_content":"\n# Pytroch Embedding\n\n## Encodings One-Hot\n\n","source":"_posts/Pytorch-Embedding.md","raw":"---\ntitle: Pytorch Embedding\ndate: 2021-08-10 10:46:30\ntags:\n---\n\n# Pytroch Embedding\n\n## Encodings One-Hot\n\n","slug":"Pytorch-Embedding","published":1,"updated":"2021-08-10T02:47:25.240Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cksjzdkep00046ykk1qsz0f1c","content":"<h1 id=\"Pytroch-Embedding\"><a href=\"#Pytroch-Embedding\" class=\"headerlink\" title=\"Pytroch Embedding\"></a>Pytroch Embedding</h1><h2 id=\"Encodings-One-Hot\"><a href=\"#Encodings-One-Hot\" class=\"headerlink\" title=\"Encodings One-Hot\"></a>Encodings One-Hot</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Pytroch-Embedding\"><a href=\"#Pytroch-Embedding\" class=\"headerlink\" title=\"Pytroch Embedding\"></a>Pytroch Embedding</h1><h2 id=\"Encodings-One-Hot\"><a href=\"#Encodings-One-Hot\" class=\"headerlink\" title=\"Encodings One-Hot\"></a>Encodings One-Hot</h2>"},{"title":"System Level I/O","date":"2021-08-10T13:28:15.000Z","cover":"/img/pic3.png","_content":"\n\n# System Level I/O\n\n## Unix I/O\n\n在Unix系统中，一切对象都是**文件**。而文件只是一组字节序列。甚至连接到系统的设备，Socket都视为文件。\n\n文件的分类\n- Regular file: Contains arbitrary data\n- Directory: Index for a related group of files\n- Socket: For communicating with a process on another machine\n- Named pipes(管道)\n- Symbolic links\n- Character and block decvices\n\n\n### Regula files \nC语言标准库中的API将常规文件分为text file, binary file。但是系统内核是不对这两者做区分的。\n\n>Linux/MAC 用'\\n', '\\r\\n'\n\n### Directories\n一组links组成了目录文件，每一个link是文件名到文件的映射。\n\n## **File Descriptor**\n文件操作符，一个小的整数，标识打开的文件。\n\n每个进程都会维护一个文件描述符表，文件描述符也就是表的索引，表的每一项是对打开的文件的引用。同时系统还维护另外两张表，分别是打开文件表，v-node表，这两者被所有进程共享。文件描述符表只由每一个进程访问，但是子进程可以继承父进程的文件描述符表。\n\n![threeTable](table.jpg)\n\n> 每个进程的描述符0， 1， 2对应的指针是固定指向stdin, stdout, stderr的。\n\n## 重定向\n可以通过C语言的函数调用**dup(old, new)**实现。首先将new的文件关闭，再把new指向old所指向的。重定向也可以用linux实现，即 **>**。如 ls>test.txt。\n\n\n\n-----\n\n## Unix I/O\nread, write, open, close是最底层的API,属于系统调用。\n\n### Reading Files & Writing Files\n需要文件描述符fd和指向buffer的指针及其大小。\n> Tips : strace 可以检查程序的系统级调用。\n读写文件时，当字节数大于1小于最大值时，称作Short Counts,发生在以下三种情况\n\n- 遇到EOF\n- 在终端读入时遇到换行符\n- 读写networks sockets\n\n不会发生Short Counts的情况\n- 从disk读入\n- 向disk写入\n\n\n### 带缓冲与非缓冲I/O\n\nRIO是健壮的I/O包，提供了非缓存和缓存的两类函数。\n\n非缓存函数将bytes直接读入到目标内存。而缓存函数通过调用非缓存函数将bytes读入到特定的内存区域中，再实现内存到内存的读写。相对于内存的读写来说，系统的调用效率非常低。通过带缓冲的函数可以提高效率。\n\n但是问题在于两者不都是需要先调用系统函数吗，带缓冲的究竟快在哪儿呢？原因在于可以减少系统调用的次数，因为缓冲区足够大，所以可以尽可能多的一次取完，再由目标地址分批从该缓存内读入，如果非缓存会增多系统调用的次数。\n\nRIO中的缓存区由一个结构体控制，其中的指针指向一片内存。\n\n### File Metadata （文件元数据）\n描述文件信息的数据，它们存放在结构体stat中。这些数据描述了文件是否可读，文件类型等基本信息。\n\n![metadataInfo](info.jpg)\n\n\n## Standard I/O\nStandard I/O 指的其实就是Ｃ语言中常见的I/O函数，如scanf,fscanf,printf,它们都是带缓冲的I/O。\n\n> Standard I/O 常用于终端文件读写，而RIO常用于网络编程。\n\n\n\n\n","source":"_posts/System-Level-I-O.md","raw":"---\ntitle: System Level I/O\ndate: 2021-08-10 21:28:15\ncategories: 深入理解计算机系统\ntags: Linux文件\ncover:\n---\n\n\n# System Level I/O\n\n## Unix I/O\n\n在Unix系统中，一切对象都是**文件**。而文件只是一组字节序列。甚至连接到系统的设备，Socket都视为文件。\n\n文件的分类\n- Regular file: Contains arbitrary data\n- Directory: Index for a related group of files\n- Socket: For communicating with a process on another machine\n- Named pipes(管道)\n- Symbolic links\n- Character and block decvices\n\n\n### Regula files \nC语言标准库中的API将常规文件分为text file, binary file。但是系统内核是不对这两者做区分的。\n\n>Linux/MAC 用'\\n', '\\r\\n'\n\n### Directories\n一组links组成了目录文件，每一个link是文件名到文件的映射。\n\n## **File Descriptor**\n文件操作符，一个小的整数，标识打开的文件。\n\n每个进程都会维护一个文件描述符表，文件描述符也就是表的索引，表的每一项是对打开的文件的引用。同时系统还维护另外两张表，分别是打开文件表，v-node表，这两者被所有进程共享。文件描述符表只由每一个进程访问，但是子进程可以继承父进程的文件描述符表。\n\n![threeTable](table.jpg)\n\n> 每个进程的描述符0， 1， 2对应的指针是固定指向stdin, stdout, stderr的。\n\n## 重定向\n可以通过C语言的函数调用**dup(old, new)**实现。首先将new的文件关闭，再把new指向old所指向的。重定向也可以用linux实现，即 **>**。如 ls>test.txt。\n\n\n\n-----\n\n## Unix I/O\nread, write, open, close是最底层的API,属于系统调用。\n\n### Reading Files & Writing Files\n需要文件描述符fd和指向buffer的指针及其大小。\n> Tips : strace 可以检查程序的系统级调用。\n读写文件时，当字节数大于1小于最大值时，称作Short Counts,发生在以下三种情况\n\n- 遇到EOF\n- 在终端读入时遇到换行符\n- 读写networks sockets\n\n不会发生Short Counts的情况\n- 从disk读入\n- 向disk写入\n\n\n### 带缓冲与非缓冲I/O\n\nRIO是健壮的I/O包，提供了非缓存和缓存的两类函数。\n\n非缓存函数将bytes直接读入到目标内存。而缓存函数通过调用非缓存函数将bytes读入到特定的内存区域中，再实现内存到内存的读写。相对于内存的读写来说，系统的调用效率非常低。通过带缓冲的函数可以提高效率。\n\n但是问题在于两者不都是需要先调用系统函数吗，带缓冲的究竟快在哪儿呢？原因在于可以减少系统调用的次数，因为缓冲区足够大，所以可以尽可能多的一次取完，再由目标地址分批从该缓存内读入，如果非缓存会增多系统调用的次数。\n\nRIO中的缓存区由一个结构体控制，其中的指针指向一片内存。\n\n### File Metadata （文件元数据）\n描述文件信息的数据，它们存放在结构体stat中。这些数据描述了文件是否可读，文件类型等基本信息。\n\n![metadataInfo](info.jpg)\n\n\n## Standard I/O\nStandard I/O 指的其实就是Ｃ语言中常见的I/O函数，如scanf,fscanf,printf,它们都是带缓冲的I/O。\n\n> Standard I/O 常用于终端文件读写，而RIO常用于网络编程。\n\n\n\n\n","slug":"System-Level-I-O","published":1,"updated":"2021-08-25T15:31:02.125Z","_id":"cksjzdkeq00056ykkdus75amu","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"System-Level-I-O\"><a href=\"#System-Level-I-O\" class=\"headerlink\" title=\"System Level I/O\"></a>System Level I/O</h1><h2 id=\"Unix-I-O\"><a href=\"#Unix-I-O\" class=\"headerlink\" title=\"Unix I/O\"></a>Unix I/O</h2><p>在Unix系统中，一切对象都是<strong>文件</strong>。而文件只是一组字节序列。甚至连接到系统的设备，Socket都视为文件。</p>\n<p>文件的分类</p>\n<ul>\n<li>Regular file: Contains arbitrary data</li>\n<li>Directory: Index for a related group of files</li>\n<li>Socket: For communicating with a process on another machine</li>\n<li>Named pipes(管道)</li>\n<li>Symbolic links</li>\n<li>Character and block decvices</li>\n</ul>\n<h3 id=\"Regula-files\"><a href=\"#Regula-files\" class=\"headerlink\" title=\"Regula files\"></a>Regula files</h3><p>C语言标准库中的API将常规文件分为text file, binary file。但是系统内核是不对这两者做区分的。</p>\n<blockquote>\n<p>Linux/MAC 用’\\n’, ‘\\r\\n’</p>\n</blockquote>\n<h3 id=\"Directories\"><a href=\"#Directories\" class=\"headerlink\" title=\"Directories\"></a>Directories</h3><p>一组links组成了目录文件，每一个link是文件名到文件的映射。</p>\n<h2 id=\"File-Descriptor\"><a href=\"#File-Descriptor\" class=\"headerlink\" title=\"File Descriptor\"></a><strong>File Descriptor</strong></h2><p>文件操作符，一个小的整数，标识打开的文件。</p>\n<p>每个进程都会维护一个文件描述符表，文件描述符也就是表的索引，表的每一项是对打开的文件的引用。同时系统还维护另外两张表，分别是打开文件表，v-node表，这两者被所有进程共享。文件描述符表只由每一个进程访问，但是子进程可以继承父进程的文件描述符表。</p>\n<p><img src=\"/2021/08/10/System-Level-I-O/table.jpg\" alt=\"threeTable\"></p>\n<blockquote>\n<p>每个进程的描述符0， 1， 2对应的指针是固定指向stdin, stdout, stderr的。</p>\n</blockquote>\n<h2 id=\"重定向\"><a href=\"#重定向\" class=\"headerlink\" title=\"重定向\"></a>重定向</h2><p>可以通过C语言的函数调用**dup(old, new)**实现。首先将new的文件关闭，再把new指向old所指向的。重定向也可以用linux实现，即 **&gt;**。如 ls&gt;test.txt。</p>\n<hr>\n<h2 id=\"Unix-I-O-1\"><a href=\"#Unix-I-O-1\" class=\"headerlink\" title=\"Unix I/O\"></a>Unix I/O</h2><p>read, write, open, close是最底层的API,属于系统调用。</p>\n<h3 id=\"Reading-Files-amp-Writing-Files\"><a href=\"#Reading-Files-amp-Writing-Files\" class=\"headerlink\" title=\"Reading Files &amp; Writing Files\"></a>Reading Files &amp; Writing Files</h3><p>需要文件描述符fd和指向buffer的指针及其大小。</p>\n<blockquote>\n<p>Tips : strace 可以检查程序的系统级调用。<br>读写文件时，当字节数大于1小于最大值时，称作Short Counts,发生在以下三种情况</p>\n</blockquote>\n<ul>\n<li>遇到EOF</li>\n<li>在终端读入时遇到换行符</li>\n<li>读写networks sockets</li>\n</ul>\n<p>不会发生Short Counts的情况</p>\n<ul>\n<li>从disk读入</li>\n<li>向disk写入</li>\n</ul>\n<h3 id=\"带缓冲与非缓冲I-O\"><a href=\"#带缓冲与非缓冲I-O\" class=\"headerlink\" title=\"带缓冲与非缓冲I/O\"></a>带缓冲与非缓冲I/O</h3><p>RIO是健壮的I/O包，提供了非缓存和缓存的两类函数。</p>\n<p>非缓存函数将bytes直接读入到目标内存。而缓存函数通过调用非缓存函数将bytes读入到特定的内存区域中，再实现内存到内存的读写。相对于内存的读写来说，系统的调用效率非常低。通过带缓冲的函数可以提高效率。</p>\n<p>但是问题在于两者不都是需要先调用系统函数吗，带缓冲的究竟快在哪儿呢？原因在于可以减少系统调用的次数，因为缓冲区足够大，所以可以尽可能多的一次取完，再由目标地址分批从该缓存内读入，如果非缓存会增多系统调用的次数。</p>\n<p>RIO中的缓存区由一个结构体控制，其中的指针指向一片内存。</p>\n<h3 id=\"File-Metadata-（文件元数据）\"><a href=\"#File-Metadata-（文件元数据）\" class=\"headerlink\" title=\"File Metadata （文件元数据）\"></a>File Metadata （文件元数据）</h3><p>描述文件信息的数据，它们存放在结构体stat中。这些数据描述了文件是否可读，文件类型等基本信息。</p>\n<p><img src=\"/2021/08/10/System-Level-I-O/info.jpg\" alt=\"metadataInfo\"></p>\n<h2 id=\"Standard-I-O\"><a href=\"#Standard-I-O\" class=\"headerlink\" title=\"Standard I/O\"></a>Standard I/O</h2><p>Standard I/O 指的其实就是Ｃ语言中常见的I/O函数，如scanf,fscanf,printf,它们都是带缓冲的I/O。</p>\n<blockquote>\n<p>Standard I/O 常用于终端文件读写，而RIO常用于网络编程。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"System-Level-I-O\"><a href=\"#System-Level-I-O\" class=\"headerlink\" title=\"System Level I/O\"></a>System Level I/O</h1><h2 id=\"Unix-I-O\"><a href=\"#Unix-I-O\" class=\"headerlink\" title=\"Unix I/O\"></a>Unix I/O</h2><p>在Unix系统中，一切对象都是<strong>文件</strong>。而文件只是一组字节序列。甚至连接到系统的设备，Socket都视为文件。</p>\n<p>文件的分类</p>\n<ul>\n<li>Regular file: Contains arbitrary data</li>\n<li>Directory: Index for a related group of files</li>\n<li>Socket: For communicating with a process on another machine</li>\n<li>Named pipes(管道)</li>\n<li>Symbolic links</li>\n<li>Character and block decvices</li>\n</ul>\n<h3 id=\"Regula-files\"><a href=\"#Regula-files\" class=\"headerlink\" title=\"Regula files\"></a>Regula files</h3><p>C语言标准库中的API将常规文件分为text file, binary file。但是系统内核是不对这两者做区分的。</p>\n<blockquote>\n<p>Linux/MAC 用’\\n’, ‘\\r\\n’</p>\n</blockquote>\n<h3 id=\"Directories\"><a href=\"#Directories\" class=\"headerlink\" title=\"Directories\"></a>Directories</h3><p>一组links组成了目录文件，每一个link是文件名到文件的映射。</p>\n<h2 id=\"File-Descriptor\"><a href=\"#File-Descriptor\" class=\"headerlink\" title=\"File Descriptor\"></a><strong>File Descriptor</strong></h2><p>文件操作符，一个小的整数，标识打开的文件。</p>\n<p>每个进程都会维护一个文件描述符表，文件描述符也就是表的索引，表的每一项是对打开的文件的引用。同时系统还维护另外两张表，分别是打开文件表，v-node表，这两者被所有进程共享。文件描述符表只由每一个进程访问，但是子进程可以继承父进程的文件描述符表。</p>\n<p><img src=\"/2021/08/10/System-Level-I-O/table.jpg\" alt=\"threeTable\"></p>\n<blockquote>\n<p>每个进程的描述符0， 1， 2对应的指针是固定指向stdin, stdout, stderr的。</p>\n</blockquote>\n<h2 id=\"重定向\"><a href=\"#重定向\" class=\"headerlink\" title=\"重定向\"></a>重定向</h2><p>可以通过C语言的函数调用**dup(old, new)**实现。首先将new的文件关闭，再把new指向old所指向的。重定向也可以用linux实现，即 **&gt;**。如 ls&gt;test.txt。</p>\n<hr>\n<h2 id=\"Unix-I-O-1\"><a href=\"#Unix-I-O-1\" class=\"headerlink\" title=\"Unix I/O\"></a>Unix I/O</h2><p>read, write, open, close是最底层的API,属于系统调用。</p>\n<h3 id=\"Reading-Files-amp-Writing-Files\"><a href=\"#Reading-Files-amp-Writing-Files\" class=\"headerlink\" title=\"Reading Files &amp; Writing Files\"></a>Reading Files &amp; Writing Files</h3><p>需要文件描述符fd和指向buffer的指针及其大小。</p>\n<blockquote>\n<p>Tips : strace 可以检查程序的系统级调用。<br>读写文件时，当字节数大于1小于最大值时，称作Short Counts,发生在以下三种情况</p>\n</blockquote>\n<ul>\n<li>遇到EOF</li>\n<li>在终端读入时遇到换行符</li>\n<li>读写networks sockets</li>\n</ul>\n<p>不会发生Short Counts的情况</p>\n<ul>\n<li>从disk读入</li>\n<li>向disk写入</li>\n</ul>\n<h3 id=\"带缓冲与非缓冲I-O\"><a href=\"#带缓冲与非缓冲I-O\" class=\"headerlink\" title=\"带缓冲与非缓冲I/O\"></a>带缓冲与非缓冲I/O</h3><p>RIO是健壮的I/O包，提供了非缓存和缓存的两类函数。</p>\n<p>非缓存函数将bytes直接读入到目标内存。而缓存函数通过调用非缓存函数将bytes读入到特定的内存区域中，再实现内存到内存的读写。相对于内存的读写来说，系统的调用效率非常低。通过带缓冲的函数可以提高效率。</p>\n<p>但是问题在于两者不都是需要先调用系统函数吗，带缓冲的究竟快在哪儿呢？原因在于可以减少系统调用的次数，因为缓冲区足够大，所以可以尽可能多的一次取完，再由目标地址分批从该缓存内读入，如果非缓存会增多系统调用的次数。</p>\n<p>RIO中的缓存区由一个结构体控制，其中的指针指向一片内存。</p>\n<h3 id=\"File-Metadata-（文件元数据）\"><a href=\"#File-Metadata-（文件元数据）\" class=\"headerlink\" title=\"File Metadata （文件元数据）\"></a>File Metadata （文件元数据）</h3><p>描述文件信息的数据，它们存放在结构体stat中。这些数据描述了文件是否可读，文件类型等基本信息。</p>\n<p><img src=\"/2021/08/10/System-Level-I-O/info.jpg\" alt=\"metadataInfo\"></p>\n<h2 id=\"Standard-I-O\"><a href=\"#Standard-I-O\" class=\"headerlink\" title=\"Standard I/O\"></a>Standard I/O</h2><p>Standard I/O 指的其实就是Ｃ语言中常见的I/O函数，如scanf,fscanf,printf,它们都是带缓冲的I/O。</p>\n<blockquote>\n<p>Standard I/O 常用于终端文件读写，而RIO常用于网络编程。</p>\n</blockquote>\n"},{"title":"deep learning part 1","date":"2021-08-09T07:39:11.000Z","_content":"","source":"_posts/deep-learning-part-1.md","raw":"---\ntitle: deep learning part 1\ndate: 2021-08-09 15:39:11\ntags:\n---\n","slug":"deep-learning-part-1","published":1,"updated":"2021-08-09T07:39:11.306Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cksjzdkes00066ykka0fd8n23","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"surprise","date":"2021-08-05T16:39:57.000Z","_content":"\n\n\nWelcome to my fiendish little bomb. You have 6 phases with\nwhich to blow yourself up. Have a nice day!\nBorder relations with Canada have never been better.\nPhase 1 defused. How about the next one?\n1 2 4 8 16 32\nThat's number 2.  Keep going!\n^CSo you think you can stop the bomb with ctrl-c, do you?\nWell...OK. :-)\n\n","source":"_posts/surprise.md","raw":"---\ntitle: surprise\ndate: 2021-08-06 00:39:57\ntags:\n---\n\n\n\nWelcome to my fiendish little bomb. You have 6 phases with\nwhich to blow yourself up. Have a nice day!\nBorder relations with Canada have never been better.\nPhase 1 defused. How about the next one?\n1 2 4 8 16 32\nThat's number 2.  Keep going!\n^CSo you think you can stop the bomb with ctrl-c, do you?\nWell...OK. :-)\n\n","slug":"surprise","published":1,"updated":"2021-08-05T16:40:28.183Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cksjzdkes00076ykk2enqfjjw","content":"<p>Welcome to my fiendish little bomb. You have 6 phases with<br>which to blow yourself up. Have a nice day!<br>Border relations with Canada have never been better.<br>Phase 1 defused. How about the next one?<br>1 2 4 8 16 32<br>That’s number 2.  Keep going!<br>^CSo you think you can stop the bomb with ctrl-c, do you?<br>Well…OK. :-)</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to my fiendish little bomb. You have 6 phases with<br>which to blow yourself up. Have a nice day!<br>Border relations with Canada have never been better.<br>Phase 1 defused. How about the next one?<br>1 2 4 8 16 32<br>That’s number 2.  Keep going!<br>^CSo you think you can stop the bomb with ctrl-c, do you?<br>Well…OK. :-)</p>\n"},{"title":"Exceptional Control Flow","date":"2021-08-11T15:57:54.000Z","keywords":"并发 信号","mathjax":true,"cover":"/img/pic1.png","_content":"\n# Exceptional Control Flow\n\n机器的一系列指令执行称作control flow,在汇编一章中，我们已经了解了**程序级别**的控制流变化,例如用Jumps and branches, call and return来改变指令操作的顺序。接下来我们要学习的是**系统级别**的改变,例如数据在磁盘和内存的运输，键盘上的Ctrl-C停止的作用等，也就是**exceptional control flow**（异常控制流）的机制。\n\n\n# Exceptions\nAn exception is a transfer of control to the OS kernel in response to some event (i.e., change in processor state)。也即异常是内核对事件的响应，是一种低级别的机制。\n\n![](Exceptional-Control-Flow/exceptions.jpg)\n\n\n> Exception Tables\n> Events组成一个表，当Event k发生时，系统查询索引为k的地址，得到处理异常的程序。\n\n## 同步与异步\n\n### 异步异常(Asynchronous Exceptions)\n异步异常是由外部时间触发的对处理器的操作，也被称为Interrupts。例如每隔几毫秒，外部的timer chip触发一次异常来从用户程序中拿回控制权，在终端输入Ctrl-C等。\n\n### 同步异常(Synchronous Exceptions)\n在执行指令中遇到了相应的Events。\n\n- Traps 人为地设置，如断点调试，系统调用等，结果是返回控制权给下一条指令。\n- Faults 非故意但可恢复或者不可恢复的，如浮点数的异常等，结果是重新执行此条指令或者终止\n- Aborts 非故意且不可恢复的，结果是停止此程序。\n\n## Processes\n**A process is an instatnce of a running program**\n\n![](process.jpg)\n\n一个进程的假象：\n- logical control flow 每个进程似乎独占CPU，由上下文切换实现\n- private address space 似乎都有自己的一块内存空间，由虚拟内存实现\n\n### Concurrent Processes\n并发进程几乎无时无刻不在发生，接下来我们将从上下文切换的角度理解。\n\n我们将process视为一个逻辑控制流，当两个逻辑控制流在时间上重叠时，它们是**并发**的。\n\n当处理器是单核时，CPU每次执行一个process，为了实现并发，它在两个process间切换执行。当处理器是多核时，每一个cpu都可以单独执行一个process，同样的此时每一个核又可以并发。\n\n![](context_switch.jpg)\n\n> 上下文切换的细节：将上一个进程的寄存器的值保存在内存中（相当于caller saved?）。上下文切换从代码角度，是一个user code到另一个user code的过渡，而这个过渡由kernel code实现。\n\n\n## Process Control(API)\n从程序员的角度考虑，进程有三种状态，Running, Stopped, Terminated。以下的函数是系统调用，可以改变进程的状态。\n\n### exit \nTerminate the program with status, 正常退出为0，非正常退出返回非0值，exit只会被执行一次，不会return\n\n\n### Fork\nfork函数构建一个子进程。\n\nfork被调用一次，但是会有两个返回值，在父进程中返回子进程号，在子进程中返回0，可以通过返回值来分辨此时在哪个进程中。可以把返回值以链表的形式理解，父进程指向下一个子进程，而子进程如果在链表末端时，则指向NULL也即是0。\n\nfork的返回值很重要，每次必须判断是否成功创建子进程，为了使判断的代码更加compact，所以用Fork重写代码。\n```C\n\nvoid unix_error(char* msg)\n{\n\tfprintf(stderr, \"%s: %s\\n\", msg, strerror(errno));\n\texit(-1);\n}\n\npid_t Fork(void)\n{\n\tpid_t pid;\n\n\tif((pid = fork())<0)\n\t\tunix_error(\"Fork error\");\n\treturn pid;\n}\n```\n\n下面来看一个例子：\n```C\n#include \"csapp.h\"\n\nint main()\n{\n    int x = 1;\n\n    if(Fork() == 0)\n        printf(\"p1: %d\\n\", ++x);\n    printf(\"p2: %d\\n\", --x);\n\n    return 0;\n}\n```\n```bash\n//输出\np2: 0 //父进程\np1: 2 //子进程\np2: 1 //子进程\n```\n首先执行到Fork时，就像薛定谔的猫，进程处于量子叠加态，既是父进程也是子进程。接下来用一个if判断将两者分离。在这个例子中，我们很容易发现，每个进程都有自己变量的拷贝，x互不影响，那么它们的虚拟地址如何呢？\n\n更改代码为输出地址，结果如下。\n\n```bash\np2: 0x7fffd2bc7fe4\np1: 0x7fffd2bc7fe4\np2: 0x7fffd2bc7fe4\n```\n\n可以看到变量的虚拟地址都是相同的，但它们的实际地址不同。\n\n\n### 僵尸进程与wait回收\n\n当子进程结束运行，而父进程没有对其回收的话，子进程沦为僵尸进程，还占用资源。\n如果父进程被kill，子进程结束运行，子进程会由**init**回收。\n如果父进程结束运行，子进程不终止，那么子进程无法结束、回收。\n使用wait可以使父进程显示地对子进程回收，父进程会一直suspend直到子进程终止。\n\n\n### execve\n```C\n#include <unistd.h>\n\nint execve(const char *filenaem, const char *argv[],\n\t\t   const char *envp[]);\n\t\t   //Does not return if OK;returns -1 on error\n```\n执行程序(第三个参数直接写入全局变量 environ，由libc决定)\n\n\n\n## Signals\n信号是传送给process的一种信息，可以用于进程间的通信。例如当键盘键入Ctrl+C时，kenerl终止所有foreground的process通过信号SIGINT。总的来说，信号的起作用可以分为两步：sending/delivering　&　receiving\n\n### sending \n\n信号总是由kernel发送的。kernel可以通过某些event的触发自主发送signal，process也可以通过kill函数的调用请求kernel发送signal。\n\n### kill\n```C\n#include <sys/types.h>\n#include <signal.h>\n\nint kill(pid_t pid, int sig);\n```\n\n- pid>0: 发送信号给pid的process\n- pid=0: 发送信号给调用process所在process gruop的所有processes\n- pid<0: 发送信号给pid group为-pid的所有processes\n\n### receiving\n信号的receive是在上下文切换中完成的，也就是从kenel code切换回user code后起作用。\n\n信号发送但是未被received（未被处理时）称为pending。信号在这个阶段起作用靠pending bit vector & blocked bit vector，不同信号的信号发送会储存在pending bit vector中，相同类型的会被抛弃（该bit已经被置1了）。bloked bit vector则是掩码，设置为1的位置对应信号不会起作用，因为最后信号的处理是由**pending & ~blocked**(unblocked pending signals)决定。当掩码起作用时，该信号被**忽略**。\n\nSignal接收的时机是在进入进程前，kernal会检查是否有待处理的信号\n\n![Receive的时机](signal_received.jpg)\n\n### Catch\n\n每个Signal有默认的执行结果，用户可以通过修改__singal_handler__函数以自定义Signal的作用。Kernel执行signal_hander称为信号的**捕获（catch）**。\n\n![常见的信号及作用](signal_type.jpg)\n\n![signal_handler](signal_handler.jpg)\n\n> signal_handelr也属于进程调用的一部分，可以被其他信号处理函数打断\n\n但是信号9 SIGKILL和信号19 SIGSTOP不可被忽略或重写。\n\n**SIGCHLD**: 当子进程的**状态改变**时，如终止，暂停等，都会发送给父进程，所以可以重设signal_chld_handler,先判断进程的情况再操作。\n\n### Handlers的处理守则\n![handler guidline](guidelines.jpg)\n\n## 竞争\n由于不知道子进程和父进程的执行先后顺序，当它们都会对某一对象操作时，其实际先后顺序可能并不是我们想要的，这称之为**竞争（Race）**。避免竞争一种的办法是显示的阻塞信号，这样可以避免进程和handler之间的竞争。\n\n![race](race.jpg)\n","source":"_posts/Exceptional-Control-Flow.md","raw":"---\ntitle: Exceptional Control Flow\ndate: 2021-08-11 23:57:54\ntags: 异常控制流\nkeywords: 并发 信号\nmathjax: true\ncategories: 深入理解计算机系统\ncover:\n---\n\n# Exceptional Control Flow\n\n机器的一系列指令执行称作control flow,在汇编一章中，我们已经了解了**程序级别**的控制流变化,例如用Jumps and branches, call and return来改变指令操作的顺序。接下来我们要学习的是**系统级别**的改变,例如数据在磁盘和内存的运输，键盘上的Ctrl-C停止的作用等，也就是**exceptional control flow**（异常控制流）的机制。\n\n\n# Exceptions\nAn exception is a transfer of control to the OS kernel in response to some event (i.e., change in processor state)。也即异常是内核对事件的响应，是一种低级别的机制。\n\n![](Exceptional-Control-Flow/exceptions.jpg)\n\n\n> Exception Tables\n> Events组成一个表，当Event k发生时，系统查询索引为k的地址，得到处理异常的程序。\n\n## 同步与异步\n\n### 异步异常(Asynchronous Exceptions)\n异步异常是由外部时间触发的对处理器的操作，也被称为Interrupts。例如每隔几毫秒，外部的timer chip触发一次异常来从用户程序中拿回控制权，在终端输入Ctrl-C等。\n\n### 同步异常(Synchronous Exceptions)\n在执行指令中遇到了相应的Events。\n\n- Traps 人为地设置，如断点调试，系统调用等，结果是返回控制权给下一条指令。\n- Faults 非故意但可恢复或者不可恢复的，如浮点数的异常等，结果是重新执行此条指令或者终止\n- Aborts 非故意且不可恢复的，结果是停止此程序。\n\n## Processes\n**A process is an instatnce of a running program**\n\n![](process.jpg)\n\n一个进程的假象：\n- logical control flow 每个进程似乎独占CPU，由上下文切换实现\n- private address space 似乎都有自己的一块内存空间，由虚拟内存实现\n\n### Concurrent Processes\n并发进程几乎无时无刻不在发生，接下来我们将从上下文切换的角度理解。\n\n我们将process视为一个逻辑控制流，当两个逻辑控制流在时间上重叠时，它们是**并发**的。\n\n当处理器是单核时，CPU每次执行一个process，为了实现并发，它在两个process间切换执行。当处理器是多核时，每一个cpu都可以单独执行一个process，同样的此时每一个核又可以并发。\n\n![](context_switch.jpg)\n\n> 上下文切换的细节：将上一个进程的寄存器的值保存在内存中（相当于caller saved?）。上下文切换从代码角度，是一个user code到另一个user code的过渡，而这个过渡由kernel code实现。\n\n\n## Process Control(API)\n从程序员的角度考虑，进程有三种状态，Running, Stopped, Terminated。以下的函数是系统调用，可以改变进程的状态。\n\n### exit \nTerminate the program with status, 正常退出为0，非正常退出返回非0值，exit只会被执行一次，不会return\n\n\n### Fork\nfork函数构建一个子进程。\n\nfork被调用一次，但是会有两个返回值，在父进程中返回子进程号，在子进程中返回0，可以通过返回值来分辨此时在哪个进程中。可以把返回值以链表的形式理解，父进程指向下一个子进程，而子进程如果在链表末端时，则指向NULL也即是0。\n\nfork的返回值很重要，每次必须判断是否成功创建子进程，为了使判断的代码更加compact，所以用Fork重写代码。\n```C\n\nvoid unix_error(char* msg)\n{\n\tfprintf(stderr, \"%s: %s\\n\", msg, strerror(errno));\n\texit(-1);\n}\n\npid_t Fork(void)\n{\n\tpid_t pid;\n\n\tif((pid = fork())<0)\n\t\tunix_error(\"Fork error\");\n\treturn pid;\n}\n```\n\n下面来看一个例子：\n```C\n#include \"csapp.h\"\n\nint main()\n{\n    int x = 1;\n\n    if(Fork() == 0)\n        printf(\"p1: %d\\n\", ++x);\n    printf(\"p2: %d\\n\", --x);\n\n    return 0;\n}\n```\n```bash\n//输出\np2: 0 //父进程\np1: 2 //子进程\np2: 1 //子进程\n```\n首先执行到Fork时，就像薛定谔的猫，进程处于量子叠加态，既是父进程也是子进程。接下来用一个if判断将两者分离。在这个例子中，我们很容易发现，每个进程都有自己变量的拷贝，x互不影响，那么它们的虚拟地址如何呢？\n\n更改代码为输出地址，结果如下。\n\n```bash\np2: 0x7fffd2bc7fe4\np1: 0x7fffd2bc7fe4\np2: 0x7fffd2bc7fe4\n```\n\n可以看到变量的虚拟地址都是相同的，但它们的实际地址不同。\n\n\n### 僵尸进程与wait回收\n\n当子进程结束运行，而父进程没有对其回收的话，子进程沦为僵尸进程，还占用资源。\n如果父进程被kill，子进程结束运行，子进程会由**init**回收。\n如果父进程结束运行，子进程不终止，那么子进程无法结束、回收。\n使用wait可以使父进程显示地对子进程回收，父进程会一直suspend直到子进程终止。\n\n\n### execve\n```C\n#include <unistd.h>\n\nint execve(const char *filenaem, const char *argv[],\n\t\t   const char *envp[]);\n\t\t   //Does not return if OK;returns -1 on error\n```\n执行程序(第三个参数直接写入全局变量 environ，由libc决定)\n\n\n\n## Signals\n信号是传送给process的一种信息，可以用于进程间的通信。例如当键盘键入Ctrl+C时，kenerl终止所有foreground的process通过信号SIGINT。总的来说，信号的起作用可以分为两步：sending/delivering　&　receiving\n\n### sending \n\n信号总是由kernel发送的。kernel可以通过某些event的触发自主发送signal，process也可以通过kill函数的调用请求kernel发送signal。\n\n### kill\n```C\n#include <sys/types.h>\n#include <signal.h>\n\nint kill(pid_t pid, int sig);\n```\n\n- pid>0: 发送信号给pid的process\n- pid=0: 发送信号给调用process所在process gruop的所有processes\n- pid<0: 发送信号给pid group为-pid的所有processes\n\n### receiving\n信号的receive是在上下文切换中完成的，也就是从kenel code切换回user code后起作用。\n\n信号发送但是未被received（未被处理时）称为pending。信号在这个阶段起作用靠pending bit vector & blocked bit vector，不同信号的信号发送会储存在pending bit vector中，相同类型的会被抛弃（该bit已经被置1了）。bloked bit vector则是掩码，设置为1的位置对应信号不会起作用，因为最后信号的处理是由**pending & ~blocked**(unblocked pending signals)决定。当掩码起作用时，该信号被**忽略**。\n\nSignal接收的时机是在进入进程前，kernal会检查是否有待处理的信号\n\n![Receive的时机](signal_received.jpg)\n\n### Catch\n\n每个Signal有默认的执行结果，用户可以通过修改__singal_handler__函数以自定义Signal的作用。Kernel执行signal_hander称为信号的**捕获（catch）**。\n\n![常见的信号及作用](signal_type.jpg)\n\n![signal_handler](signal_handler.jpg)\n\n> signal_handelr也属于进程调用的一部分，可以被其他信号处理函数打断\n\n但是信号9 SIGKILL和信号19 SIGSTOP不可被忽略或重写。\n\n**SIGCHLD**: 当子进程的**状态改变**时，如终止，暂停等，都会发送给父进程，所以可以重设signal_chld_handler,先判断进程的情况再操作。\n\n### Handlers的处理守则\n![handler guidline](guidelines.jpg)\n\n## 竞争\n由于不知道子进程和父进程的执行先后顺序，当它们都会对某一对象操作时，其实际先后顺序可能并不是我们想要的，这称之为**竞争（Race）**。避免竞争一种的办法是显示的阻塞信号，这样可以避免进程和handler之间的竞争。\n\n![race](race.jpg)\n","slug":"Exceptional-Control-Flow","published":1,"updated":"2021-08-24T08:41:00.219Z","_id":"ckskmdeiq0000umkk5741g9rx","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Exceptional-Control-Flow\"><a href=\"#Exceptional-Control-Flow\" class=\"headerlink\" title=\"Exceptional Control Flow\"></a>Exceptional Control Flow</h1><p>机器的一系列指令执行称作control flow,在汇编一章中，我们已经了解了<strong>程序级别</strong>的控制流变化,例如用Jumps and branches, call and return来改变指令操作的顺序。接下来我们要学习的是<strong>系统级别</strong>的改变,例如数据在磁盘和内存的运输，键盘上的Ctrl-C停止的作用等，也就是<strong>exceptional control flow</strong>（异常控制流）的机制。</p>\n<h1 id=\"Exceptions\"><a href=\"#Exceptions\" class=\"headerlink\" title=\"Exceptions\"></a>Exceptions</h1><p>An exception is a transfer of control to the OS kernel in response to some event (i.e., change in processor state)。也即异常是内核对事件的响应，是一种低级别的机制。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/exceptions.jpg\"></p>\n<blockquote>\n<p>Exception Tables<br>Events组成一个表，当Event k发生时，系统查询索引为k的地址，得到处理异常的程序。</p>\n</blockquote>\n<h2 id=\"同步与异步\"><a href=\"#同步与异步\" class=\"headerlink\" title=\"同步与异步\"></a>同步与异步</h2><h3 id=\"异步异常-Asynchronous-Exceptions\"><a href=\"#异步异常-Asynchronous-Exceptions\" class=\"headerlink\" title=\"异步异常(Asynchronous Exceptions)\"></a>异步异常(Asynchronous Exceptions)</h3><p>异步异常是由外部时间触发的对处理器的操作，也被称为Interrupts。例如每隔几毫秒，外部的timer chip触发一次异常来从用户程序中拿回控制权，在终端输入Ctrl-C等。</p>\n<h3 id=\"同步异常-Synchronous-Exceptions\"><a href=\"#同步异常-Synchronous-Exceptions\" class=\"headerlink\" title=\"同步异常(Synchronous Exceptions)\"></a>同步异常(Synchronous Exceptions)</h3><p>在执行指令中遇到了相应的Events。</p>\n<ul>\n<li>Traps 人为地设置，如断点调试，系统调用等，结果是返回控制权给下一条指令。</li>\n<li>Faults 非故意但可恢复或者不可恢复的，如浮点数的异常等，结果是重新执行此条指令或者终止</li>\n<li>Aborts 非故意且不可恢复的，结果是停止此程序。</li>\n</ul>\n<h2 id=\"Processes\"><a href=\"#Processes\" class=\"headerlink\" title=\"Processes\"></a>Processes</h2><p><strong>A process is an instatnce of a running program</strong></p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/process.jpg\"></p>\n<p>一个进程的假象：</p>\n<ul>\n<li>logical control flow 每个进程似乎独占CPU，由上下文切换实现</li>\n<li>private address space 似乎都有自己的一块内存空间，由虚拟内存实现</li>\n</ul>\n<h3 id=\"Concurrent-Processes\"><a href=\"#Concurrent-Processes\" class=\"headerlink\" title=\"Concurrent Processes\"></a>Concurrent Processes</h3><p>并发进程几乎无时无刻不在发生，接下来我们将从上下文切换的角度理解。</p>\n<p>我们将process视为一个逻辑控制流，当两个逻辑控制流在时间上重叠时，它们是<strong>并发</strong>的。</p>\n<p>当处理器是单核时，CPU每次执行一个process，为了实现并发，它在两个process间切换执行。当处理器是多核时，每一个cpu都可以单独执行一个process，同样的此时每一个核又可以并发。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/context_switch.jpg\"></p>\n<blockquote>\n<p>上下文切换的细节：将上一个进程的寄存器的值保存在内存中（相当于caller saved?）。上下文切换从代码角度，是一个user code到另一个user code的过渡，而这个过渡由kernel code实现。</p>\n</blockquote>\n<h2 id=\"Process-Control-API\"><a href=\"#Process-Control-API\" class=\"headerlink\" title=\"Process Control(API)\"></a>Process Control(API)</h2><p>从程序员的角度考虑，进程有三种状态，Running, Stopped, Terminated。以下的函数是系统调用，可以改变进程的状态。</p>\n<h3 id=\"exit\"><a href=\"#exit\" class=\"headerlink\" title=\"exit\"></a>exit</h3><p>Terminate the program with status, 正常退出为0，非正常退出返回非0值，exit只会被执行一次，不会return</p>\n<h3 id=\"Fork\"><a href=\"#Fork\" class=\"headerlink\" title=\"Fork\"></a>Fork</h3><p>fork函数构建一个子进程。</p>\n<p>fork被调用一次，但是会有两个返回值，在父进程中返回子进程号，在子进程中返回0，可以通过返回值来分辨此时在哪个进程中。可以把返回值以链表的形式理解，父进程指向下一个子进程，而子进程如果在链表末端时，则指向NULL也即是0。</p>\n<p>fork的返回值很重要，每次必须判断是否成功创建子进程，为了使判断的代码更加compact，所以用Fork重写代码。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">unix_error</span><span class=\"params\">(<span class=\"keyword\">char</span>* msg)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">fprintf</span>(<span class=\"built_in\">stderr</span>, <span class=\"string\">&quot;%s: %s\\n&quot;</span>, msg, strerror(errno));</span><br><span class=\"line\">\t<span class=\"built_in\">exit</span>(<span class=\"number\">-1</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">pid_t</span> <span class=\"title\">Fork</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">pid_t</span> pid;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span>((pid = fork())&lt;<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\tunix_error(<span class=\"string\">&quot;Fork error&quot;</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> pid;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>下面来看一个例子：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&quot;csapp.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> x = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Fork() == <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;p1: %d\\n&quot;</span>, ++x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;p2: %d\\n&quot;</span>, --x);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//输出</span><br><span class=\"line\">p2: 0 //父进程</span><br><span class=\"line\">p1: 2 //子进程</span><br><span class=\"line\">p2: 1 //子进程</span><br></pre></td></tr></table></figure>\n<p>首先执行到Fork时，就像薛定谔的猫，进程处于量子叠加态，既是父进程也是子进程。接下来用一个if判断将两者分离。在这个例子中，我们很容易发现，每个进程都有自己变量的拷贝，x互不影响，那么它们的虚拟地址如何呢？</p>\n<p>更改代码为输出地址，结果如下。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">p2: 0x7fffd2bc7fe4</span><br><span class=\"line\">p1: 0x7fffd2bc7fe4</span><br><span class=\"line\">p2: 0x7fffd2bc7fe4</span><br></pre></td></tr></table></figure>\n\n<p>可以看到变量的虚拟地址都是相同的，但它们的实际地址不同。</p>\n<h3 id=\"僵尸进程与wait回收\"><a href=\"#僵尸进程与wait回收\" class=\"headerlink\" title=\"僵尸进程与wait回收\"></a>僵尸进程与wait回收</h3><p>当子进程结束运行，而父进程没有对其回收的话，子进程沦为僵尸进程，还占用资源。<br>如果父进程被kill，子进程结束运行，子进程会由<strong>init</strong>回收。<br>如果父进程结束运行，子进程不终止，那么子进程无法结束、回收。<br>使用wait可以使父进程显示地对子进程回收，父进程会一直suspend直到子进程终止。</p>\n<h3 id=\"execve\"><a href=\"#execve\" class=\"headerlink\" title=\"execve\"></a>execve</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">execve</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *filenaem, <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *argv[],</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">\t\t   <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *envp[])</span></span>;</span><br><span class=\"line\">\t\t   <span class=\"comment\">//Does not return if OK;returns -1 on error</span></span><br></pre></td></tr></table></figure>\n<p>执行程序(第三个参数直接写入全局变量 environ，由libc决定)</p>\n<h2 id=\"Signals\"><a href=\"#Signals\" class=\"headerlink\" title=\"Signals\"></a>Signals</h2><p>信号是传送给process的一种信息，可以用于进程间的通信。例如当键盘键入Ctrl+C时，kenerl终止所有foreground的process通过信号SIGINT。总的来说，信号的起作用可以分为两步：sending/delivering　&amp;　receiving</p>\n<h3 id=\"sending\"><a href=\"#sending\" class=\"headerlink\" title=\"sending\"></a>sending</h3><p>信号总是由kernel发送的。kernel可以通过某些event的触发自主发送signal，process也可以通过kill函数的调用请求kernel发送signal。</p>\n<h3 id=\"kill\"><a href=\"#kill\" class=\"headerlink\" title=\"kill\"></a>kill</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;sys/types.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;signal.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">kill</span><span class=\"params\">(<span class=\"keyword\">pid_t</span> pid, <span class=\"keyword\">int</span> sig)</span></span>;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>pid&gt;0: 发送信号给pid的process</li>\n<li>pid=0: 发送信号给调用process所在process gruop的所有processes</li>\n<li>pid&lt;0: 发送信号给pid group为-pid的所有processes</li>\n</ul>\n<h3 id=\"receiving\"><a href=\"#receiving\" class=\"headerlink\" title=\"receiving\"></a>receiving</h3><p>信号的receive是在上下文切换中完成的，也就是从kenel code切换回user code后起作用。</p>\n<p>信号发送但是未被received（未被处理时）称为pending。信号在这个阶段起作用靠pending bit vector &amp; blocked bit vector，不同信号的信号发送会储存在pending bit vector中，相同类型的会被抛弃（该bit已经被置1了）。bloked bit vector则是掩码，设置为1的位置对应信号不会起作用，因为最后信号的处理是由<strong>pending &amp; ~blocked</strong>(unblocked pending signals)决定。当掩码起作用时，该信号被<strong>忽略</strong>。</p>\n<p>Signal接收的时机是在进入进程前，kernal会检查是否有待处理的信号</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/signal_received.jpg\" alt=\"Receive的时机\"></p>\n<h3 id=\"Catch\"><a href=\"#Catch\" class=\"headerlink\" title=\"Catch\"></a>Catch</h3><p>每个Signal有默认的执行结果，用户可以通过修改__singal_handler__函数以自定义Signal的作用。Kernel执行signal_hander称为信号的<strong>捕获（catch）</strong>。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/signal_type.jpg\" alt=\"常见的信号及作用\"></p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/signal_handler.jpg\" alt=\"signal_handler\"></p>\n<blockquote>\n<p>signal_handelr也属于进程调用的一部分，可以被其他信号处理函数打断</p>\n</blockquote>\n<p>但是信号9 SIGKILL和信号19 SIGSTOP不可被忽略或重写。</p>\n<p><strong>SIGCHLD</strong>: 当子进程的<strong>状态改变</strong>时，如终止，暂停等，都会发送给父进程，所以可以重设signal_chld_handler,先判断进程的情况再操作。</p>\n<h3 id=\"Handlers的处理守则\"><a href=\"#Handlers的处理守则\" class=\"headerlink\" title=\"Handlers的处理守则\"></a>Handlers的处理守则</h3><p><img src=\"/2021/08/11/Exceptional-Control-Flow/guidelines.jpg\" alt=\"handler guidline\"></p>\n<h2 id=\"竞争\"><a href=\"#竞争\" class=\"headerlink\" title=\"竞争\"></a>竞争</h2><p>由于不知道子进程和父进程的执行先后顺序，当它们都会对某一对象操作时，其实际先后顺序可能并不是我们想要的，这称之为<strong>竞争（Race）</strong>。避免竞争一种的办法是显示的阻塞信号，这样可以避免进程和handler之间的竞争。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/race.jpg\" alt=\"race\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Exceptional-Control-Flow\"><a href=\"#Exceptional-Control-Flow\" class=\"headerlink\" title=\"Exceptional Control Flow\"></a>Exceptional Control Flow</h1><p>机器的一系列指令执行称作control flow,在汇编一章中，我们已经了解了<strong>程序级别</strong>的控制流变化,例如用Jumps and branches, call and return来改变指令操作的顺序。接下来我们要学习的是<strong>系统级别</strong>的改变,例如数据在磁盘和内存的运输，键盘上的Ctrl-C停止的作用等，也就是<strong>exceptional control flow</strong>（异常控制流）的机制。</p>\n<h1 id=\"Exceptions\"><a href=\"#Exceptions\" class=\"headerlink\" title=\"Exceptions\"></a>Exceptions</h1><p>An exception is a transfer of control to the OS kernel in response to some event (i.e., change in processor state)。也即异常是内核对事件的响应，是一种低级别的机制。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/exceptions.jpg\"></p>\n<blockquote>\n<p>Exception Tables<br>Events组成一个表，当Event k发生时，系统查询索引为k的地址，得到处理异常的程序。</p>\n</blockquote>\n<h2 id=\"同步与异步\"><a href=\"#同步与异步\" class=\"headerlink\" title=\"同步与异步\"></a>同步与异步</h2><h3 id=\"异步异常-Asynchronous-Exceptions\"><a href=\"#异步异常-Asynchronous-Exceptions\" class=\"headerlink\" title=\"异步异常(Asynchronous Exceptions)\"></a>异步异常(Asynchronous Exceptions)</h3><p>异步异常是由外部时间触发的对处理器的操作，也被称为Interrupts。例如每隔几毫秒，外部的timer chip触发一次异常来从用户程序中拿回控制权，在终端输入Ctrl-C等。</p>\n<h3 id=\"同步异常-Synchronous-Exceptions\"><a href=\"#同步异常-Synchronous-Exceptions\" class=\"headerlink\" title=\"同步异常(Synchronous Exceptions)\"></a>同步异常(Synchronous Exceptions)</h3><p>在执行指令中遇到了相应的Events。</p>\n<ul>\n<li>Traps 人为地设置，如断点调试，系统调用等，结果是返回控制权给下一条指令。</li>\n<li>Faults 非故意但可恢复或者不可恢复的，如浮点数的异常等，结果是重新执行此条指令或者终止</li>\n<li>Aborts 非故意且不可恢复的，结果是停止此程序。</li>\n</ul>\n<h2 id=\"Processes\"><a href=\"#Processes\" class=\"headerlink\" title=\"Processes\"></a>Processes</h2><p><strong>A process is an instatnce of a running program</strong></p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/process.jpg\"></p>\n<p>一个进程的假象：</p>\n<ul>\n<li>logical control flow 每个进程似乎独占CPU，由上下文切换实现</li>\n<li>private address space 似乎都有自己的一块内存空间，由虚拟内存实现</li>\n</ul>\n<h3 id=\"Concurrent-Processes\"><a href=\"#Concurrent-Processes\" class=\"headerlink\" title=\"Concurrent Processes\"></a>Concurrent Processes</h3><p>并发进程几乎无时无刻不在发生，接下来我们将从上下文切换的角度理解。</p>\n<p>我们将process视为一个逻辑控制流，当两个逻辑控制流在时间上重叠时，它们是<strong>并发</strong>的。</p>\n<p>当处理器是单核时，CPU每次执行一个process，为了实现并发，它在两个process间切换执行。当处理器是多核时，每一个cpu都可以单独执行一个process，同样的此时每一个核又可以并发。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/context_switch.jpg\"></p>\n<blockquote>\n<p>上下文切换的细节：将上一个进程的寄存器的值保存在内存中（相当于caller saved?）。上下文切换从代码角度，是一个user code到另一个user code的过渡，而这个过渡由kernel code实现。</p>\n</blockquote>\n<h2 id=\"Process-Control-API\"><a href=\"#Process-Control-API\" class=\"headerlink\" title=\"Process Control(API)\"></a>Process Control(API)</h2><p>从程序员的角度考虑，进程有三种状态，Running, Stopped, Terminated。以下的函数是系统调用，可以改变进程的状态。</p>\n<h3 id=\"exit\"><a href=\"#exit\" class=\"headerlink\" title=\"exit\"></a>exit</h3><p>Terminate the program with status, 正常退出为0，非正常退出返回非0值，exit只会被执行一次，不会return</p>\n<h3 id=\"Fork\"><a href=\"#Fork\" class=\"headerlink\" title=\"Fork\"></a>Fork</h3><p>fork函数构建一个子进程。</p>\n<p>fork被调用一次，但是会有两个返回值，在父进程中返回子进程号，在子进程中返回0，可以通过返回值来分辨此时在哪个进程中。可以把返回值以链表的形式理解，父进程指向下一个子进程，而子进程如果在链表末端时，则指向NULL也即是0。</p>\n<p>fork的返回值很重要，每次必须判断是否成功创建子进程，为了使判断的代码更加compact，所以用Fork重写代码。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">unix_error</span><span class=\"params\">(<span class=\"keyword\">char</span>* msg)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">fprintf</span>(<span class=\"built_in\">stderr</span>, <span class=\"string\">&quot;%s: %s\\n&quot;</span>, msg, strerror(errno));</span><br><span class=\"line\">\t<span class=\"built_in\">exit</span>(<span class=\"number\">-1</span>);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">pid_t</span> <span class=\"title\">Fork</span><span class=\"params\">(<span class=\"keyword\">void</span>)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">pid_t</span> pid;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">if</span>((pid = fork())&lt;<span class=\"number\">0</span>)</span><br><span class=\"line\">\t\tunix_error(<span class=\"string\">&quot;Fork error&quot;</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> pid;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>下面来看一个例子：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&quot;csapp.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> x = <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span>(Fork() == <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;p1: %d\\n&quot;</span>, ++x);</span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">&quot;p2: %d\\n&quot;</span>, --x);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//输出</span><br><span class=\"line\">p2: 0 //父进程</span><br><span class=\"line\">p1: 2 //子进程</span><br><span class=\"line\">p2: 1 //子进程</span><br></pre></td></tr></table></figure>\n<p>首先执行到Fork时，就像薛定谔的猫，进程处于量子叠加态，既是父进程也是子进程。接下来用一个if判断将两者分离。在这个例子中，我们很容易发现，每个进程都有自己变量的拷贝，x互不影响，那么它们的虚拟地址如何呢？</p>\n<p>更改代码为输出地址，结果如下。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">p2: 0x7fffd2bc7fe4</span><br><span class=\"line\">p1: 0x7fffd2bc7fe4</span><br><span class=\"line\">p2: 0x7fffd2bc7fe4</span><br></pre></td></tr></table></figure>\n\n<p>可以看到变量的虚拟地址都是相同的，但它们的实际地址不同。</p>\n<h3 id=\"僵尸进程与wait回收\"><a href=\"#僵尸进程与wait回收\" class=\"headerlink\" title=\"僵尸进程与wait回收\"></a>僵尸进程与wait回收</h3><p>当子进程结束运行，而父进程没有对其回收的话，子进程沦为僵尸进程，还占用资源。<br>如果父进程被kill，子进程结束运行，子进程会由<strong>init</strong>回收。<br>如果父进程结束运行，子进程不终止，那么子进程无法结束、回收。<br>使用wait可以使父进程显示地对子进程回收，父进程会一直suspend直到子进程终止。</p>\n<h3 id=\"execve\"><a href=\"#execve\" class=\"headerlink\" title=\"execve\"></a>execve</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;unistd.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">execve</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *filenaem, <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *argv[],</span></span></span><br><span class=\"line\"><span class=\"params\"><span class=\"function\">\t\t   <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *envp[])</span></span>;</span><br><span class=\"line\">\t\t   <span class=\"comment\">//Does not return if OK;returns -1 on error</span></span><br></pre></td></tr></table></figure>\n<p>执行程序(第三个参数直接写入全局变量 environ，由libc决定)</p>\n<h2 id=\"Signals\"><a href=\"#Signals\" class=\"headerlink\" title=\"Signals\"></a>Signals</h2><p>信号是传送给process的一种信息，可以用于进程间的通信。例如当键盘键入Ctrl+C时，kenerl终止所有foreground的process通过信号SIGINT。总的来说，信号的起作用可以分为两步：sending/delivering　&amp;　receiving</p>\n<h3 id=\"sending\"><a href=\"#sending\" class=\"headerlink\" title=\"sending\"></a>sending</h3><p>信号总是由kernel发送的。kernel可以通过某些event的触发自主发送signal，process也可以通过kill函数的调用请求kernel发送signal。</p>\n<h3 id=\"kill\"><a href=\"#kill\" class=\"headerlink\" title=\"kill\"></a>kill</h3><figure class=\"highlight c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;sys/types.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span> <span class=\"meta-string\">&lt;signal.h&gt;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">kill</span><span class=\"params\">(<span class=\"keyword\">pid_t</span> pid, <span class=\"keyword\">int</span> sig)</span></span>;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>pid&gt;0: 发送信号给pid的process</li>\n<li>pid=0: 发送信号给调用process所在process gruop的所有processes</li>\n<li>pid&lt;0: 发送信号给pid group为-pid的所有processes</li>\n</ul>\n<h3 id=\"receiving\"><a href=\"#receiving\" class=\"headerlink\" title=\"receiving\"></a>receiving</h3><p>信号的receive是在上下文切换中完成的，也就是从kenel code切换回user code后起作用。</p>\n<p>信号发送但是未被received（未被处理时）称为pending。信号在这个阶段起作用靠pending bit vector &amp; blocked bit vector，不同信号的信号发送会储存在pending bit vector中，相同类型的会被抛弃（该bit已经被置1了）。bloked bit vector则是掩码，设置为1的位置对应信号不会起作用，因为最后信号的处理是由<strong>pending &amp; ~blocked</strong>(unblocked pending signals)决定。当掩码起作用时，该信号被<strong>忽略</strong>。</p>\n<p>Signal接收的时机是在进入进程前，kernal会检查是否有待处理的信号</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/signal_received.jpg\" alt=\"Receive的时机\"></p>\n<h3 id=\"Catch\"><a href=\"#Catch\" class=\"headerlink\" title=\"Catch\"></a>Catch</h3><p>每个Signal有默认的执行结果，用户可以通过修改__singal_handler__函数以自定义Signal的作用。Kernel执行signal_hander称为信号的<strong>捕获（catch）</strong>。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/signal_type.jpg\" alt=\"常见的信号及作用\"></p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/signal_handler.jpg\" alt=\"signal_handler\"></p>\n<blockquote>\n<p>signal_handelr也属于进程调用的一部分，可以被其他信号处理函数打断</p>\n</blockquote>\n<p>但是信号9 SIGKILL和信号19 SIGSTOP不可被忽略或重写。</p>\n<p><strong>SIGCHLD</strong>: 当子进程的<strong>状态改变</strong>时，如终止，暂停等，都会发送给父进程，所以可以重设signal_chld_handler,先判断进程的情况再操作。</p>\n<h3 id=\"Handlers的处理守则\"><a href=\"#Handlers的处理守则\" class=\"headerlink\" title=\"Handlers的处理守则\"></a>Handlers的处理守则</h3><p><img src=\"/2021/08/11/Exceptional-Control-Flow/guidelines.jpg\" alt=\"handler guidline\"></p>\n<h2 id=\"竞争\"><a href=\"#竞争\" class=\"headerlink\" title=\"竞争\"></a>竞争</h2><p>由于不知道子进程和父进程的执行先后顺序，当它们都会对某一对象操作时，其实际先后顺序可能并不是我们想要的，这称之为<strong>竞争（Race）</strong>。避免竞争一种的办法是显示的阻塞信号，这样可以避免进程和handler之间的竞争。</p>\n<p><img src=\"/2021/08/11/Exceptional-Control-Flow/race.jpg\" alt=\"race\"></p>\n"},{"title":"deeplearning pipeline for pytorch","date":"2021-08-26T14:05:13.000Z","keywords":"torch pipeline","cover":"/img/pic2.png","_content":"\n#Pytorch深度学习的一般流程\n\n这篇文章适用于有机器学习和深度学习基础但是不能熟练使用pytorch完成完整任务的读者。本文将基于一个心型分类器归纳代码的组成。\n\n本文给出100k个点，根据心形函数对点打上里（1）和外（0）的标签，我们的目标是将点的坐标作为输入，给出标签也即是一个二分类的问题。\n\n\n\n首先对于一个深度学习网络我们一般要完成三个部分：\n- train 训练\n- eval 阶段性评估，在eval中选取更优的网络\n- interface/test 提供使用网路的接口（可选）\n\n\n## 数据准备说明\n\n```python\nfrom math import pow\nimport random\npoints = []\ndef func(x, y):\n    return pow((x**2 + y**2 - 1), 3) - pow(x, 2)*pow(y, 3)<0\nrandom.seed(123)\nfor i in range(100000):\n    x = (random.random() - 0.5)*4\n    y = (random.random() - 0.5)*4\n    label = int(func(x, y))\n    points.append([x, y, label])\n```\nfunc定义一个心形函数，random.random()生成[0, 1)范围内的实数，将它们放缩到[-2, 2]区间。\n\n![](init.png)\n\n```python\nimport pandas as pd\ndf = pd.DataFrame(points,columns=['x','y', 'label'])\nimport seaborn as sns\nsns_plot = sns.scatterplot(data = df, x = 'x', y = 'y', hue = 'label')\nsns_plot.figure.savefig('./init.png')\n```\n将点和label信息放入数据框中作为我们的数据集，利用sns可视化数据。\n\n## 训练\n\n**Training = Dataset and Dataloader + Model + Loss and Optimizer**\n\n## Dataset & Dataloader\nDataset 和 DataLoader的构建。\n```python\nfrom torch.utils.data import Dataset, DataLoader, random_split\n```\n### Dataset 自定义\n实际工程中拿到的数据格式、类型不一，但是基本的Dataset构建方式是有一定范式的。\n\n自定义数据集的要点有三：\n- 继承torch.utils.data.Dataset\n- 重写 __getitem__函数\n- 重写 __len__函数\n\n```python\nclass PointDataset(Dataset):\n    def __init__(self, dataframe:pd.DataFrame):\n        super(PointDataset, self).__init__()\n        self.data = dataframe\n    def __getitem__(self, index):\n        point = torch.tensor([[self.data['x'][index], self.data['y'][index]]], dtype = torch.float32)\n        label = torch.tensor([self.data['label'][index]], dtype = torch.float32)\n        return point, label\n    def __len__(self):\n        return len(self.data)\n```\n\n__getitem__函数是提供给DataLoader获取一个数据集元素的方法，给定一个index返回tensor化的数据集元素。\n__len__函数是返回数据集的总长度\n\n> Tips : 当传入的数据比较大比如读入图片时，一般方法是通过__init__函数构建一个存放着图像路径的数据框， 然后在__getitem__中读入图片到内存中。\n\nDataset实际上是一个迭代器，可以试试以下代码输出dataset中的元素。\n\n### DataLoader\n\nDataLoader的使用方法比较一般，只要传入上一步自己定义的Dataset和一些常规参数即可。\n\n\n### Model\n\nModel继承torch.nn.Module类，在model的构建上有较多的tensor和模型模块的技巧，这里只是简单地构建一个MLP。在__init__中定义模块，在__forward__中阐述完整模型的构建。\n\n一般来说在__init__中定义需要训练的部分，比如nn.Linear等，在__forward__处定义无需训练的模块如normalize。\n\n```python\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        \n        self.layers = nn.Sequential(\n          nn.Flatten(),\n          nn.Linear(2, 72),\n          nn.ReLU(),\n          nn.Linear(72, 36),\n          nn.ReLU(),\n          nn.Linear(36, 1),\n          nn.Sigmoid(),\n    )\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n```\n\n在此处虽然我们的点是二维的，但是模型只是线性神经元的操作，不涉及卷积，所以将tensro用nn.Flatten()展成一维的。\n> sigmoid函数将得分放缩到[0, 1)之间，得到每一个标签的概率，这里与loss的选择相对应。\n\n\n### Loss & Optimizer\n```python\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=0.9)\n```\n分类任务一般使用交叉熵损失函数，二分类使用BCELoss，多分类使用nn.CrossEntropyLoss。\n\noptimizer一般使用Adam和SGD。此处采用的是带动量的随机梯度下降法。优化器的选择也是一个重（玄）要（学）的方面，不同的任务在不同的优化器上差异较大。\n\n### 训练步骤\n\n```python\ndef train(config, train_loader, val_loader):\n    net = MLP()\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=0.9)\n    \n    net.to(device)\n    net.train()\n    for e in range(config.epochs):\n        train_loss = 0\n        n_total = 0\n        eval_loss = 0\n        for batch, (x, y) in enumerate((train_loader)):\n            optimizer.zero_grad()\n            x = x.to(device)\n            y = y.to(device)\n            outputs = net(x)\n            #todo cuda\n            loss = criterion(outputs, y)\n            loss.backward()\n            train_loss += loss.item()*len(outputs)\n            n_total += len(outputs)\n            optimizer.step()\n        eval_loss = eval(net, config, val_loader)\n        train_loss = train_loss / n_total\n        if ((e % 5 == 0) and (e >= 40) ):\n            torch.save(net, config.save_path + \"{:4f}.pth\".format(eval_loss))\n        print(\"Epoch: {}/{}, Train Loss: {}, Val Loss: {}\".format(e, config.epochs, train_loss, eval_loss))\n```\n- 定义网络结构，loss，optimizer\n- epoch循环，定义每一个epoch中的loss,acc,miou等\n　- batch循环\n　- 优化器梯度清0，正向传播计算loss,自动反向传播，step更新所有参数\n　- eval模型，在验证集上跑模型\n　- 输出epoch结果，保存最优模型\n\n额外需要说明的两点：\n1. 如果使用gpu的话，net和tensor都要放在gpu上跑，因为cpu和gpu的数据是不互通的\n2. 保存模型有两种方法，这里采用的是最简单粗暴的方法，直接序列化。\n\neval函数大同小异\n```python\ndef eval(model, congfig, data_loader):\n    model.eval()\n    eval_loss = 0\n    n_total = 0\n    criterion = nn.BCELoss()\n    with torch.no_grad():\n        for batch, (x, y) in enumerate((data_loader)):\n            x = x.to(device)\n            y = y.to(device)\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            eval_loss += loss.item()*len(outputs)\n            n_total += len(outputs)\n    model.train()\n    return eval_loss / n_total\n```\n注意在两者间切换时利用net.eval()和net.train()做相应的改变。\n\n至此，基于pytorch的深度学习pipeline完成。\n\n## 训练结果\n\n给出测试接口如下，对结果做可视化\n```python\ndef cut_threshold(x):\n    return int(1) if x > 0.5 else int(0)\ndef test_model(model_path, data_loader):\n    net = torch.load(model_path)\n    net.to(device)\n    target = []\n    \n    for batch, (x, y) in enumerate((data_loader)):\n        x = x.to(device)\n        y = y.to(device)\n        outputs = net(x)\n        for i in range(x.shape[0]):\n            target.append([x[i][0][0].item(), x[i][0][1].item(), cut_threshold(outputs[i][0].item())])\n    df = pd.DataFrame(target,columns=['x','y', 'label'])\n    sns_plot = sns.scatterplot(data = df, x = 'x', y = 'y', hue = 'label')\n    sns_plot.figure.savefig(model_path[:12]+ '.png')\n\n```\n设置阈值为0.5即当概率大于threshold时认为是正样本。每5个epoch保存的模型测试可视化结果并对比原始标签如下：\n\n\n\n![](1.png)![](2.png)![](3.png)![](4.png)\n\n![](5.png)![](6.png)![](7.png)![](8.png)\n\n[](init.png)\n\n## 工程常用方法\n1. 创建directory分类，常把函数分入Model, Dataset, utils模块。\n2. 使用argparse包以做到命令行传参，并设立单独的class初始化默认参数。\n","source":"_posts/deeplearning-pipeline-for-pytorch.md","raw":"---\ntitle: deeplearning pipeline for pytorch\ndate: 2021-08-26 22:05:13\ntags: pytorch深度学习的一般流程\nkeywords: torch pipeline\ncategories: 深度学习\ncover: \n---\n\n#Pytorch深度学习的一般流程\n\n这篇文章适用于有机器学习和深度学习基础但是不能熟练使用pytorch完成完整任务的读者。本文将基于一个心型分类器归纳代码的组成。\n\n本文给出100k个点，根据心形函数对点打上里（1）和外（0）的标签，我们的目标是将点的坐标作为输入，给出标签也即是一个二分类的问题。\n\n\n\n首先对于一个深度学习网络我们一般要完成三个部分：\n- train 训练\n- eval 阶段性评估，在eval中选取更优的网络\n- interface/test 提供使用网路的接口（可选）\n\n\n## 数据准备说明\n\n```python\nfrom math import pow\nimport random\npoints = []\ndef func(x, y):\n    return pow((x**2 + y**2 - 1), 3) - pow(x, 2)*pow(y, 3)<0\nrandom.seed(123)\nfor i in range(100000):\n    x = (random.random() - 0.5)*4\n    y = (random.random() - 0.5)*4\n    label = int(func(x, y))\n    points.append([x, y, label])\n```\nfunc定义一个心形函数，random.random()生成[0, 1)范围内的实数，将它们放缩到[-2, 2]区间。\n\n![](init.png)\n\n```python\nimport pandas as pd\ndf = pd.DataFrame(points,columns=['x','y', 'label'])\nimport seaborn as sns\nsns_plot = sns.scatterplot(data = df, x = 'x', y = 'y', hue = 'label')\nsns_plot.figure.savefig('./init.png')\n```\n将点和label信息放入数据框中作为我们的数据集，利用sns可视化数据。\n\n## 训练\n\n**Training = Dataset and Dataloader + Model + Loss and Optimizer**\n\n## Dataset & Dataloader\nDataset 和 DataLoader的构建。\n```python\nfrom torch.utils.data import Dataset, DataLoader, random_split\n```\n### Dataset 自定义\n实际工程中拿到的数据格式、类型不一，但是基本的Dataset构建方式是有一定范式的。\n\n自定义数据集的要点有三：\n- 继承torch.utils.data.Dataset\n- 重写 __getitem__函数\n- 重写 __len__函数\n\n```python\nclass PointDataset(Dataset):\n    def __init__(self, dataframe:pd.DataFrame):\n        super(PointDataset, self).__init__()\n        self.data = dataframe\n    def __getitem__(self, index):\n        point = torch.tensor([[self.data['x'][index], self.data['y'][index]]], dtype = torch.float32)\n        label = torch.tensor([self.data['label'][index]], dtype = torch.float32)\n        return point, label\n    def __len__(self):\n        return len(self.data)\n```\n\n__getitem__函数是提供给DataLoader获取一个数据集元素的方法，给定一个index返回tensor化的数据集元素。\n__len__函数是返回数据集的总长度\n\n> Tips : 当传入的数据比较大比如读入图片时，一般方法是通过__init__函数构建一个存放着图像路径的数据框， 然后在__getitem__中读入图片到内存中。\n\nDataset实际上是一个迭代器，可以试试以下代码输出dataset中的元素。\n\n### DataLoader\n\nDataLoader的使用方法比较一般，只要传入上一步自己定义的Dataset和一些常规参数即可。\n\n\n### Model\n\nModel继承torch.nn.Module类，在model的构建上有较多的tensor和模型模块的技巧，这里只是简单地构建一个MLP。在__init__中定义模块，在__forward__中阐述完整模型的构建。\n\n一般来说在__init__中定义需要训练的部分，比如nn.Linear等，在__forward__处定义无需训练的模块如normalize。\n\n```python\nclass MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        \n        self.layers = nn.Sequential(\n          nn.Flatten(),\n          nn.Linear(2, 72),\n          nn.ReLU(),\n          nn.Linear(72, 36),\n          nn.ReLU(),\n          nn.Linear(36, 1),\n          nn.Sigmoid(),\n    )\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n```\n\n在此处虽然我们的点是二维的，但是模型只是线性神经元的操作，不涉及卷积，所以将tensro用nn.Flatten()展成一维的。\n> sigmoid函数将得分放缩到[0, 1)之间，得到每一个标签的概率，这里与loss的选择相对应。\n\n\n### Loss & Optimizer\n```python\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=0.9)\n```\n分类任务一般使用交叉熵损失函数，二分类使用BCELoss，多分类使用nn.CrossEntropyLoss。\n\noptimizer一般使用Adam和SGD。此处采用的是带动量的随机梯度下降法。优化器的选择也是一个重（玄）要（学）的方面，不同的任务在不同的优化器上差异较大。\n\n### 训练步骤\n\n```python\ndef train(config, train_loader, val_loader):\n    net = MLP()\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=0.9)\n    \n    net.to(device)\n    net.train()\n    for e in range(config.epochs):\n        train_loss = 0\n        n_total = 0\n        eval_loss = 0\n        for batch, (x, y) in enumerate((train_loader)):\n            optimizer.zero_grad()\n            x = x.to(device)\n            y = y.to(device)\n            outputs = net(x)\n            #todo cuda\n            loss = criterion(outputs, y)\n            loss.backward()\n            train_loss += loss.item()*len(outputs)\n            n_total += len(outputs)\n            optimizer.step()\n        eval_loss = eval(net, config, val_loader)\n        train_loss = train_loss / n_total\n        if ((e % 5 == 0) and (e >= 40) ):\n            torch.save(net, config.save_path + \"{:4f}.pth\".format(eval_loss))\n        print(\"Epoch: {}/{}, Train Loss: {}, Val Loss: {}\".format(e, config.epochs, train_loss, eval_loss))\n```\n- 定义网络结构，loss，optimizer\n- epoch循环，定义每一个epoch中的loss,acc,miou等\n　- batch循环\n　- 优化器梯度清0，正向传播计算loss,自动反向传播，step更新所有参数\n　- eval模型，在验证集上跑模型\n　- 输出epoch结果，保存最优模型\n\n额外需要说明的两点：\n1. 如果使用gpu的话，net和tensor都要放在gpu上跑，因为cpu和gpu的数据是不互通的\n2. 保存模型有两种方法，这里采用的是最简单粗暴的方法，直接序列化。\n\neval函数大同小异\n```python\ndef eval(model, congfig, data_loader):\n    model.eval()\n    eval_loss = 0\n    n_total = 0\n    criterion = nn.BCELoss()\n    with torch.no_grad():\n        for batch, (x, y) in enumerate((data_loader)):\n            x = x.to(device)\n            y = y.to(device)\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            eval_loss += loss.item()*len(outputs)\n            n_total += len(outputs)\n    model.train()\n    return eval_loss / n_total\n```\n注意在两者间切换时利用net.eval()和net.train()做相应的改变。\n\n至此，基于pytorch的深度学习pipeline完成。\n\n## 训练结果\n\n给出测试接口如下，对结果做可视化\n```python\ndef cut_threshold(x):\n    return int(1) if x > 0.5 else int(0)\ndef test_model(model_path, data_loader):\n    net = torch.load(model_path)\n    net.to(device)\n    target = []\n    \n    for batch, (x, y) in enumerate((data_loader)):\n        x = x.to(device)\n        y = y.to(device)\n        outputs = net(x)\n        for i in range(x.shape[0]):\n            target.append([x[i][0][0].item(), x[i][0][1].item(), cut_threshold(outputs[i][0].item())])\n    df = pd.DataFrame(target,columns=['x','y', 'label'])\n    sns_plot = sns.scatterplot(data = df, x = 'x', y = 'y', hue = 'label')\n    sns_plot.figure.savefig(model_path[:12]+ '.png')\n\n```\n设置阈值为0.5即当概率大于threshold时认为是正样本。每5个epoch保存的模型测试可视化结果并对比原始标签如下：\n\n\n\n![](1.png)![](2.png)![](3.png)![](4.png)\n\n![](5.png)![](6.png)![](7.png)![](8.png)\n\n[](init.png)\n\n## 工程常用方法\n1. 创建directory分类，常把函数分入Model, Dataset, utils模块。\n2. 使用argparse包以做到命令行传参，并设立单独的class初始化默认参数。\n","slug":"deeplearning-pipeline-for-pytorch","published":1,"updated":"2021-08-26T15:50:10.096Z","_id":"ckst2t1370000ffkkhxjl5goy","comments":1,"layout":"post","photos":[],"link":"","content":"<p>#Pytorch深度学习的一般流程</p>\n<p>这篇文章适用于有机器学习和深度学习基础但是不能熟练使用pytorch完成完整任务的读者。本文将基于一个心型分类器归纳代码的组成。</p>\n<p>本文给出100k个点，根据心形函数对点打上里（1）和外（0）的标签，我们的目标是将点的坐标作为输入，给出标签也即是一个二分类的问题。</p>\n<p>首先对于一个深度学习网络我们一般要完成三个部分：</p>\n<ul>\n<li>train 训练</li>\n<li>eval 阶段性评估，在eval中选取更优的网络</li>\n<li>interface/test 提供使用网路的接口（可选）</li>\n</ul>\n<h2 id=\"数据准备说明\"><a href=\"#数据准备说明\" class=\"headerlink\" title=\"数据准备说明\"></a>数据准备说明</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> math <span class=\"keyword\">import</span> <span class=\"built_in\">pow</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\">points = []</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span>(<span class=\"params\">x, y</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">pow</span>((x**<span class=\"number\">2</span> + y**<span class=\"number\">2</span> - <span class=\"number\">1</span>), <span class=\"number\">3</span>) - <span class=\"built_in\">pow</span>(x, <span class=\"number\">2</span>)*<span class=\"built_in\">pow</span>(y, <span class=\"number\">3</span>)&lt;<span class=\"number\">0</span></span><br><span class=\"line\">random.seed(<span class=\"number\">123</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">100000</span>):</span><br><span class=\"line\">    x = (random.random() - <span class=\"number\">0.5</span>)*<span class=\"number\">4</span></span><br><span class=\"line\">    y = (random.random() - <span class=\"number\">0.5</span>)*<span class=\"number\">4</span></span><br><span class=\"line\">    label = <span class=\"built_in\">int</span>(func(x, y))</span><br><span class=\"line\">    points.append([x, y, label])</span><br></pre></td></tr></table></figure>\n<p>func定义一个心形函数，random.random()生成[0, 1)范围内的实数，将它们放缩到[-2, 2]区间。</p>\n<p><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/init.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.DataFrame(points,columns=[<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>, <span class=\"string\">&#x27;label&#x27;</span>])</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">sns_plot = sns.scatterplot(data = df, x = <span class=\"string\">&#x27;x&#x27;</span>, y = <span class=\"string\">&#x27;y&#x27;</span>, hue = <span class=\"string\">&#x27;label&#x27;</span>)</span><br><span class=\"line\">sns_plot.figure.savefig(<span class=\"string\">&#x27;./init.png&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>将点和label信息放入数据框中作为我们的数据集，利用sns可视化数据。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><p><strong>Training = Dataset and Dataloader + Model + Loss and Optimizer</strong></p>\n<h2 id=\"Dataset-amp-Dataloader\"><a href=\"#Dataset-amp-Dataloader\" class=\"headerlink\" title=\"Dataset &amp; Dataloader\"></a>Dataset &amp; Dataloader</h2><p>Dataset 和 DataLoader的构建。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader, random_split</span><br></pre></td></tr></table></figure>\n<h3 id=\"Dataset-自定义\"><a href=\"#Dataset-自定义\" class=\"headerlink\" title=\"Dataset 自定义\"></a>Dataset 自定义</h3><p>实际工程中拿到的数据格式、类型不一，但是基本的Dataset构建方式是有一定范式的。</p>\n<p>自定义数据集的要点有三：</p>\n<ul>\n<li>继承torch.utils.data.Dataset</li>\n<li>重写 __getitem__函数</li>\n<li>重写 __len__函数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PointDataset</span>(<span class=\"params\">Dataset</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, dataframe:pd.DataFrame</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(PointDataset, self).__init__()</span><br><span class=\"line\">        self.data = dataframe</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span>(<span class=\"params\">self, index</span>):</span></span><br><span class=\"line\">        point = torch.tensor([[self.data[<span class=\"string\">&#x27;x&#x27;</span>][index], self.data[<span class=\"string\">&#x27;y&#x27;</span>][index]]], dtype = torch.float32)</span><br><span class=\"line\">        label = torch.tensor([self.data[<span class=\"string\">&#x27;label&#x27;</span>][index]], dtype = torch.float32)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> point, label</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.data)</span><br></pre></td></tr></table></figure>\n\n<p>__getitem__函数是提供给DataLoader获取一个数据集元素的方法，给定一个index返回tensor化的数据集元素。<br>__len__函数是返回数据集的总长度</p>\n<blockquote>\n<p>Tips : 当传入的数据比较大比如读入图片时，一般方法是通过__init__函数构建一个存放着图像路径的数据框， 然后在__getitem__中读入图片到内存中。</p>\n</blockquote>\n<p>Dataset实际上是一个迭代器，可以试试以下代码输出dataset中的元素。</p>\n<h3 id=\"DataLoader\"><a href=\"#DataLoader\" class=\"headerlink\" title=\"DataLoader\"></a>DataLoader</h3><p>DataLoader的使用方法比较一般，只要传入上一步自己定义的Dataset和一些常规参数即可。</p>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>Model继承torch.nn.Module类，在model的构建上有较多的tensor和模型模块的技巧，这里只是简单地构建一个MLP。在__init__中定义模块，在__forward__中阐述完整模型的构建。</p>\n<p>一般来说在__init__中定义需要训练的部分，比如nn.Linear等，在__forward__处定义无需训练的模块如normalize。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLP</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MLP, self).__init__()</span><br><span class=\"line\">        </span><br><span class=\"line\">        self.layers = nn.Sequential(</span><br><span class=\"line\">          nn.Flatten(),</span><br><span class=\"line\">          nn.Linear(<span class=\"number\">2</span>, <span class=\"number\">72</span>),</span><br><span class=\"line\">          nn.ReLU(),</span><br><span class=\"line\">          nn.Linear(<span class=\"number\">72</span>, <span class=\"number\">36</span>),</span><br><span class=\"line\">          nn.ReLU(),</span><br><span class=\"line\">          nn.Linear(<span class=\"number\">36</span>, <span class=\"number\">1</span>),</span><br><span class=\"line\">          nn.Sigmoid(),</span><br><span class=\"line\">    )</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        x = self.layers(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p>在此处虽然我们的点是二维的，但是模型只是线性神经元的操作，不涉及卷积，所以将tensro用nn.Flatten()展成一维的。</p>\n<blockquote>\n<p>sigmoid函数将得分放缩到[0, 1)之间，得到每一个标签的概率，这里与loss的选择相对应。</p>\n</blockquote>\n<h3 id=\"Loss-amp-Optimizer\"><a href=\"#Loss-amp-Optimizer\" class=\"headerlink\" title=\"Loss &amp; Optimizer\"></a>Loss &amp; Optimizer</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">criterion = nn.BCELoss()</span><br><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=<span class=\"number\">0.9</span>)</span><br></pre></td></tr></table></figure>\n<p>分类任务一般使用交叉熵损失函数，二分类使用BCELoss，多分类使用nn.CrossEntropyLoss。</p>\n<p>optimizer一般使用Adam和SGD。此处采用的是带动量的随机梯度下降法。优化器的选择也是一个重（玄）要（学）的方面，不同的任务在不同的优化器上差异较大。</p>\n<h3 id=\"训练步骤\"><a href=\"#训练步骤\" class=\"headerlink\" title=\"训练步骤\"></a>训练步骤</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span>(<span class=\"params\">config, train_loader, val_loader</span>):</span></span><br><span class=\"line\">    net = MLP()</span><br><span class=\"line\">    criterion = nn.BCELoss()</span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    net.train()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(config.epochs):</span><br><span class=\"line\">        train_loss = <span class=\"number\">0</span></span><br><span class=\"line\">        n_total = <span class=\"number\">0</span></span><br><span class=\"line\">        eval_loss = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> batch, (x, y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>((train_loader)):</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            x = x.to(device)</span><br><span class=\"line\">            y = y.to(device)</span><br><span class=\"line\">            outputs = net(x)</span><br><span class=\"line\">            <span class=\"comment\">#todo cuda</span></span><br><span class=\"line\">            loss = criterion(outputs, y)</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            train_loss += loss.item()*<span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">            n_total += <span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\">        eval_loss = <span class=\"built_in\">eval</span>(net, config, val_loader)</span><br><span class=\"line\">        train_loss = train_loss / n_total</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((e % <span class=\"number\">5</span> == <span class=\"number\">0</span>) <span class=\"keyword\">and</span> (e &gt;= <span class=\"number\">40</span>) ):</span><br><span class=\"line\">            torch.save(net, config.save_path + <span class=\"string\">&quot;&#123;:4f&#125;.pth&quot;</span>.<span class=\"built_in\">format</span>(eval_loss))</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Epoch: &#123;&#125;/&#123;&#125;, Train Loss: &#123;&#125;, Val Loss: &#123;&#125;&quot;</span>.<span class=\"built_in\">format</span>(e, config.epochs, train_loss, eval_loss))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>定义网络结构，loss，optimizer</li>\n<li>epoch循环，定义每一个epoch中的loss,acc,miou等<br>　- batch循环<br>　- 优化器梯度清0，正向传播计算loss,自动反向传播，step更新所有参数<br>　- eval模型，在验证集上跑模型<br>　- 输出epoch结果，保存最优模型</li>\n</ul>\n<p>额外需要说明的两点：</p>\n<ol>\n<li>如果使用gpu的话，net和tensor都要放在gpu上跑，因为cpu和gpu的数据是不互通的</li>\n<li>保存模型有两种方法，这里采用的是最简单粗暴的方法，直接序列化。</li>\n</ol>\n<p>eval函数大同小异</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">eval</span>(<span class=\"params\">model, congfig, data_loader</span>):</span></span><br><span class=\"line\">    model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">    eval_loss = <span class=\"number\">0</span></span><br><span class=\"line\">    n_total = <span class=\"number\">0</span></span><br><span class=\"line\">    criterion = nn.BCELoss()</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> batch, (x, y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>((data_loader)):</span><br><span class=\"line\">            x = x.to(device)</span><br><span class=\"line\">            y = y.to(device)</span><br><span class=\"line\">            outputs = model(x)</span><br><span class=\"line\">            loss = criterion(outputs, y)</span><br><span class=\"line\">            eval_loss += loss.item()*<span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">            n_total += <span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">    model.train()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> eval_loss / n_total</span><br></pre></td></tr></table></figure>\n<p>注意在两者间切换时利用net.eval()和net.train()做相应的改变。</p>\n<p>至此，基于pytorch的深度学习pipeline完成。</p>\n<h2 id=\"训练结果\"><a href=\"#训练结果\" class=\"headerlink\" title=\"训练结果\"></a>训练结果</h2><p>给出测试接口如下，对结果做可视化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cut_threshold</span>(<span class=\"params\">x</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">int</span>(<span class=\"number\">1</span>) <span class=\"keyword\">if</span> x &gt; <span class=\"number\">0.5</span> <span class=\"keyword\">else</span> <span class=\"built_in\">int</span>(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_model</span>(<span class=\"params\">model_path, data_loader</span>):</span></span><br><span class=\"line\">    net = torch.load(model_path)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    target = []</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> batch, (x, y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>((data_loader)):</span><br><span class=\"line\">        x = x.to(device)</span><br><span class=\"line\">        y = y.to(device)</span><br><span class=\"line\">        outputs = net(x)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(x.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">            target.append([x[i][<span class=\"number\">0</span>][<span class=\"number\">0</span>].item(), x[i][<span class=\"number\">0</span>][<span class=\"number\">1</span>].item(), cut_threshold(outputs[i][<span class=\"number\">0</span>].item())])</span><br><span class=\"line\">    df = pd.DataFrame(target,columns=[<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>, <span class=\"string\">&#x27;label&#x27;</span>])</span><br><span class=\"line\">    sns_plot = sns.scatterplot(data = df, x = <span class=\"string\">&#x27;x&#x27;</span>, y = <span class=\"string\">&#x27;y&#x27;</span>, hue = <span class=\"string\">&#x27;label&#x27;</span>)</span><br><span class=\"line\">    sns_plot.figure.savefig(model_path[:<span class=\"number\">12</span>]+ <span class=\"string\">&#x27;.png&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>设置阈值为0.5即当概率大于threshold时认为是正样本。每5个epoch保存的模型测试可视化结果并对比原始标签如下：</p>\n<p><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/1.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/2.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/3.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/4.png\"></p>\n<p><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/5.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/6.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/7.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/8.png\"></p>\n<p><a href=\"init.png\"></a></p>\n<h2 id=\"工程常用方法\"><a href=\"#工程常用方法\" class=\"headerlink\" title=\"工程常用方法\"></a>工程常用方法</h2><ol>\n<li>创建directory分类，常把函数分入Model, Dataset, utils模块。</li>\n<li>使用argparse包以做到命令行传参，并设立单独的class初始化默认参数。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p>#Pytorch深度学习的一般流程</p>\n<p>这篇文章适用于有机器学习和深度学习基础但是不能熟练使用pytorch完成完整任务的读者。本文将基于一个心型分类器归纳代码的组成。</p>\n<p>本文给出100k个点，根据心形函数对点打上里（1）和外（0）的标签，我们的目标是将点的坐标作为输入，给出标签也即是一个二分类的问题。</p>\n<p>首先对于一个深度学习网络我们一般要完成三个部分：</p>\n<ul>\n<li>train 训练</li>\n<li>eval 阶段性评估，在eval中选取更优的网络</li>\n<li>interface/test 提供使用网路的接口（可选）</li>\n</ul>\n<h2 id=\"数据准备说明\"><a href=\"#数据准备说明\" class=\"headerlink\" title=\"数据准备说明\"></a>数据准备说明</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> math <span class=\"keyword\">import</span> <span class=\"built_in\">pow</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\">points = []</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span>(<span class=\"params\">x, y</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">pow</span>((x**<span class=\"number\">2</span> + y**<span class=\"number\">2</span> - <span class=\"number\">1</span>), <span class=\"number\">3</span>) - <span class=\"built_in\">pow</span>(x, <span class=\"number\">2</span>)*<span class=\"built_in\">pow</span>(y, <span class=\"number\">3</span>)&lt;<span class=\"number\">0</span></span><br><span class=\"line\">random.seed(<span class=\"number\">123</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">100000</span>):</span><br><span class=\"line\">    x = (random.random() - <span class=\"number\">0.5</span>)*<span class=\"number\">4</span></span><br><span class=\"line\">    y = (random.random() - <span class=\"number\">0.5</span>)*<span class=\"number\">4</span></span><br><span class=\"line\">    label = <span class=\"built_in\">int</span>(func(x, y))</span><br><span class=\"line\">    points.append([x, y, label])</span><br></pre></td></tr></table></figure>\n<p>func定义一个心形函数，random.random()生成[0, 1)范围内的实数，将它们放缩到[-2, 2]区间。</p>\n<p><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/init.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.DataFrame(points,columns=[<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>, <span class=\"string\">&#x27;label&#x27;</span>])</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">sns_plot = sns.scatterplot(data = df, x = <span class=\"string\">&#x27;x&#x27;</span>, y = <span class=\"string\">&#x27;y&#x27;</span>, hue = <span class=\"string\">&#x27;label&#x27;</span>)</span><br><span class=\"line\">sns_plot.figure.savefig(<span class=\"string\">&#x27;./init.png&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>将点和label信息放入数据框中作为我们的数据集，利用sns可视化数据。</p>\n<h2 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h2><p><strong>Training = Dataset and Dataloader + Model + Loss and Optimizer</strong></p>\n<h2 id=\"Dataset-amp-Dataloader\"><a href=\"#Dataset-amp-Dataloader\" class=\"headerlink\" title=\"Dataset &amp; Dataloader\"></a>Dataset &amp; Dataloader</h2><p>Dataset 和 DataLoader的构建。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader, random_split</span><br></pre></td></tr></table></figure>\n<h3 id=\"Dataset-自定义\"><a href=\"#Dataset-自定义\" class=\"headerlink\" title=\"Dataset 自定义\"></a>Dataset 自定义</h3><p>实际工程中拿到的数据格式、类型不一，但是基本的Dataset构建方式是有一定范式的。</p>\n<p>自定义数据集的要点有三：</p>\n<ul>\n<li>继承torch.utils.data.Dataset</li>\n<li>重写 __getitem__函数</li>\n<li>重写 __len__函数</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PointDataset</span>(<span class=\"params\">Dataset</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, dataframe:pd.DataFrame</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(PointDataset, self).__init__()</span><br><span class=\"line\">        self.data = dataframe</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__getitem__</span>(<span class=\"params\">self, index</span>):</span></span><br><span class=\"line\">        point = torch.tensor([[self.data[<span class=\"string\">&#x27;x&#x27;</span>][index], self.data[<span class=\"string\">&#x27;y&#x27;</span>][index]]], dtype = torch.float32)</span><br><span class=\"line\">        label = torch.tensor([self.data[<span class=\"string\">&#x27;label&#x27;</span>][index]], dtype = torch.float32)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> point, label</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__len__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">len</span>(self.data)</span><br></pre></td></tr></table></figure>\n\n<p>__getitem__函数是提供给DataLoader获取一个数据集元素的方法，给定一个index返回tensor化的数据集元素。<br>__len__函数是返回数据集的总长度</p>\n<blockquote>\n<p>Tips : 当传入的数据比较大比如读入图片时，一般方法是通过__init__函数构建一个存放着图像路径的数据框， 然后在__getitem__中读入图片到内存中。</p>\n</blockquote>\n<p>Dataset实际上是一个迭代器，可以试试以下代码输出dataset中的元素。</p>\n<h3 id=\"DataLoader\"><a href=\"#DataLoader\" class=\"headerlink\" title=\"DataLoader\"></a>DataLoader</h3><p>DataLoader的使用方法比较一般，只要传入上一步自己定义的Dataset和一些常规参数即可。</p>\n<h3 id=\"Model\"><a href=\"#Model\" class=\"headerlink\" title=\"Model\"></a>Model</h3><p>Model继承torch.nn.Module类，在model的构建上有较多的tensor和模型模块的技巧，这里只是简单地构建一个MLP。在__init__中定义模块，在__forward__中阐述完整模型的构建。</p>\n<p>一般来说在__init__中定义需要训练的部分，比如nn.Linear等，在__forward__处定义无需训练的模块如normalize。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MLP</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MLP, self).__init__()</span><br><span class=\"line\">        </span><br><span class=\"line\">        self.layers = nn.Sequential(</span><br><span class=\"line\">          nn.Flatten(),</span><br><span class=\"line\">          nn.Linear(<span class=\"number\">2</span>, <span class=\"number\">72</span>),</span><br><span class=\"line\">          nn.ReLU(),</span><br><span class=\"line\">          nn.Linear(<span class=\"number\">72</span>, <span class=\"number\">36</span>),</span><br><span class=\"line\">          nn.ReLU(),</span><br><span class=\"line\">          nn.Linear(<span class=\"number\">36</span>, <span class=\"number\">1</span>),</span><br><span class=\"line\">          nn.Sigmoid(),</span><br><span class=\"line\">    )</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        x = self.layers(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n\n<p>在此处虽然我们的点是二维的，但是模型只是线性神经元的操作，不涉及卷积，所以将tensro用nn.Flatten()展成一维的。</p>\n<blockquote>\n<p>sigmoid函数将得分放缩到[0, 1)之间，得到每一个标签的概率，这里与loss的选择相对应。</p>\n</blockquote>\n<h3 id=\"Loss-amp-Optimizer\"><a href=\"#Loss-amp-Optimizer\" class=\"headerlink\" title=\"Loss &amp; Optimizer\"></a>Loss &amp; Optimizer</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">criterion = nn.BCELoss()</span><br><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=<span class=\"number\">0.9</span>)</span><br></pre></td></tr></table></figure>\n<p>分类任务一般使用交叉熵损失函数，二分类使用BCELoss，多分类使用nn.CrossEntropyLoss。</p>\n<p>optimizer一般使用Adam和SGD。此处采用的是带动量的随机梯度下降法。优化器的选择也是一个重（玄）要（学）的方面，不同的任务在不同的优化器上差异较大。</p>\n<h3 id=\"训练步骤\"><a href=\"#训练步骤\" class=\"headerlink\" title=\"训练步骤\"></a>训练步骤</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span>(<span class=\"params\">config, train_loader, val_loader</span>):</span></span><br><span class=\"line\">    net = MLP()</span><br><span class=\"line\">    criterion = nn.BCELoss()</span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr = config.lr, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    net.train()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> e <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(config.epochs):</span><br><span class=\"line\">        train_loss = <span class=\"number\">0</span></span><br><span class=\"line\">        n_total = <span class=\"number\">0</span></span><br><span class=\"line\">        eval_loss = <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> batch, (x, y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>((train_loader)):</span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            x = x.to(device)</span><br><span class=\"line\">            y = y.to(device)</span><br><span class=\"line\">            outputs = net(x)</span><br><span class=\"line\">            <span class=\"comment\">#todo cuda</span></span><br><span class=\"line\">            loss = criterion(outputs, y)</span><br><span class=\"line\">            loss.backward()</span><br><span class=\"line\">            train_loss += loss.item()*<span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">            n_total += <span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\">        eval_loss = <span class=\"built_in\">eval</span>(net, config, val_loader)</span><br><span class=\"line\">        train_loss = train_loss / n_total</span><br><span class=\"line\">        <span class=\"keyword\">if</span> ((e % <span class=\"number\">5</span> == <span class=\"number\">0</span>) <span class=\"keyword\">and</span> (e &gt;= <span class=\"number\">40</span>) ):</span><br><span class=\"line\">            torch.save(net, config.save_path + <span class=\"string\">&quot;&#123;:4f&#125;.pth&quot;</span>.<span class=\"built_in\">format</span>(eval_loss))</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Epoch: &#123;&#125;/&#123;&#125;, Train Loss: &#123;&#125;, Val Loss: &#123;&#125;&quot;</span>.<span class=\"built_in\">format</span>(e, config.epochs, train_loss, eval_loss))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>定义网络结构，loss，optimizer</li>\n<li>epoch循环，定义每一个epoch中的loss,acc,miou等<br>　- batch循环<br>　- 优化器梯度清0，正向传播计算loss,自动反向传播，step更新所有参数<br>　- eval模型，在验证集上跑模型<br>　- 输出epoch结果，保存最优模型</li>\n</ul>\n<p>额外需要说明的两点：</p>\n<ol>\n<li>如果使用gpu的话，net和tensor都要放在gpu上跑，因为cpu和gpu的数据是不互通的</li>\n<li>保存模型有两种方法，这里采用的是最简单粗暴的方法，直接序列化。</li>\n</ol>\n<p>eval函数大同小异</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">eval</span>(<span class=\"params\">model, congfig, data_loader</span>):</span></span><br><span class=\"line\">    model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">    eval_loss = <span class=\"number\">0</span></span><br><span class=\"line\">    n_total = <span class=\"number\">0</span></span><br><span class=\"line\">    criterion = nn.BCELoss()</span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">        <span class=\"keyword\">for</span> batch, (x, y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>((data_loader)):</span><br><span class=\"line\">            x = x.to(device)</span><br><span class=\"line\">            y = y.to(device)</span><br><span class=\"line\">            outputs = model(x)</span><br><span class=\"line\">            loss = criterion(outputs, y)</span><br><span class=\"line\">            eval_loss += loss.item()*<span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">            n_total += <span class=\"built_in\">len</span>(outputs)</span><br><span class=\"line\">    model.train()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> eval_loss / n_total</span><br></pre></td></tr></table></figure>\n<p>注意在两者间切换时利用net.eval()和net.train()做相应的改变。</p>\n<p>至此，基于pytorch的深度学习pipeline完成。</p>\n<h2 id=\"训练结果\"><a href=\"#训练结果\" class=\"headerlink\" title=\"训练结果\"></a>训练结果</h2><p>给出测试接口如下，对结果做可视化</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cut_threshold</span>(<span class=\"params\">x</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">int</span>(<span class=\"number\">1</span>) <span class=\"keyword\">if</span> x &gt; <span class=\"number\">0.5</span> <span class=\"keyword\">else</span> <span class=\"built_in\">int</span>(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">test_model</span>(<span class=\"params\">model_path, data_loader</span>):</span></span><br><span class=\"line\">    net = torch.load(model_path)</span><br><span class=\"line\">    net.to(device)</span><br><span class=\"line\">    target = []</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">for</span> batch, (x, y) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>((data_loader)):</span><br><span class=\"line\">        x = x.to(device)</span><br><span class=\"line\">        y = y.to(device)</span><br><span class=\"line\">        outputs = net(x)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(x.shape[<span class=\"number\">0</span>]):</span><br><span class=\"line\">            target.append([x[i][<span class=\"number\">0</span>][<span class=\"number\">0</span>].item(), x[i][<span class=\"number\">0</span>][<span class=\"number\">1</span>].item(), cut_threshold(outputs[i][<span class=\"number\">0</span>].item())])</span><br><span class=\"line\">    df = pd.DataFrame(target,columns=[<span class=\"string\">&#x27;x&#x27;</span>,<span class=\"string\">&#x27;y&#x27;</span>, <span class=\"string\">&#x27;label&#x27;</span>])</span><br><span class=\"line\">    sns_plot = sns.scatterplot(data = df, x = <span class=\"string\">&#x27;x&#x27;</span>, y = <span class=\"string\">&#x27;y&#x27;</span>, hue = <span class=\"string\">&#x27;label&#x27;</span>)</span><br><span class=\"line\">    sns_plot.figure.savefig(model_path[:<span class=\"number\">12</span>]+ <span class=\"string\">&#x27;.png&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>设置阈值为0.5即当概率大于threshold时认为是正样本。每5个epoch保存的模型测试可视化结果并对比原始标签如下：</p>\n<p><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/1.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/2.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/3.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/4.png\"></p>\n<p><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/5.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/6.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/7.png\"><img src=\"/2021/08/26/deeplearning-pipeline-for-pytorch/8.png\"></p>\n<p><a href=\"init.png\"></a></p>\n<h2 id=\"工程常用方法\"><a href=\"#工程常用方法\" class=\"headerlink\" title=\"工程常用方法\"></a>工程常用方法</h2><ol>\n<li>创建directory分类，常把函数分入Model, Dataset, utils模块。</li>\n<li>使用argparse包以做到命令行传参，并设立单独的class初始化默认参数。</li>\n</ol>\n"},{"title":"Convolution","date":"2021-08-27T02:06:19.000Z","keywords":"卷积","mathjax":true,"cover":"/img/pic2.png","_content":"\n# 卷积\n\n卷积是对高维数据的常规操作，用于对高维数据的特征提取。\n\n## 为什么需要卷积\n\n在深度学习上应用卷积的历史需要从图像说起。在没有卷积时，图像识别的一般操作是利用神经网络提取特征，再对特征应用另一个神经网络作为解码器，以做图像分类。但是单纯的神经网络全连接层做特征提取是有很大的问题，即对位置敏感。\n\n例如给定一组马的照片喂入神经网络训练，假设一半马头朝向左边，一半马头朝向右边，那么神经网络学习到什么呢？在这个神经网络的认知里，马是双头的。这是因为全连接层其实没有利用二维的信息，它还是将图像展成一组一维的向量，导致学到的特征必须出现在特定的位置。如今卷积布置局限于图像识别，在文本分类中也出现了textcnn的卷积分类网络。\n\n卷积操作是线性的，利用卷积提取的特征的方式是非线性的，这样的特征对位置不敏感。卷积是一个三维的卷积核在三维的矩阵由左至右，由上至下滑动运算得到一个二维的矩阵。从如下的示意图中可以看出，卷积后的特征的宽和高回缩小，而深度取决于卷积核的个数。\n\n![](c.gif)\n\n## torch 卷积\n\n### conv2d\n\npytroch中的二维卷积定义如下：\n```python\ntorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n```\n\n对应给定的[N, C, H, W]输入（N为batch_size, C为Channels数，彩色图像中为3，H、W分别为高和宽），卷积操作后得到[N, C_hat, H_hat, W_hat]，C_hat为卷积核个数。不考虑padd,H_hat, W_hat的公式如下：\n\n(图像尺寸 - 卷积核尺寸 + 2`*填充值)/步长 + 1\n\ntorch的一个示例：\n\n```python\nimport torch\ninput = torch.tensor([[1., 2.,  3., 4.,],\n                      [5., 6., 7., 8. ],\n                      [9., 10., 11., 12.],\n                      [13., 14., 15., 16.]])\ninput = input.unsqueeze(0)\ninput = input.unsqueeze(0)\nconv = nn.Conv2d(1, 1, 2, stride = 1, padding = 0)\noutput = conv(input)\nprint(output)\nprint(output.shape)\nconv = nn.Conv2d(1, 2, 2, stride = 1, padding = 0)\noutput = conv(input)\nprint(output)\nprint(output.shape)\n```\n\n\n```\n//output\ntensor([[[[0.3269, 0.4242, 0.5214],\n          [0.7158, 0.8131, 0.9103],\n          [1.1047, 1.2020, 1.2992]]]], grad_fn=<ThnnConv2DBackward>)\n\ntorch.Size([1, 1, 3, 3])\n\ntensor([[[[2.4007, 2.8337, 3.2666],\n          [4.1324, 4.5653, 4.9982],\n          [5.8640, 6.2969, 6.7298]],\n\n         [[0.1943, 0.3063, 0.4184],\n          [0.6425, 0.7546, 0.8667],\n          [1.0908, 1.2029, 1.3150]]]], grad_fn=<ThnnConv2DBackward>)\ntorch.Size([1, 2, 3, 3])\n\n```\n\n### padding\n\n单纯的卷积必然伴随着图像高和宽度的减小，但是我们并不想使得特征过窄，padding即填充操作可以**先**把图像扩充，然后**卷积**。\n\n示例对比如下：\n```python\nprint(input.shape)\nconv = nn.Conv2d(1, 1, 2, stride = 1, padding = 2)\noutput = conv(input)\nprint(output.shape)\n```\n\n```\n//输出\n[1, 1, 4, 4]\n[1, 1, 7, 7]\n```\n7 = 4+2+2-1\n\n\n### 感受野\n\n网友给出的定义\n> 某一层feature map中某个位置的特征向量，是由前面某一层固定区域的输入计算出来的那这个区域就是感受野。\nconv(5, 5)的感受野即是 $ 5 \\ast 5$ 的区域，一般来说感受野越大越好。\n\n\n### 其他卷积\n\n**空洞卷积**示意图如下：\n![](d.gif)\n作用\n- 扩大感受野\n- 捕获多尺度上下文信息\n\n**深度可分离卷积**\n\n作用：\n应用于轻量级网络，mobielnetv+系列，极大减少参数量，适用于web端以及移动端部署，代价是精度的一定程度下降。\n","source":"_posts/Convolution.md","raw":"---\ntitle: Convolution\ndate: 2021-08-27 10:06:19\ntags: 卷积\nkeywords: 卷积\ncategories: 深度学习\nmathjax: true\ncover:\n---\n\n# 卷积\n\n卷积是对高维数据的常规操作，用于对高维数据的特征提取。\n\n## 为什么需要卷积\n\n在深度学习上应用卷积的历史需要从图像说起。在没有卷积时，图像识别的一般操作是利用神经网络提取特征，再对特征应用另一个神经网络作为解码器，以做图像分类。但是单纯的神经网络全连接层做特征提取是有很大的问题，即对位置敏感。\n\n例如给定一组马的照片喂入神经网络训练，假设一半马头朝向左边，一半马头朝向右边，那么神经网络学习到什么呢？在这个神经网络的认知里，马是双头的。这是因为全连接层其实没有利用二维的信息，它还是将图像展成一组一维的向量，导致学到的特征必须出现在特定的位置。如今卷积布置局限于图像识别，在文本分类中也出现了textcnn的卷积分类网络。\n\n卷积操作是线性的，利用卷积提取的特征的方式是非线性的，这样的特征对位置不敏感。卷积是一个三维的卷积核在三维的矩阵由左至右，由上至下滑动运算得到一个二维的矩阵。从如下的示意图中可以看出，卷积后的特征的宽和高回缩小，而深度取决于卷积核的个数。\n\n![](c.gif)\n\n## torch 卷积\n\n### conv2d\n\npytroch中的二维卷积定义如下：\n```python\ntorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n```\n\n对应给定的[N, C, H, W]输入（N为batch_size, C为Channels数，彩色图像中为3，H、W分别为高和宽），卷积操作后得到[N, C_hat, H_hat, W_hat]，C_hat为卷积核个数。不考虑padd,H_hat, W_hat的公式如下：\n\n(图像尺寸 - 卷积核尺寸 + 2`*填充值)/步长 + 1\n\ntorch的一个示例：\n\n```python\nimport torch\ninput = torch.tensor([[1., 2.,  3., 4.,],\n                      [5., 6., 7., 8. ],\n                      [9., 10., 11., 12.],\n                      [13., 14., 15., 16.]])\ninput = input.unsqueeze(0)\ninput = input.unsqueeze(0)\nconv = nn.Conv2d(1, 1, 2, stride = 1, padding = 0)\noutput = conv(input)\nprint(output)\nprint(output.shape)\nconv = nn.Conv2d(1, 2, 2, stride = 1, padding = 0)\noutput = conv(input)\nprint(output)\nprint(output.shape)\n```\n\n\n```\n//output\ntensor([[[[0.3269, 0.4242, 0.5214],\n          [0.7158, 0.8131, 0.9103],\n          [1.1047, 1.2020, 1.2992]]]], grad_fn=<ThnnConv2DBackward>)\n\ntorch.Size([1, 1, 3, 3])\n\ntensor([[[[2.4007, 2.8337, 3.2666],\n          [4.1324, 4.5653, 4.9982],\n          [5.8640, 6.2969, 6.7298]],\n\n         [[0.1943, 0.3063, 0.4184],\n          [0.6425, 0.7546, 0.8667],\n          [1.0908, 1.2029, 1.3150]]]], grad_fn=<ThnnConv2DBackward>)\ntorch.Size([1, 2, 3, 3])\n\n```\n\n### padding\n\n单纯的卷积必然伴随着图像高和宽度的减小，但是我们并不想使得特征过窄，padding即填充操作可以**先**把图像扩充，然后**卷积**。\n\n示例对比如下：\n```python\nprint(input.shape)\nconv = nn.Conv2d(1, 1, 2, stride = 1, padding = 2)\noutput = conv(input)\nprint(output.shape)\n```\n\n```\n//输出\n[1, 1, 4, 4]\n[1, 1, 7, 7]\n```\n7 = 4+2+2-1\n\n\n### 感受野\n\n网友给出的定义\n> 某一层feature map中某个位置的特征向量，是由前面某一层固定区域的输入计算出来的那这个区域就是感受野。\nconv(5, 5)的感受野即是 $ 5 \\ast 5$ 的区域，一般来说感受野越大越好。\n\n\n### 其他卷积\n\n**空洞卷积**示意图如下：\n![](d.gif)\n作用\n- 扩大感受野\n- 捕获多尺度上下文信息\n\n**深度可分离卷积**\n\n作用：\n应用于轻量级网络，mobielnetv+系列，极大减少参数量，适用于web端以及移动端部署，代价是精度的一定程度下降。\n","slug":"Convolution","published":1,"updated":"2021-08-27T14:39:59.197Z","_id":"ckstsv84g00000vkk9guabgrp","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h1><p>卷积是对高维数据的常规操作，用于对高维数据的特征提取。</p>\n<h2 id=\"为什么需要卷积\"><a href=\"#为什么需要卷积\" class=\"headerlink\" title=\"为什么需要卷积\"></a>为什么需要卷积</h2><p>在深度学习上应用卷积的历史需要从图像说起。在没有卷积时，图像识别的一般操作是利用神经网络提取特征，再对特征应用另一个神经网络作为解码器，以做图像分类。但是单纯的神经网络全连接层做特征提取是有很大的问题，即对位置敏感。</p>\n<p>例如给定一组马的照片喂入神经网络训练，假设一半马头朝向左边，一半马头朝向右边，那么神经网络学习到什么呢？在这个神经网络的认知里，马是双头的。这是因为全连接层其实没有利用二维的信息，它还是将图像展成一组一维的向量，导致学到的特征必须出现在特定的位置。如今卷积布置局限于图像识别，在文本分类中也出现了textcnn的卷积分类网络。</p>\n<p>卷积操作是线性的，利用卷积提取的特征的方式是非线性的，这样的特征对位置不敏感。卷积是一个三维的卷积核在三维的矩阵由左至右，由上至下滑动运算得到一个二维的矩阵。从如下的示意图中可以看出，卷积后的特征的宽和高回缩小，而深度取决于卷积核的个数。</p>\n<p><img src=\"/2021/08/27/Convolution/c.gif\"></p>\n<h2 id=\"torch-卷积\"><a href=\"#torch-卷积\" class=\"headerlink\" title=\"torch 卷积\"></a>torch 卷积</h2><h3 id=\"conv2d\"><a href=\"#conv2d\" class=\"headerlink\" title=\"conv2d\"></a>conv2d</h3><p>pytroch中的二维卷积定义如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>, dilation=<span class=\"number\">1</span>, groups=<span class=\"number\">1</span>, bias=<span class=\"literal\">True</span>, padding_mode=<span class=\"string\">&#x27;zeros&#x27;</span>, device=<span class=\"literal\">None</span>, dtype=<span class=\"literal\">None</span>)</span><br></pre></td></tr></table></figure>\n\n<p>对应给定的[N, C, H, W]输入（N为batch_size, C为Channels数，彩色图像中为3，H、W分别为高和宽），卷积操作后得到[N, C_hat, H_hat, W_hat]，C_hat为卷积核个数。不考虑padd,H_hat, W_hat的公式如下：</p>\n<p>(图像尺寸 - 卷积核尺寸 + 2`*填充值)/步长 + 1</p>\n<p>torch的一个示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([[<span class=\"number\">1.</span>, <span class=\"number\">2.</span>,  <span class=\"number\">3.</span>, <span class=\"number\">4.</span>,],</span><br><span class=\"line\">                      [<span class=\"number\">5.</span>, <span class=\"number\">6.</span>, <span class=\"number\">7.</span>, <span class=\"number\">8.</span> ],</span><br><span class=\"line\">                      [<span class=\"number\">9.</span>, <span class=\"number\">10.</span>, <span class=\"number\">11.</span>, <span class=\"number\">12.</span>],</span><br><span class=\"line\">                      [<span class=\"number\">13.</span>, <span class=\"number\">14.</span>, <span class=\"number\">15.</span>, <span class=\"number\">16.</span>]])</span><br><span class=\"line\"><span class=\"built_in\">input</span> = <span class=\"built_in\">input</span>.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">input</span> = <span class=\"built_in\">input</span>.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">conv = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, stride = <span class=\"number\">1</span>, padding = <span class=\"number\">0</span>)</span><br><span class=\"line\">output = conv(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)</span><br><span class=\"line\">conv = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, stride = <span class=\"number\">1</span>, padding = <span class=\"number\">0</span>)</span><br><span class=\"line\">output = conv(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//output</span><br><span class=\"line\">tensor([[[[0.3269, 0.4242, 0.5214],</span><br><span class=\"line\">          [0.7158, 0.8131, 0.9103],</span><br><span class=\"line\">          [1.1047, 1.2020, 1.2992]]]], grad_fn=&lt;ThnnConv2DBackward&gt;)</span><br><span class=\"line\"></span><br><span class=\"line\">torch.Size([1, 1, 3, 3])</span><br><span class=\"line\"></span><br><span class=\"line\">tensor([[[[2.4007, 2.8337, 3.2666],</span><br><span class=\"line\">          [4.1324, 4.5653, 4.9982],</span><br><span class=\"line\">          [5.8640, 6.2969, 6.7298]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[0.1943, 0.3063, 0.4184],</span><br><span class=\"line\">          [0.6425, 0.7546, 0.8667],</span><br><span class=\"line\">          [1.0908, 1.2029, 1.3150]]]], grad_fn=&lt;ThnnConv2DBackward&gt;)</span><br><span class=\"line\">torch.Size([1, 2, 3, 3])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"padding\"><a href=\"#padding\" class=\"headerlink\" title=\"padding\"></a>padding</h3><p>单纯的卷积必然伴随着图像高和宽度的减小，但是我们并不想使得特征过窄，padding即填充操作可以<strong>先</strong>把图像扩充，然后<strong>卷积</strong>。</p>\n<p>示例对比如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">input</span>.shape)</span><br><span class=\"line\">conv = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, stride = <span class=\"number\">1</span>, padding = <span class=\"number\">2</span>)</span><br><span class=\"line\">output = conv(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//输出</span><br><span class=\"line\">[1, 1, 4, 4]</span><br><span class=\"line\">[1, 1, 7, 7]</span><br></pre></td></tr></table></figure>\n<p>7 = 4+2+2-1</p>\n<h3 id=\"感受野\"><a href=\"#感受野\" class=\"headerlink\" title=\"感受野\"></a>感受野</h3><p>网友给出的定义</p>\n<blockquote>\n<p>某一层feature map中某个位置的特征向量，是由前面某一层固定区域的输入计算出来的那这个区域就是感受野。<br>conv(5, 5)的感受野即是 $ 5 \\ast 5$ 的区域，一般来说感受野越大越好。</p>\n</blockquote>\n<h3 id=\"其他卷积\"><a href=\"#其他卷积\" class=\"headerlink\" title=\"其他卷积\"></a>其他卷积</h3><p><strong>空洞卷积</strong>示意图如下：<br><img src=\"/2021/08/27/Convolution/d.gif\"><br>作用</p>\n<ul>\n<li>扩大感受野</li>\n<li>捕获多尺度上下文信息</li>\n</ul>\n<p><strong>深度可分离卷积</strong></p>\n<p>作用：<br>应用于轻量级网络，mobielnetv+系列，极大减少参数量，适用于web端以及移动端部署，代价是精度的一定程度下降。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h1><p>卷积是对高维数据的常规操作，用于对高维数据的特征提取。</p>\n<h2 id=\"为什么需要卷积\"><a href=\"#为什么需要卷积\" class=\"headerlink\" title=\"为什么需要卷积\"></a>为什么需要卷积</h2><p>在深度学习上应用卷积的历史需要从图像说起。在没有卷积时，图像识别的一般操作是利用神经网络提取特征，再对特征应用另一个神经网络作为解码器，以做图像分类。但是单纯的神经网络全连接层做特征提取是有很大的问题，即对位置敏感。</p>\n<p>例如给定一组马的照片喂入神经网络训练，假设一半马头朝向左边，一半马头朝向右边，那么神经网络学习到什么呢？在这个神经网络的认知里，马是双头的。这是因为全连接层其实没有利用二维的信息，它还是将图像展成一组一维的向量，导致学到的特征必须出现在特定的位置。如今卷积布置局限于图像识别，在文本分类中也出现了textcnn的卷积分类网络。</p>\n<p>卷积操作是线性的，利用卷积提取的特征的方式是非线性的，这样的特征对位置不敏感。卷积是一个三维的卷积核在三维的矩阵由左至右，由上至下滑动运算得到一个二维的矩阵。从如下的示意图中可以看出，卷积后的特征的宽和高回缩小，而深度取决于卷积核的个数。</p>\n<p><img src=\"/2021/08/27/Convolution/c.gif\"></p>\n<h2 id=\"torch-卷积\"><a href=\"#torch-卷积\" class=\"headerlink\" title=\"torch 卷积\"></a>torch 卷积</h2><h3 id=\"conv2d\"><a href=\"#conv2d\" class=\"headerlink\" title=\"conv2d\"></a>conv2d</h3><p>pytroch中的二维卷积定义如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">0</span>, dilation=<span class=\"number\">1</span>, groups=<span class=\"number\">1</span>, bias=<span class=\"literal\">True</span>, padding_mode=<span class=\"string\">&#x27;zeros&#x27;</span>, device=<span class=\"literal\">None</span>, dtype=<span class=\"literal\">None</span>)</span><br></pre></td></tr></table></figure>\n\n<p>对应给定的[N, C, H, W]输入（N为batch_size, C为Channels数，彩色图像中为3，H、W分别为高和宽），卷积操作后得到[N, C_hat, H_hat, W_hat]，C_hat为卷积核个数。不考虑padd,H_hat, W_hat的公式如下：</p>\n<p>(图像尺寸 - 卷积核尺寸 + 2`*填充值)/步长 + 1</p>\n<p>torch的一个示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"built_in\">input</span> = torch.tensor([[<span class=\"number\">1.</span>, <span class=\"number\">2.</span>,  <span class=\"number\">3.</span>, <span class=\"number\">4.</span>,],</span><br><span class=\"line\">                      [<span class=\"number\">5.</span>, <span class=\"number\">6.</span>, <span class=\"number\">7.</span>, <span class=\"number\">8.</span> ],</span><br><span class=\"line\">                      [<span class=\"number\">9.</span>, <span class=\"number\">10.</span>, <span class=\"number\">11.</span>, <span class=\"number\">12.</span>],</span><br><span class=\"line\">                      [<span class=\"number\">13.</span>, <span class=\"number\">14.</span>, <span class=\"number\">15.</span>, <span class=\"number\">16.</span>]])</span><br><span class=\"line\"><span class=\"built_in\">input</span> = <span class=\"built_in\">input</span>.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"built_in\">input</span> = <span class=\"built_in\">input</span>.unsqueeze(<span class=\"number\">0</span>)</span><br><span class=\"line\">conv = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, stride = <span class=\"number\">1</span>, padding = <span class=\"number\">0</span>)</span><br><span class=\"line\">output = conv(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)</span><br><span class=\"line\">conv = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, stride = <span class=\"number\">1</span>, padding = <span class=\"number\">0</span>)</span><br><span class=\"line\">output = conv(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//output</span><br><span class=\"line\">tensor([[[[0.3269, 0.4242, 0.5214],</span><br><span class=\"line\">          [0.7158, 0.8131, 0.9103],</span><br><span class=\"line\">          [1.1047, 1.2020, 1.2992]]]], grad_fn=&lt;ThnnConv2DBackward&gt;)</span><br><span class=\"line\"></span><br><span class=\"line\">torch.Size([1, 1, 3, 3])</span><br><span class=\"line\"></span><br><span class=\"line\">tensor([[[[2.4007, 2.8337, 3.2666],</span><br><span class=\"line\">          [4.1324, 4.5653, 4.9982],</span><br><span class=\"line\">          [5.8640, 6.2969, 6.7298]],</span><br><span class=\"line\"></span><br><span class=\"line\">         [[0.1943, 0.3063, 0.4184],</span><br><span class=\"line\">          [0.6425, 0.7546, 0.8667],</span><br><span class=\"line\">          [1.0908, 1.2029, 1.3150]]]], grad_fn=&lt;ThnnConv2DBackward&gt;)</span><br><span class=\"line\">torch.Size([1, 2, 3, 3])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"padding\"><a href=\"#padding\" class=\"headerlink\" title=\"padding\"></a>padding</h3><p>单纯的卷积必然伴随着图像高和宽度的减小，但是我们并不想使得特征过窄，padding即填充操作可以<strong>先</strong>把图像扩充，然后<strong>卷积</strong>。</p>\n<p>示例对比如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">input</span>.shape)</span><br><span class=\"line\">conv = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, stride = <span class=\"number\">1</span>, padding = <span class=\"number\">2</span>)</span><br><span class=\"line\">output = conv(<span class=\"built_in\">input</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(output.shape)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//输出</span><br><span class=\"line\">[1, 1, 4, 4]</span><br><span class=\"line\">[1, 1, 7, 7]</span><br></pre></td></tr></table></figure>\n<p>7 = 4+2+2-1</p>\n<h3 id=\"感受野\"><a href=\"#感受野\" class=\"headerlink\" title=\"感受野\"></a>感受野</h3><p>网友给出的定义</p>\n<blockquote>\n<p>某一层feature map中某个位置的特征向量，是由前面某一层固定区域的输入计算出来的那这个区域就是感受野。<br>conv(5, 5)的感受野即是 $ 5 \\ast 5$ 的区域，一般来说感受野越大越好。</p>\n</blockquote>\n<h3 id=\"其他卷积\"><a href=\"#其他卷积\" class=\"headerlink\" title=\"其他卷积\"></a>其他卷积</h3><p><strong>空洞卷积</strong>示意图如下：<br><img src=\"/2021/08/27/Convolution/d.gif\"><br>作用</p>\n<ul>\n<li>扩大感受野</li>\n<li>捕获多尺度上下文信息</li>\n</ul>\n<p><strong>深度可分离卷积</strong></p>\n<p>作用：<br>应用于轻量级网络，mobielnetv+系列，极大减少参数量，适用于web端以及移动端部署，代价是精度的一定程度下降。</p>\n"},{"title":"models-summary","date":"2021-08-27T07:02:36.000Z","mathjax":true,"cover":"/img/pic1.png","keywords":"CNN RNN GAN Resnet MobileNet","_content":"\n# 模型总结\n\n本篇博客对深度学习中的常见模型做出总结\n\n## CNN\n\n卷积神经网络是图像处理中最常用的网络之一。\n\n![](c.png)\n组成：\n- 卷积层\n- 池化层\n\n特点：在CV领域，应用卷积的网络中提取窄而深。\n\n## RNN\n\nRNN，循环神经网络主要应用于文本生成。\n特点：考虑输入序列的上下文信息，由此在NLP中可以预测文本生成。\n\n组成：RNN单元\n\n![](RNN.png)\n\n\n- X 向量输入\n- O 向量输出\n- S 每一个隐藏层的中间的输出\n- W U V 权重矩阵，不同时刻权重共享\n\n每个隐藏层都计算一个中间值，把这个值作为下一个隐藏层输入的之一，由此网络可以考虑到上下文信息。\n\n变种及改进：\n1. BiRNN（双向循环神经网络）RNN只考虑到前文对后文的影响，但是后文对前文没有影响。一个trival想法是多加一组隐藏层，完成由后至前的影响。\n![](BiRNN.png)\n2. LSTM（长短时记忆网络）RNN的模式决定了长期以前的上文对下文的影响很小，为了改进LSTM多输入一个向量作为门，输出是一个0，1的向量，表示是否利用长期的内容对后文的影响。\n![](LSTM.png)\n## GAN\n\n生成对抗网络利用模型对抗使得生成的样本符合要求。生成对抗网络多用在图片生成，表情变换等。\n\n![](GAN.png)\n\n特点： GAN有一个判别器和一个生成器。生成器用来生成数据，判别器用来判别生成的数据的真假。例如先训练生成器，当生成的图片在一定程度上骗过判别器，也即判别的表现不好使，停止训练生成器，训练判别器以提升判别能力，然后再次训练生成器。\n\n变种： \n1. 条件GAN,多输入一个label，要求判别器判别图片是否逼真的同时判断图片是否满足label，以生成满足label的图片。\n![](conditionGAN.png)\n2. Cycle GAN Cycle不仅要求图片逼真，还要求图片能够有另一个生成器复原回原来的图片，个人觉得和自编码器有异曲同工之妙。\n![](CycleGAN.png)\n## Resnet\n残差网络是一大创举，它的提出意味这更深的网络可以表现更优。\n背景：在卷积神经网络提出后，研究者为了更好的效果不断增加网络的层数到百层，但是发现训练效果不好由此有人唱衰深度学习，restnet在这样的情况下无疑是救深度学习于水火。\n\n![](resnet.png)\n\nResnet的思想是我添加一个bottleneck模块，将输入与bottleneck的输出相加作为下一个输入，而bottleneck的输出是可以为0的，也意味着我可以抛弃掉这个模块，但是不为0的情况说明网络在训练过程中，这个module work，也就是说我怎么样都不亏。专业一点说就是在网络加深的过程中，至少不会因为bottleneck的增加而退化。\n\n## Mobilenet\nMobilenet是一个轻量级网络系列，适用于对处理速度要求高，可以稍微牺牲一点精度的任务，如web端和嵌入式设备部署。\nMobilenet系列的基本思想是应用**深度卷积**和**逐点卷积**，极大得减少了参数量。\n\n深度卷积：卷积核只是一维的，只负责一个通道的运算。\n\n![](dw.png)\n\n逐点卷积：由于深度卷积的定义，特征在深度卷积后维度是不会改变的，利用逐点卷积可以做到升维的效果。其卷积核的大小为 $1 \\ast 1 \\ast C $, C为输入的通道数，也即逐点卷积后的维度由逐点卷积的卷积核个数决定，大小由于是(1, 1)的核所以不变。\n\n![](pw2.png)\n\n\n将两者结合得到深度可分离卷积架构。\n\n![](dsc.png)\n\n![](dsc2.png)\n","source":"_posts/models-summary.md","raw":"---\ntitle: models-summary\ndate: 2021-08-27 15:02:36\ntags: 模型总结\nmathjax: true\ncategories: 深度学习\ncover:\nkeywords: CNN RNN GAN Resnet MobileNet\n---\n\n# 模型总结\n\n本篇博客对深度学习中的常见模型做出总结\n\n## CNN\n\n卷积神经网络是图像处理中最常用的网络之一。\n\n![](c.png)\n组成：\n- 卷积层\n- 池化层\n\n特点：在CV领域，应用卷积的网络中提取窄而深。\n\n## RNN\n\nRNN，循环神经网络主要应用于文本生成。\n特点：考虑输入序列的上下文信息，由此在NLP中可以预测文本生成。\n\n组成：RNN单元\n\n![](RNN.png)\n\n\n- X 向量输入\n- O 向量输出\n- S 每一个隐藏层的中间的输出\n- W U V 权重矩阵，不同时刻权重共享\n\n每个隐藏层都计算一个中间值，把这个值作为下一个隐藏层输入的之一，由此网络可以考虑到上下文信息。\n\n变种及改进：\n1. BiRNN（双向循环神经网络）RNN只考虑到前文对后文的影响，但是后文对前文没有影响。一个trival想法是多加一组隐藏层，完成由后至前的影响。\n![](BiRNN.png)\n2. LSTM（长短时记忆网络）RNN的模式决定了长期以前的上文对下文的影响很小，为了改进LSTM多输入一个向量作为门，输出是一个0，1的向量，表示是否利用长期的内容对后文的影响。\n![](LSTM.png)\n## GAN\n\n生成对抗网络利用模型对抗使得生成的样本符合要求。生成对抗网络多用在图片生成，表情变换等。\n\n![](GAN.png)\n\n特点： GAN有一个判别器和一个生成器。生成器用来生成数据，判别器用来判别生成的数据的真假。例如先训练生成器，当生成的图片在一定程度上骗过判别器，也即判别的表现不好使，停止训练生成器，训练判别器以提升判别能力，然后再次训练生成器。\n\n变种： \n1. 条件GAN,多输入一个label，要求判别器判别图片是否逼真的同时判断图片是否满足label，以生成满足label的图片。\n![](conditionGAN.png)\n2. Cycle GAN Cycle不仅要求图片逼真，还要求图片能够有另一个生成器复原回原来的图片，个人觉得和自编码器有异曲同工之妙。\n![](CycleGAN.png)\n## Resnet\n残差网络是一大创举，它的提出意味这更深的网络可以表现更优。\n背景：在卷积神经网络提出后，研究者为了更好的效果不断增加网络的层数到百层，但是发现训练效果不好由此有人唱衰深度学习，restnet在这样的情况下无疑是救深度学习于水火。\n\n![](resnet.png)\n\nResnet的思想是我添加一个bottleneck模块，将输入与bottleneck的输出相加作为下一个输入，而bottleneck的输出是可以为0的，也意味着我可以抛弃掉这个模块，但是不为0的情况说明网络在训练过程中，这个module work，也就是说我怎么样都不亏。专业一点说就是在网络加深的过程中，至少不会因为bottleneck的增加而退化。\n\n## Mobilenet\nMobilenet是一个轻量级网络系列，适用于对处理速度要求高，可以稍微牺牲一点精度的任务，如web端和嵌入式设备部署。\nMobilenet系列的基本思想是应用**深度卷积**和**逐点卷积**，极大得减少了参数量。\n\n深度卷积：卷积核只是一维的，只负责一个通道的运算。\n\n![](dw.png)\n\n逐点卷积：由于深度卷积的定义，特征在深度卷积后维度是不会改变的，利用逐点卷积可以做到升维的效果。其卷积核的大小为 $1 \\ast 1 \\ast C $, C为输入的通道数，也即逐点卷积后的维度由逐点卷积的卷积核个数决定，大小由于是(1, 1)的核所以不变。\n\n![](pw2.png)\n\n\n将两者结合得到深度可分离卷积架构。\n\n![](dsc.png)\n\n![](dsc2.png)\n","slug":"models-summary","published":1,"updated":"2021-08-27T14:38:19.967Z","_id":"cksu0ciry00008tkk12893wz8","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"模型总结\"><a href=\"#模型总结\" class=\"headerlink\" title=\"模型总结\"></a>模型总结</h1><p>本篇博客对深度学习中的常见模型做出总结</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><p>卷积神经网络是图像处理中最常用的网络之一。</p>\n<p><img src=\"/2021/08/27/models-summary/c.png\"><br>组成：</p>\n<ul>\n<li>卷积层</li>\n<li>池化层</li>\n</ul>\n<p>特点：在CV领域，应用卷积的网络中提取窄而深。</p>\n<h2 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h2><p>RNN，循环神经网络主要应用于文本生成。<br>特点：考虑输入序列的上下文信息，由此在NLP中可以预测文本生成。</p>\n<p>组成：RNN单元</p>\n<p><img src=\"/2021/08/27/models-summary/RNN.png\"></p>\n<ul>\n<li>X 向量输入</li>\n<li>O 向量输出</li>\n<li>S 每一个隐藏层的中间的输出</li>\n<li>W U V 权重矩阵，不同时刻权重共享</li>\n</ul>\n<p>每个隐藏层都计算一个中间值，把这个值作为下一个隐藏层输入的之一，由此网络可以考虑到上下文信息。</p>\n<p>变种及改进：</p>\n<ol>\n<li>BiRNN（双向循环神经网络）RNN只考虑到前文对后文的影响，但是后文对前文没有影响。一个trival想法是多加一组隐藏层，完成由后至前的影响。<br><img src=\"/2021/08/27/models-summary/BiRNN.png\"></li>\n<li>LSTM（长短时记忆网络）RNN的模式决定了长期以前的上文对下文的影响很小，为了改进LSTM多输入一个向量作为门，输出是一个0，1的向量，表示是否利用长期的内容对后文的影响。<br><img src=\"/2021/08/27/models-summary/LSTM.png\"><h2 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h2></li>\n</ol>\n<p>生成对抗网络利用模型对抗使得生成的样本符合要求。生成对抗网络多用在图片生成，表情变换等。</p>\n<p><img src=\"/2021/08/27/models-summary/GAN.png\"></p>\n<p>特点： GAN有一个判别器和一个生成器。生成器用来生成数据，判别器用来判别生成的数据的真假。例如先训练生成器，当生成的图片在一定程度上骗过判别器，也即判别的表现不好使，停止训练生成器，训练判别器以提升判别能力，然后再次训练生成器。</p>\n<p>变种： </p>\n<ol>\n<li>条件GAN,多输入一个label，要求判别器判别图片是否逼真的同时判断图片是否满足label，以生成满足label的图片。<br><img src=\"/2021/08/27/models-summary/conditionGAN.png\"></li>\n<li>Cycle GAN Cycle不仅要求图片逼真，还要求图片能够有另一个生成器复原回原来的图片，个人觉得和自编码器有异曲同工之妙。<br><img src=\"/2021/08/27/models-summary/CycleGAN.png\"><h2 id=\"Resnet\"><a href=\"#Resnet\" class=\"headerlink\" title=\"Resnet\"></a>Resnet</h2>残差网络是一大创举，它的提出意味这更深的网络可以表现更优。<br>背景：在卷积神经网络提出后，研究者为了更好的效果不断增加网络的层数到百层，但是发现训练效果不好由此有人唱衰深度学习，restnet在这样的情况下无疑是救深度学习于水火。</li>\n</ol>\n<p><img src=\"/2021/08/27/models-summary/resnet.png\"></p>\n<p>Resnet的思想是我添加一个bottleneck模块，将输入与bottleneck的输出相加作为下一个输入，而bottleneck的输出是可以为0的，也意味着我可以抛弃掉这个模块，但是不为0的情况说明网络在训练过程中，这个module work，也就是说我怎么样都不亏。专业一点说就是在网络加深的过程中，至少不会因为bottleneck的增加而退化。</p>\n<h2 id=\"Mobilenet\"><a href=\"#Mobilenet\" class=\"headerlink\" title=\"Mobilenet\"></a>Mobilenet</h2><p>Mobilenet是一个轻量级网络系列，适用于对处理速度要求高，可以稍微牺牲一点精度的任务，如web端和嵌入式设备部署。<br>Mobilenet系列的基本思想是应用<strong>深度卷积</strong>和<strong>逐点卷积</strong>，极大得减少了参数量。</p>\n<p>深度卷积：卷积核只是一维的，只负责一个通道的运算。</p>\n<p><img src=\"/2021/08/27/models-summary/dw.png\"></p>\n<p>逐点卷积：由于深度卷积的定义，特征在深度卷积后维度是不会改变的，利用逐点卷积可以做到升维的效果。其卷积核的大小为 $1 \\ast 1 \\ast C $, C为输入的通道数，也即逐点卷积后的维度由逐点卷积的卷积核个数决定，大小由于是(1, 1)的核所以不变。</p>\n<p><img src=\"/2021/08/27/models-summary/pw2.png\"></p>\n<p>将两者结合得到深度可分离卷积架构。</p>\n<p><img src=\"/2021/08/27/models-summary/dsc.png\"></p>\n<p><img src=\"/2021/08/27/models-summary/dsc2.png\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"模型总结\"><a href=\"#模型总结\" class=\"headerlink\" title=\"模型总结\"></a>模型总结</h1><p>本篇博客对深度学习中的常见模型做出总结</p>\n<h2 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h2><p>卷积神经网络是图像处理中最常用的网络之一。</p>\n<p><img src=\"/2021/08/27/models-summary/c.png\"><br>组成：</p>\n<ul>\n<li>卷积层</li>\n<li>池化层</li>\n</ul>\n<p>特点：在CV领域，应用卷积的网络中提取窄而深。</p>\n<h2 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h2><p>RNN，循环神经网络主要应用于文本生成。<br>特点：考虑输入序列的上下文信息，由此在NLP中可以预测文本生成。</p>\n<p>组成：RNN单元</p>\n<p><img src=\"/2021/08/27/models-summary/RNN.png\"></p>\n<ul>\n<li>X 向量输入</li>\n<li>O 向量输出</li>\n<li>S 每一个隐藏层的中间的输出</li>\n<li>W U V 权重矩阵，不同时刻权重共享</li>\n</ul>\n<p>每个隐藏层都计算一个中间值，把这个值作为下一个隐藏层输入的之一，由此网络可以考虑到上下文信息。</p>\n<p>变种及改进：</p>\n<ol>\n<li>BiRNN（双向循环神经网络）RNN只考虑到前文对后文的影响，但是后文对前文没有影响。一个trival想法是多加一组隐藏层，完成由后至前的影响。<br><img src=\"/2021/08/27/models-summary/BiRNN.png\"></li>\n<li>LSTM（长短时记忆网络）RNN的模式决定了长期以前的上文对下文的影响很小，为了改进LSTM多输入一个向量作为门，输出是一个0，1的向量，表示是否利用长期的内容对后文的影响。<br><img src=\"/2021/08/27/models-summary/LSTM.png\"><h2 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h2></li>\n</ol>\n<p>生成对抗网络利用模型对抗使得生成的样本符合要求。生成对抗网络多用在图片生成，表情变换等。</p>\n<p><img src=\"/2021/08/27/models-summary/GAN.png\"></p>\n<p>特点： GAN有一个判别器和一个生成器。生成器用来生成数据，判别器用来判别生成的数据的真假。例如先训练生成器，当生成的图片在一定程度上骗过判别器，也即判别的表现不好使，停止训练生成器，训练判别器以提升判别能力，然后再次训练生成器。</p>\n<p>变种： </p>\n<ol>\n<li>条件GAN,多输入一个label，要求判别器判别图片是否逼真的同时判断图片是否满足label，以生成满足label的图片。<br><img src=\"/2021/08/27/models-summary/conditionGAN.png\"></li>\n<li>Cycle GAN Cycle不仅要求图片逼真，还要求图片能够有另一个生成器复原回原来的图片，个人觉得和自编码器有异曲同工之妙。<br><img src=\"/2021/08/27/models-summary/CycleGAN.png\"><h2 id=\"Resnet\"><a href=\"#Resnet\" class=\"headerlink\" title=\"Resnet\"></a>Resnet</h2>残差网络是一大创举，它的提出意味这更深的网络可以表现更优。<br>背景：在卷积神经网络提出后，研究者为了更好的效果不断增加网络的层数到百层，但是发现训练效果不好由此有人唱衰深度学习，restnet在这样的情况下无疑是救深度学习于水火。</li>\n</ol>\n<p><img src=\"/2021/08/27/models-summary/resnet.png\"></p>\n<p>Resnet的思想是我添加一个bottleneck模块，将输入与bottleneck的输出相加作为下一个输入，而bottleneck的输出是可以为0的，也意味着我可以抛弃掉这个模块，但是不为0的情况说明网络在训练过程中，这个module work，也就是说我怎么样都不亏。专业一点说就是在网络加深的过程中，至少不会因为bottleneck的增加而退化。</p>\n<h2 id=\"Mobilenet\"><a href=\"#Mobilenet\" class=\"headerlink\" title=\"Mobilenet\"></a>Mobilenet</h2><p>Mobilenet是一个轻量级网络系列，适用于对处理速度要求高，可以稍微牺牲一点精度的任务，如web端和嵌入式设备部署。<br>Mobilenet系列的基本思想是应用<strong>深度卷积</strong>和<strong>逐点卷积</strong>，极大得减少了参数量。</p>\n<p>深度卷积：卷积核只是一维的，只负责一个通道的运算。</p>\n<p><img src=\"/2021/08/27/models-summary/dw.png\"></p>\n<p>逐点卷积：由于深度卷积的定义，特征在深度卷积后维度是不会改变的，利用逐点卷积可以做到升维的效果。其卷积核的大小为 $1 \\ast 1 \\ast C $, C为输入的通道数，也即逐点卷积后的维度由逐点卷积的卷积核个数决定，大小由于是(1, 1)的核所以不变。</p>\n<p><img src=\"/2021/08/27/models-summary/pw2.png\"></p>\n<p>将两者结合得到深度可分离卷积架构。</p>\n<p><img src=\"/2021/08/27/models-summary/dsc.png\"></p>\n<p><img src=\"/2021/08/27/models-summary/dsc2.png\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/Exceptional-Control-Flow/exceptions.jpg","slug":"exceptions.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Exceptional-Control-Flow/context_switch.jpg","slug":"context_switch.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Exceptional-Control-Flow/process.jpg","slug":"process.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Exceptional-Control-Flow/signal_received.jpg","slug":"signal_received.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Exceptional-Control-Flow/signal_handler.jpg","slug":"signal_handler.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Exceptional-Control-Flow/signal_type.jpg","slug":"signal_type.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Exceptional-Control-Flow/race.jpg","slug":"race.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Exceptional-Control-Flow/guidelines.jpg","slug":"guidelines.jpg","post":"ckskmdeiq0000umkk5741g9rx","modified":0,"renderable":0},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/stack_cr.jpg","slug":"stack_cr.jpg","post":"ckrsmhqht0000zhkk7mplcltu","modified":0,"renderable":0},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/jump_table.jpg","slug":"jump_table.jpg","post":"ckrsmhqht0000zhkk7mplcltu","modified":0,"renderable":0},{"_id":"source/_posts/Machine-Level-Representation-of-Programs/saved.jpg","slug":"saved.jpg","post":"ckrsmhqht0000zhkk7mplcltu","modified":0,"renderable":0},{"_id":"source/_posts/System-Level-I-O/table.jpg","slug":"table.jpg","post":"cksjzdkeq00056ykkdus75amu","modified":0,"renderable":0},{"_id":"source/_posts/System-Level-I-O/info.jpg","slug":"info.jpg","post":"cksjzdkeq00056ykkdus75amu","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/1.png","slug":"1.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/2.png","slug":"2.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/3.png","slug":"3.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/4.png","slug":"4.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/5.png","slug":"5.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/6.png","slug":"6.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/7.png","slug":"7.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/8.png","slug":"8.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/deeplearning-pipeline-for-pytorch/init.png","slug":"init.png","post":"ckst2t1370000ffkkhxjl5goy","modified":0,"renderable":0},{"_id":"source/_posts/Convolution/c.gif","slug":"c.gif","post":"ckstsv84g00000vkk9guabgrp","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/c.png","slug":"c.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/RNN.png","slug":"RNN.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/BiRNN.png","slug":"BiRNN.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/LSTM.png","slug":"LSTM.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/GAN.png","slug":"GAN.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/CycleGAN.png","slug":"CycleGAN.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/resnet.png","slug":"resnet.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/dw.png","slug":"dw.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/pw.png","slug":"pw.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/dsc.png","slug":"dsc.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/conditionGAN.png","slug":"conditionGAN.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/dsc2.png","slug":"dsc2.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/models-summary/pw2.png","slug":"pw2.png","post":"cksu0ciry00008tkk12893wz8","modified":0,"renderable":0},{"_id":"source/_posts/Convolution/d.gif","slug":"d.gif","post":"ckstsv84g00000vkk9guabgrp","modified":1,"renderable":0}],"PostCategory":[{"post_id":"ckrsmhqht0000zhkk7mplcltu","category_id":"ckskg3dm70000i7kk03tmd7su","_id":"ckskg868q0001rzkk6us9f1vq"},{"post_id":"ckrnkqmec0000gckk2sozbnip","category_id":"ckskg3dm70000i7kk03tmd7su","_id":"ckskg9i6o0004rzkk3bged1d9"},{"post_id":"cksjzdken00026ykk3nhmdcow","category_id":"ckskg3dm70000i7kk03tmd7su","_id":"ckskgae3p0007rzkkhnzlbuz6"},{"post_id":"cksjzdkeq00056ykkdus75amu","category_id":"ckskg3dm70000i7kk03tmd7su","_id":"ckskgbglb000arzkkalwfbobs"},{"post_id":"cksjzdkek00006ykkcoi40ps7","category_id":"ckskg3dm70000i7kk03tmd7su","_id":"ckskgd8am000105kk8nxt7k7g"},{"post_id":"ckskmdeiq0000umkk5741g9rx","category_id":"ckskg3dm70000i7kk03tmd7su","_id":"ckskme4c00002umkka865fnb4"},{"post_id":"ckst2t1370000ffkkhxjl5goy","category_id":"ckst2t1380001ffkk4sdc7viy","_id":"ckst2t13c0004ffkk6h7d7rp8"},{"post_id":"ckstsv84g00000vkk9guabgrp","category_id":"ckst2t1380001ffkk4sdc7viy","_id":"ckstsv84n00020vkkfo7o9t1l"},{"post_id":"cksu0ciry00008tkk12893wz8","category_id":"ckst2t1380001ffkk4sdc7viy","_id":"cksu3evb200028tkkaqqf7feh"}],"PostTag":[{"post_id":"ckrsmhqht0000zhkk7mplcltu","tag_id":"ckskg868o0000rzkk37nt2zcj","_id":"ckskg868q0002rzkk0tctdbb0"},{"post_id":"ckrnkqmec0000gckk2sozbnip","tag_id":"ckskg9i6o0003rzkkhmynec5y","_id":"ckskg9i6p0005rzkk09jwe9hk"},{"post_id":"cksjzdken00026ykk3nhmdcow","tag_id":"ckskgae3p0006rzkk15sidzkr","_id":"ckskgae3q0008rzkkh1lq5aym"},{"post_id":"cksjzdkeq00056ykkdus75amu","tag_id":"ckskgbgl80009rzkk49yf6psv","_id":"ckskgbglb000brzkkdim958cq"},{"post_id":"cksjzdkek00006ykkcoi40ps7","tag_id":"ckskgd8al000005kk4wza4mum","_id":"ckskgd8an000205kkbooud5xs"},{"post_id":"ckskmdeiq0000umkk5741g9rx","tag_id":"ckskg3dma0001i7kk9usufpvu","_id":"ckskme4c00001umkk472jddft"},{"post_id":"ckst2t1370000ffkkhxjl5goy","tag_id":"ckst2t13c0002ffkk5jrwf7kp","_id":"ckst2t13c0003ffkkfv55g5gf"},{"post_id":"ckstsv84g00000vkk9guabgrp","tag_id":"ckstsv84j00010vkk0e4r4o1l","_id":"ckstsv84n00030vkkfus00mql"},{"post_id":"cksu0ciry00008tkk12893wz8","tag_id":"cksu3evb100018tkkc04cf1rf","_id":"cksu3evb200038tkk3jevgilh"}],"Tag":[{"name":"异常控制流","_id":"ckskg3dma0001i7kk9usufpvu"},{"name":"汇编","_id":"ckskg868o0000rzkk37nt2zcj"},{"name":"计算机的位级理解","_id":"ckskg9i6o0003rzkkhmynec5y"},{"name":"网络编程","_id":"ckskgae3p0006rzkk15sidzkr"},{"name":"Linux文件","_id":"ckskgbgl80009rzkk49yf6psv"},{"name":"并发","_id":"ckskgd8al000005kk4wza4mum"},{"name":"pytorch深度学习的一般流程","_id":"ckst2t13c0002ffkk5jrwf7kp"},{"name":"卷积","_id":"ckstsv84j00010vkk0e4r4o1l"},{"name":"模型总结","_id":"cksu3evb100018tkkc04cf1rf"}]}}