---
title: models-summary
date: 2021-08-27 15:02:36
tags: 模型总结
mathjax: true
categories: 深度学习
cover:
keywords: CNN RNN GAN Resnet MobileNet
---

# 模型总结

本篇博客对深度学习中的常见模型做出总结

## CNN

卷积神经网络是图像处理中最常用的网络之一。

![](c.png)
组成：
- 卷积层
- 池化层

特点：在CV领域，应用卷积的网络中提取窄而深。

## RNN

RNN，循环神经网络主要应用于文本生成。
特点：考虑输入序列的上下文信息，由此在NLP中可以预测文本生成。

组成：RNN单元

![](RNN.png)


- X 向量输入
- O 向量输出
- S 每一个隐藏层的中间的输出
- W U V 权重矩阵，不同时刻权重共享

每个隐藏层都计算一个中间值，把这个值作为下一个隐藏层输入的之一，由此网络可以考虑到上下文信息。

变种及改进：
1. BiRNN（双向循环神经网络）RNN只考虑到前文对后文的影响，但是后文对前文没有影响。一个trival想法是多加一组隐藏层，完成由后至前的影响。
![](BiRNN.png)
2. LSTM（长短时记忆网络）RNN的模式决定了长期以前的上文对下文的影响很小，为了改进LSTM多输入一个向量作为门，输出是一个0，1的向量，表示是否利用长期的内容对后文的影响。
![](LSTM.png)
## GAN

生成对抗网络利用模型对抗使得生成的样本符合要求。生成对抗网络多用在图片生成，表情变换等。

![](GAN.png)

特点： GAN有一个判别器和一个生成器。生成器用来生成数据，判别器用来判别生成的数据的真假。例如先训练生成器，当生成的图片在一定程度上骗过判别器，也即判别的表现不好使，停止训练生成器，训练判别器以提升判别能力，然后再次训练生成器。

变种： 
1. 条件GAN,多输入一个label，要求判别器判别图片是否逼真的同时判断图片是否满足label，以生成满足label的图片。
![](conditionGAN.png)
2. Cycle GAN Cycle不仅要求图片逼真，还要求图片能够有另一个生成器复原回原来的图片，个人觉得和自编码器有异曲同工之妙。
![](CycleGAN.png)
## Resnet
残差网络是一大创举，它的提出意味这更深的网络可以表现更优。
背景：在卷积神经网络提出后，研究者为了更好的效果不断增加网络的层数到百层，但是发现训练效果不好由此有人唱衰深度学习，restnet在这样的情况下无疑是救深度学习于水火。

![](resnet.png)

Resnet的思想是我添加一个bottleneck模块，将输入与bottleneck的输出相加作为下一个输入，而bottleneck的输出是可以为0的，也意味着我可以抛弃掉这个模块，但是不为0的情况说明网络在训练过程中，这个module work，也就是说我怎么样都不亏。专业一点说就是在网络加深的过程中，至少不会因为bottleneck的增加而退化。

## Mobilenet
Mobilenet是一个轻量级网络系列，适用于对处理速度要求高，可以稍微牺牲一点精度的任务，如web端和嵌入式设备部署。
Mobilenet系列的基本思想是应用**深度卷积**和**逐点卷积**，极大得减少了参数量。

深度卷积：卷积核只是一维的，只负责一个通道的运算。

![](dw.png)

逐点卷积：由于深度卷积的定义，特征在深度卷积后维度是不会改变的，利用逐点卷积可以做到升维的效果。其卷积核的大小为 $1 \ast 1 \ast C $, C为输入的通道数，也即逐点卷积后的维度由逐点卷积的卷积核个数决定，大小由于是(1, 1)的核所以不变。

![](pw2.png)


将两者结合得到深度可分离卷积架构。

![](dsc.png)

![](dsc2.png)
